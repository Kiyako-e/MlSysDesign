spark-worker-2-1     | [38;5;6mspark [38;5;5m12:00:31.06 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:00:31.06 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:00:31.06 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:00:31.06 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:00:31.06 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:00:31.10 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:00:31.11 [0m[38;5;2mINFO [0m ==> Generating Spark configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:00:31.12 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:00:31.12 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:00:31.13 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 12:00:32 INFO Worker: Started daemon with process name: 35@e92f47d0559d
spark-worker-2-1     | 24/12/13 12:00:32 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 12:00:32 INFO SignalUtils: Registering signal handler for HUP
spark-worker-2-1     | 24/12/13 12:00:32 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 12:00:32 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 12:00:32 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 12:00:32 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 12:00:32 INFO SecurityManager: Changing modify acls groups to: 
airflow-webserver-1  | 
spark-worker-2-1     | 24/12/13 12:00:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 12:00:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 12:00:33 INFO Utils: Successfully started service 'sparkWorker' on port 45411.
spark-worker-2-1     | 24/12/13 12:00:33 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 12:00:33 INFO Worker: Starting Spark worker 172.18.0.8:45411 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 12:00:33 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 12:00:33 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 12:00:33 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 12:00:33 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 12:00:33 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 12:00:33 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 12:00:33 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 12:00:33 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 12:00:33 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 12:00:33 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 52 ms (0 ms spent in bootstraps)
spark-worker-2-1     | 24/12/13 12:00:34 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:14:13.86 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:14:13.86 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:14:13.86 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:14:13.86 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:14:13.86 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:14:13.93 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.4:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.4:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.5:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.5:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.5:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.5:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.2:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.2:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:14:13.94 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:14:13.95 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:14:13.95 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:14:13.96 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 12:14:16 INFO Worker: Started daemon with process name: 33@e92f47d0559d
spark-worker-2-1     | 24/12/13 12:14:16 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 12:14:16 INFO SignalUtils: Registering signal handler for HUP
spark-worker-2-1     | 24/12/13 12:14:16 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 12:14:16 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 12:14:16 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 12:14:16 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 12:14:16 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 12:14:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 12:14:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 12:14:16 INFO Utils: Successfully started service 'sparkWorker' on port 40817.
spark-worker-2-1     | 24/12/13 12:14:16 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 12:14:16 INFO Worker: Starting Spark worker 172.18.0.7:40817 with 10 cores, 2.0 GiB RAM
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.3:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.3:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.4:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.4:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.4:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.4:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.4:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.4:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
redis-1              | 1:C 13 Dec 2024 12:00:30.875 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 12:00:30.875 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 12:00:30.875 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 12:00:30.875 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 12:00:30.876 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 12:00:30.876 * Server initialized
redis-1              | 1:M 13 Dec 2024 12:00:30.876 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734091344) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 12:02:24.315 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 12:02:24.315 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 12:02:24.317 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 12:02:24.317 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 12:09:57.857 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 12:09:57.857 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 12:09:57.857 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 12:09:57.857 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 12:09:57.858 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 12:09:57.858 * Server initialized
redis-1              | 1:M 13 Dec 2024 12:09:57.859 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 12:09:57.859 * RDB age 453 seconds
redis-1              | 1:M 13 Dec 2024 12:09:57.859 * RDB memory usage when created 1.40 Mb
redis-1              | 1:M 13 Dec 2024 12:09:57.859 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 12:09:57.859 * DB loaded from disk: 0.001 seconds
redis-1              | 1:M 13 Dec 2024 12:09:57.859 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734092083) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 12:14:43.837 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 12:14:43.837 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 12:14:43.839 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 12:14:43.839 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 12:15:06.585 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 12:15:06.585 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 12:15:06.585 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 12:15:06.585 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 12:15:06.586 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 12:15:06.586 * Server initialized
redis-1              | 1:M 13 Dec 2024 12:15:06.586 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 12:15:06.586 * RDB age 23 seconds
redis-1              | 1:M 13 Dec 2024 12:15:06.586 * RDB memory usage when created 1.30 Mb
redis-1              | 1:M 13 Dec 2024 12:15:06.586 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 12:15:06.586 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 12:15:06.586 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734092504) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 12:21:44.436 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 12:21:44.436 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 12:21:44.438 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 12:21:44.438 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 12:22:21.648 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 12:22:21.648 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 12:22:21.648 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 12:22:21.648 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 12:22:21.649 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 12:22:21.649 * Server initialized
redis-1              | 1:M 13 Dec 2024 12:22:21.649 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 12:22:21.649 * RDB age 37 seconds
redis-1              | 1:M 13 Dec 2024 12:22:21.649 * RDB memory usage when created 1.40 Mb
redis-1              | 1:M 13 Dec 2024 12:22:21.649 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 12:22:21.650 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 12:22:21.650 * Ready to accept connections tcp
redis-1              | 1:M 13 Dec 2024 12:29:21.238 * 100 changes in 300 seconds. Saving...
redis-1              | 1:M 13 Dec 2024 12:29:21.239 * Background saving started by pid 265
redis-1              | 265:C 13 Dec 2024 12:29:21.245 * DB saved on disk
redis-1              | 265:C 13 Dec 2024 12:29:21.246 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
redis-1              | 1:M 13 Dec 2024 12:29:21.340 * Background saving terminated with success
redis-1              | 1:signal-handler (1734092963) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 12:29:23.940 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 12:29:23.940 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 12:29:23.942 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 12:29:23.942 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 12:30:09.640 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 12:30:09.640 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 12:30:09.640 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 12:30:09.640 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 12:30:09.641 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 12:30:09.641 * Server initialized
redis-1              | 1:M 13 Dec 2024 12:30:09.641 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 12:30:09.641 * RDB age 46 seconds
redis-1              | 1:M 13 Dec 2024 12:30:09.641 * RDB memory usage when created 1.40 Mb
redis-1              | 1:M 13 Dec 2024 12:30:09.642 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 12:30:09.642 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 12:30:09.642 * Ready to accept connections tcp
redis-1              | 1:M 13 Dec 2024 12:37:29.440 * 100 changes in 300 seconds. Saving...
redis-1              | 1:M 13 Dec 2024 12:37:29.441 * Background saving started by pid 283
redis-1              | 283:C 13 Dec 2024 12:37:29.443 * DB saved on disk
redis-1              | 283:C 13 Dec 2024 12:37:29.444 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
redis-1              | 1:M 13 Dec 2024 12:37:29.546 * Background saving terminated with success
redis-1              | 1:signal-handler (1734093537) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 12:38:57.954 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 12:38:57.954 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 12:38:57.965 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 12:38:57.965 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 13:02:08.712 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 13:02:08.712 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 13:02:08.712 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 13:02:08.712 * monotonic clock: POSIX clock_gettime
airflow-scheduler-1  | 
airflow-scheduler-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-scheduler-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-scheduler-1  | BACKEND=redis
airflow-scheduler-1  | DB_HOST=redis
airflow-scheduler-1  | DB_PORT=6379
airflow-scheduler-1  | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:00:31.00 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:00:31.00 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:00:31.00 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:00:31.00 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:00:31.00 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:00:31.01 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:00:31.03 [0m[38;5;2mINFO [0m ==> Generating Spark configuration file...
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
airflow-scheduler-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-scheduler-1  |   ____________       _____________
airflow-scheduler-1  |  ____    |__( )_________  __/__  /________      __
airflow-scheduler-1  | ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
airflow-scheduler-1  | ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
airflow-scheduler-1  |  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
airflow-scheduler-1  | [2024-12-13T14:41:24.046+0000] {_client.py:1026} INFO - HTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.3&python_version=3.12&platform=Linux&arch=aarch64&database=postgresql&db_version=13.18&executor=CeleryExecutor "HTTP/1.1 200 OK"
airflow-scheduler-1  | [2024-12-13T14:41:25.925+0000] {executor_loader.py:254} INFO - Loaded executor: CeleryExecutor
airflow-scheduler-1  | [2024-12-13T14:41:26.003+0000] {scheduler_job_runner.py:938} INFO - Starting the scheduler
airflow-scheduler-1  | [2024-12-13T14:41:26.007+0000] {scheduler_job_runner.py:945} INFO - Processing each file at most -1 times
airflow-scheduler-1  | [2024-12-13T14:41:26.029+0000] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 47
airflow-scheduler-1  | [2024-12-13T14:41:26.031+0000] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
airflow-scheduler-1  | [2024-12-13T14:41:26.038+0000] {settings.py:63} INFO - Configured default timezone UTC
airflow-scheduler-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:41:26] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T14:41:26.892+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 54)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:41:56] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T14:41:57.453+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 118)
airflow-scheduler-1  | Dag run  in running state
airflow-scheduler-1  | Dag information Queued at: 2024-12-13 14:42:07.093532+00:00 hash info: b7cb26b06dc6445439e201c41d451bd8
airflow-scheduler-1  | [2024-12-13T14:42:07.298+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
airflow-scheduler-1  | 	<TaskInstance: pipeline.download_and_unwrap manual__2024-12-13T14:42:07.082582+00:00 [scheduled]>
airflow-scheduler-1  | [2024-12-13T14:42:07.298+0000] {scheduler_job_runner.py:495} INFO - DAG pipeline has 0/16 running and queued tasks
airflow-scheduler-1  | [2024-12-13T14:42:07.298+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
airflow-scheduler-1  | 	<TaskInstance: pipeline.download_and_unwrap manual__2024-12-13T14:42:07.082582+00:00 [scheduled]>
airflow-scheduler-1  | [2024-12-13T14:42:07.299+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: pipeline.download_and_unwrap manual__2024-12-13T14:42:07.082582+00:00 [scheduled]>] for executor: CeleryExecutor(parallelism=32)
airflow-scheduler-1  | [2024-12-13T14:42:07.300+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='pipeline', task_id='download_and_unwrap', run_id='manual__2024-12-13T14:42:07.082582+00:00', try_number=1, map_index=-1) to CeleryExecutor with priority 4 and queue default
airflow-scheduler-1  | [2024-12-13T14:42:07.300+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'pipeline', 'download_and_unwrap', 'manual__2024-12-13T14:42:07.082582+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-scheduler-1  | [2024-12-13T14:42:07.600+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state queued for task instance TaskInstanceKey(dag_id='pipeline', task_id='download_and_unwrap', run_id='manual__2024-12-13T14:42:07.082582+00:00', try_number=1, map_index=-1)
airflow-scheduler-1  | [2024-12-13T14:42:07.605+0000] {scheduler_job_runner.py:791} INFO - Setting external_id for <TaskInstance: pipeline.download_and_unwrap manual__2024-12-13T14:42:07.082582+00:00 [queued]> to 39e1513b-69f2-4b79-9b57-55902906e741
airflow-scheduler-1  | [2024-12-13T14:42:16.215+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
airflow-scheduler-1  | 	<TaskInstance: pipeline.split manual__2024-12-13T14:42:07.082582+00:00 [scheduled]>
airflow-scheduler-1  | [2024-12-13T14:42:16.216+0000] {scheduler_job_runner.py:495} INFO - DAG pipeline has 0/16 running and queued tasks
airflow-scheduler-1  | [2024-12-13T14:42:16.218+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
airflow-scheduler-1  | 	<TaskInstance: pipeline.split manual__2024-12-13T14:42:07.082582+00:00 [scheduled]>
airflow-scheduler-1  | [2024-12-13T14:42:16.220+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: pipeline.split manual__2024-12-13T14:42:07.082582+00:00 [scheduled]>] for executor: CeleryExecutor(parallelism=32)
airflow-scheduler-1  | [2024-12-13T14:42:16.221+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='pipeline', task_id='split', run_id='manual__2024-12-13T14:42:07.082582+00:00', try_number=1, map_index=-1) to CeleryExecutor with priority 3 and queue default
airflow-scheduler-1  | [2024-12-13T14:42:16.221+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'pipeline', 'split', 'manual__2024-12-13T14:42:07.082582+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-scheduler-1  | [2024-12-13T14:42:16.240+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state queued for task instance TaskInstanceKey(dag_id='pipeline', task_id='split', run_id='manual__2024-12-13T14:42:07.082582+00:00', try_number=1, map_index=-1)
airflow-scheduler-1  | [2024-12-13T14:42:16.241+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='pipeline', task_id='download_and_unwrap', run_id='manual__2024-12-13T14:42:07.082582+00:00', try_number=1, map_index=-1)
airflow-scheduler-1  | [2024-12-13T14:42:16.244+0000] {scheduler_job_runner.py:791} INFO - Setting external_id for <TaskInstance: pipeline.split manual__2024-12-13T14:42:07.082582+00:00 [queued]> to 6789a300-d913-4a6d-aee0-508369a38049
airflow-scheduler-1  | [2024-12-13T14:42:16.244+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=pipeline, task_id=download_and_unwrap, run_id=manual__2024-12-13T14:42:07.082582+00:00, map_index=-1, run_start_date=2024-12-13 14:42:10.369056+00:00, run_end_date=2024-12-13 14:42:15.600586+00:00, run_duration=5.23153, state=success, executor=CeleryExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=174, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-12-13 14:42:07.299059+00:00, queued_by_job_id=173, pid=77
airflow-scheduler-1  | [2024-12-13T14:42:19.626+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
airflow-scheduler-1  | 	<TaskInstance: pipeline.upload_to_minio manual__2024-12-13T14:42:07.082582+00:00 [scheduled]>
airflow-scheduler-1  | [2024-12-13T14:42:19.627+0000] {scheduler_job_runner.py:495} INFO - DAG pipeline has 0/16 running and queued tasks
airflow-scheduler-1  | [2024-12-13T14:42:19.628+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
airflow-scheduler-1  | 	<TaskInstance: pipeline.upload_to_minio manual__2024-12-13T14:42:07.082582+00:00 [scheduled]>
airflow-scheduler-1  | [2024-12-13T14:42:19.629+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: pipeline.upload_to_minio manual__2024-12-13T14:42:07.082582+00:00 [scheduled]>] for executor: CeleryExecutor(parallelism=32)
airflow-scheduler-1  | [2024-12-13T14:42:19.630+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='pipeline', task_id='upload_to_minio', run_id='manual__2024-12-13T14:42:07.082582+00:00', try_number=1, map_index=-1) to CeleryExecutor with priority 2 and queue default
airflow-scheduler-1  | [2024-12-13T14:42:19.630+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'pipeline', 'upload_to_minio', 'manual__2024-12-13T14:42:07.082582+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-scheduler-1  | [2024-12-13T14:42:19.655+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state queued for task instance TaskInstanceKey(dag_id='pipeline', task_id='upload_to_minio', run_id='manual__2024-12-13T14:42:07.082582+00:00', try_number=1, map_index=-1)
airflow-scheduler-1  | [2024-12-13T14:42:19.660+0000] {scheduler_job_runner.py:791} INFO - Setting external_id for <TaskInstance: pipeline.upload_to_minio manual__2024-12-13T14:42:07.082582+00:00 [queued]> to 4bfa70d0-8f9d-47ad-ad62-f601a99fd476
airflow-scheduler-1  | [2024-12-13T14:42:20.762+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='pipeline', task_id='split', run_id='manual__2024-12-13T14:42:07.082582+00:00', try_number=1, map_index=-1)
minio-1              | API: http://172.18.0.3:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.3:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.4:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.4:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.5:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.5:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.4:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.4:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.4:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.4:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.4:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.4:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.4:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.4:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.3:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.3:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.3:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.3:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
minio-1              | MinIO Object Storage Server
minio-1              | Copyright: 2015-2024 MinIO, Inc.
minio-1              | License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
minio-1              | Version: RELEASE.2024-11-07T00-52-20Z (go1.23.3 linux/arm64)
minio-1              | 
minio-1              | API: http://172.18.0.3:9000  http://127.0.0.1:9000 
minio-1              | WebUI: http://172.18.0.3:9090 http://127.0.0.1:9090  
minio-1              | 
minio-1              | Docs: https://docs.min.io
minio-1              | WARN: Detected default credentials 'minioadmin:minioadmin', we recommend that you change these values with 'MINIO_ROOT_USER' and 'MINIO_ROOT_PASSWORD' environment variables
minio-1              | INFO: Exiting on signal: TERMINATED
airflow-triggerer-1  | 
airflow-triggerer-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-triggerer-1  |   ____________       _____________
airflow-triggerer-1  |  ____    |__( )_________  __/__  /________      __
airflow-triggerer-1  | ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
airflow-triggerer-1  | ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
airflow-triggerer-1  |  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
airflow-triggerer-1  | [2024-12-13 14:41:17 +0000] [23] [INFO] Starting gunicorn 23.0.0
airflow-triggerer-1  | [2024-12-13 14:41:17 +0000] [23] [INFO] Listening at: http://[::]:8794 (23)
airflow-triggerer-1  | [2024-12-13 14:41:17 +0000] [23] [INFO] Using worker: sync
airflow-triggerer-1  | [2024-12-13 14:41:17 +0000] [24] [INFO] Booting worker with pid: 24
airflow-triggerer-1  | [2024-12-13 14:41:17 +0000] [25] [INFO] Booting worker with pid: 25
airflow-triggerer-1  | [2024-12-13T14:41:20.456+0000] {triggerer_job_runner.py:181} INFO - Setting up TriggererHandlerWrapper with handler <FileTaskHandler (NOTSET)>
airflow-triggerer-1  | [2024-12-13T14:41:20.457+0000] {triggerer_job_runner.py:237} INFO - Setting up logging queue listener with handlers [<RedirectStdHandler <stdout> (NOTSET)>, <TriggererHandlerWrapper (NOTSET)>]
airflow-triggerer-1  | [2024-12-13T14:41:20.463+0000] {triggerer_job_runner.py:338} INFO - Starting the triggerer
airflow-triggerer-1  | [2024-12-13T14:42:19.325+0000] {triggerer_job_runner.py:605} INFO - Triggerer's async thread was blocked for 0.29 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.
airflow-triggerer-1  | [2024-12-13T14:42:20.634+0000] {triggerer_job_runner.py:510} INFO - 0 triggers currently running
airflow-triggerer-1  | [2024-12-13T14:43:20.822+0000] {triggerer_job_runner.py:510} INFO - 0 triggers currently running
airflow-triggerer-1  | [2024-12-13T14:44:19.074+0000] {triggerer_job_runner.py:605} INFO - Triggerer's async thread was blocked for 0.30 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.
airflow-triggerer-1  | [2024-12-13T14:44:21.097+0000] {triggerer_job_runner.py:510} INFO - 0 triggers currently running
airflow-scheduler-1  | [2024-12-13T14:42:20.765+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=pipeline, task_id=split, run_id=manual__2024-12-13T14:42:07.082582+00:00, map_index=-1, run_start_date=2024-12-13 14:42:18.246718+00:00, run_end_date=2024-12-13 14:42:19.556095+00:00, run_duration=1.309377, state=success, executor=CeleryExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=175, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-12-13 14:42:16.219255+00:00, queued_by_job_id=173, pid=80
airflow-scheduler-1  | [2024-12-13T14:42:22.369+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
airflow-scheduler-1  | 	<TaskInstance: pipeline.train_and_predict manual__2024-12-13T14:42:07.082582+00:00 [scheduled]>
airflow-scheduler-1  | [2024-12-13T14:42:22.370+0000] {scheduler_job_runner.py:495} INFO - DAG pipeline has 0/16 running and queued tasks
airflow-scheduler-1  | [2024-12-13T14:42:22.370+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
airflow-scheduler-1  | 	<TaskInstance: pipeline.train_and_predict manual__2024-12-13T14:42:07.082582+00:00 [scheduled]>
airflow-scheduler-1  | [2024-12-13T14:42:22.372+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: pipeline.train_and_predict manual__2024-12-13T14:42:07.082582+00:00 [scheduled]>] for executor: CeleryExecutor(parallelism=32)
spark-worker-2-1     | 24/12/13 12:14:16 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 12:14:16 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 12:14:16 INFO ResourceUtils: ==============================================================
airflow-scheduler-1  | [2024-12-13T14:42:22.372+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='pipeline', task_id='train_and_predict', run_id='manual__2024-12-13T14:42:07.082582+00:00', try_number=1, map_index=-1) to CeleryExecutor with priority 1 and queue default
airflow-scheduler-1  | [2024-12-13T14:42:22.372+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'pipeline', 'train_and_predict', 'manual__2024-12-13T14:42:07.082582+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-scheduler-1  | [2024-12-13T14:42:22.381+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state queued for task instance TaskInstanceKey(dag_id='pipeline', task_id='train_and_predict', run_id='manual__2024-12-13T14:42:07.082582+00:00', try_number=1, map_index=-1)
redis-1              | 1:M 13 Dec 2024 13:02:08.713 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 13:02:08.713 * Server initialized
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:00:31.05 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:00:31.05 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:00:31.05 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:00:31.05 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:00:31.06 [0m[38;5;2mINFO [0m ==> 
airflow-worker-1     | 
airflow-worker-1     | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-worker-1     | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-worker-1     | BACKEND=redis
airflow-worker-1     | DB_HOST=redis
airflow-worker-1     | DB_PORT=6379
airflow-worker-1     | 
airflow-worker-1     | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-worker-1     | [2024-12-13T14:41:22.529+0000] {configuration.py:2111} INFO - Creating new FAB webserver config file in: /opt/airflow/webserver_config.py
airflow-worker-1     | [2024-12-13 14:41:23 +0000] [33] [INFO] Starting gunicorn 23.0.0
airflow-worker-1     | [2024-12-13 14:41:23 +0000] [33] [INFO] Listening at: http://[::]:8793 (33)
airflow-worker-1     | [2024-12-13 14:41:23 +0000] [33] [INFO] Using worker: sync
airflow-worker-1     | [2024-12-13 14:41:23 +0000] [34] [INFO] Booting worker with pid: 34
airflow-worker-1     | [2024-12-13 14:41:23 +0000] [35] [INFO] Booting worker with pid: 35
redis-1              | 1:M 13 Dec 2024 13:02:08.714 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 13:02:08.714 * RDB age 1391 seconds
redis-1              | 1:M 13 Dec 2024 13:02:08.714 * RDB memory usage when created 1.40 Mb
redis-1              | 1:M 13 Dec 2024 13:02:08.714 * Done loading RDB, keys loaded: 1, keys expired: 0.
airflow-worker-1     |  
airflow-scheduler-1  | [2024-12-13T14:42:22.382+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='pipeline', task_id='upload_to_minio', run_id='manual__2024-12-13T14:42:07.082582+00:00', try_number=1, map_index=-1)
airflow-scheduler-1  | [2024-12-13T14:42:22.384+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=pipeline, task_id=upload_to_minio, run_id=manual__2024-12-13T14:42:07.082582+00:00, map_index=-1, run_start_date=2024-12-13 14:42:21.119955+00:00, run_end_date=2024-12-13 14:42:21.392750+00:00, run_duration=0.272795, state=success, executor=CeleryExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=176, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-12-13 14:42:19.628673+00:00, queued_by_job_id=173, pid=83
airflow-scheduler-1  | [2024-12-13T14:42:22.385+0000] {scheduler_job_runner.py:791} INFO - Setting external_id for <TaskInstance: pipeline.train_and_predict manual__2024-12-13T14:42:07.082582+00:00 [queued]> to 6b4a109a-dc6a-4f8e-a6dd-bd03e7bafc3b
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:42:26] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T14:42:28.185+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 182)
airflow-worker-1     |  -------------- celery@65b6e979e5f1 v5.4.0 (opalescent)
airflow-worker-1     | --- ***** ----- 
airflow-worker-1     | -- ******* ---- Linux-6.10.14-linuxkit-aarch64-with-glibc2.36 2024-12-13 14:41:23
airflow-worker-1     | - *** --- * --- 
airflow-worker-1     | - ** ---------- [config]
airflow-worker-1     | - ** ---------- .> app:         airflow.providers.celery.executors.celery_executor:0xffff91dc92b0
airflow-worker-1     | - ** ---------- .> transport:   redis://redis:6379/0
airflow-worker-1     | - ** ---------- .> results:     postgresql://airflow:**@postgres/airflow
redis-1              | 1:M 13 Dec 2024 13:02:08.714 * DB loaded from disk: 0.001 seconds
redis-1              | 1:M 13 Dec 2024 13:02:08.714 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734095210) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 13:06:50.338 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 13:06:50.338 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 13:06:50.340 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 13:06:50.340 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 13:07:15.714 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 13:07:15.715 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 13:07:15.715 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 13:07:15.715 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 13:07:15.715 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 13:07:15.715 * Server initialized
redis-1              | 1:M 13 Dec 2024 13:07:15.715 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 13:07:15.715 * RDB age 25 seconds
redis-1              | 1:M 13 Dec 2024 13:07:15.715 * RDB memory usage when created 1.40 Mb
redis-1              | 1:M 13 Dec 2024 13:07:15.715 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 13:07:15.715 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 13:07:15.715 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734095619) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 13:13:39.846 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 13:13:39.846 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 13:13:39.850 * DB saved on disk
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:00:31.10 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:00:31.11 [0m[38;5;2mINFO [0m ==> Generating Spark configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:00:31.12 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:00:31.12 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:00:31.13 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 12:00:32 INFO Worker: Started daemon with process name: 35@f484bf7bba3e
spark-worker-1-1     | 24/12/13 12:00:32 INFO SignalUtils: Registering signal handler for TERM
spark-worker-1-1     | 24/12/13 12:00:32 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 12:00:32 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 12:00:32 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 12:00:32 INFO SecurityManager: Changing modify acls to: spark
airflow-triggerer-1  | [2024-12-13T14:45:21.261+0000] {triggerer_job_runner.py:510} INFO - 0 triggers currently running
airflow-triggerer-1  | [2024-12-13T14:46:21.434+0000] {triggerer_job_runner.py:510} INFO - 0 triggers currently running
airflow-triggerer-1  | [2024-12-13 14:46:58 +0000] [24] [INFO] Worker exiting (pid: 24)
spark-worker-1-1     | 24/12/13 12:00:32 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 12:00:32 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 12:00:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 12:00:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 12:00:33 INFO Utils: Successfully started service 'sparkWorker' on port 42235.
spark-worker-1-1     | 24/12/13 12:00:33 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 12:00:33 INFO Worker: Starting Spark worker 172.18.0.7:42235 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 12:00:33 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 12:00:33 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 12:00:33 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 12:00:33 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 12:00:33 INFO ResourceUtils: ==============================================================
redis-1              | 1:M 13 Dec 2024 13:13:39.850 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 13:22:10.551 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 13:22:10.552 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
spark-worker-2-1     | 24/12/13 12:14:16 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 12:14:16 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 12:14:17 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 12:14:17 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 12:14:17 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 12:14:17 INFO Worker: Connecting to master spark-master:7077...
airflow-worker-1     | - *** --- * --- .> concurrency: 16 (prefork)
airflow-worker-1     | -- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)
airflow-worker-1     | --- ***** ----- 
airflow-worker-1     |  -------------- [queues]
spark-worker-1-1     | 24/12/13 12:00:33 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 12:00:33 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-1-1     | 24/12/13 12:00:33 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 12:00:33 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 12:00:34 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 20 ms (0 ms spent in bootstraps)
spark-worker-1-1     | 24/12/13 12:00:34 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:14:13.85 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:14:13.86 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:14:13.86 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:14:13.86 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:14:13.86 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:14:13.92 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:14:13.94 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:14:13.94 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:14:13.95 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:14:13.95 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
redis-1              | 1:C 13 Dec 2024 13:22:10.552 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 13:22:10.552 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 13:22:10.553 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 13:22:10.554 * Server initialized
redis-1              | 1:M 13 Dec 2024 13:22:10.554 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 13:22:10.554 * RDB age 511 seconds
redis-1              | 1:M 13 Dec 2024 13:22:10.554 * RDB memory usage when created 1.40 Mb
redis-1              | 1:M 13 Dec 2024 13:22:10.554 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 13:22:10.554 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 13:22:10.554 * Ready to accept connections tcp
redis-1              | 1:M 13 Dec 2024 13:29:38.346 * 100 changes in 300 seconds. Saving...
redis-1              | 1:M 13 Dec 2024 13:29:38.347 * Background saving started by pid 290
redis-1              | 290:C 13 Dec 2024 13:29:38.354 * DB saved on disk
redis-1              | 290:C 13 Dec 2024 13:29:38.354 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
redis-1              | 1:M 13 Dec 2024 13:29:38.448 * Background saving terminated with success
redis-1              | 1:signal-handler (1734096758) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 13:32:38.321 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 13:32:38.321 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 13:32:38.322 * DB saved on disk
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:00:31.03 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:00:31.03 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:00:31.04 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
airflow-triggerer-1  | [2024-12-13 14:46:58 +0000] [25] [INFO] Worker exiting (pid: 25)
airflow-triggerer-1  | [2024-12-13T14:46:58.804+0000] {triggerer_job_runner.py:351} INFO - Waiting for triggers to clean up
airflow-triggerer-1  | [2024-12-13 14:46:58 +0000] [23] [INFO] Handling signal: term
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-webserver-1  | [2024-12-13T14:41:17.382+0000] {configuration.py:2111} INFO - Creating new FAB webserver config file in: /opt/airflow/webserver_config.py
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/flask_limiter/extension.py:333 UserWarning: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. This is not recommended for production use. See: https://flask-limiter.readthedocs.io#configuring-a-storage-backend for documentation about configuring the storage backend.
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/providers/fab/auth_manager/fab_auth_manager.py:558 FutureWarning: section/key [webserver/update_fab_perms] has been deprecated, you should use[fab/update_fab_perms] instead. Please update your `conf.get*` call to use the new name
airflow-webserver-1  | [2024-12-13 14:41:23 +0000] [19] [INFO] Starting gunicorn 23.0.0
airflow-webserver-1  | [2024-12-13T14:41:29.877+0000] {providers_manager.py:287} INFO - Optional provider feature disabled when importing 'airflow.providers.google.leveldb.hooks.leveldb.LevelDBHook' from 'apache-airflow-providers-google' package
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/batch/models/_models_py3.py:4839 SyntaxWarning: invalid escape sequence '\s'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:175 SyntaxWarning: invalid escape sequence '\d'
spark-worker-1-1     | 24/12/13 12:14:16 INFO Worker: Started daemon with process name: 34@f484bf7bba3e
spark-worker-1-1     | 24/12/13 12:14:16 INFO SignalUtils: Registering signal handler for TERM
spark-worker-1-1     | 24/12/13 12:14:16 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 12:14:16 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 12:14:16 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 12:14:16 INFO SecurityManager: Changing modify acls to: spark
airflow-triggerer-1  | [2024-12-13 14:46:58 +0000] [23] [INFO] Shutting down: Master
airflow-triggerer-1  | [2024-12-13T14:46:59.532+0000] {triggerer_job_runner.py:357} INFO - Exited trigger loop
airflow-scheduler-1  | [2024-12-13T14:42:31.396+0000] {dagrun.py:854} INFO - Marking run <DagRun pipeline @ 2024-12-13 14:42:07.082582+00:00: manual__2024-12-13T14:42:07.082582+00:00, state:running, queued_at: 2024-12-13 14:42:07.093532+00:00. externally triggered: True> successful
airflow-scheduler-1  | Dag run in success state
airflow-scheduler-1  | Dag run start:2024-12-13 14:42:07.274703+00:00 end:2024-12-13 14:42:31.397645+00:00
airflow-scheduler-1  | [2024-12-13T14:42:31.397+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=pipeline, execution_date=2024-12-13 14:42:07.082582+00:00, run_id=manual__2024-12-13T14:42:07.082582+00:00, run_start_date=2024-12-13 14:42:07.274703+00:00, run_end_date=2024-12-13 14:42:31.397645+00:00, run_duration=24.122942, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-12-13 14:42:07.082582+00:00, data_interval_end=2024-12-13 14:42:07.082582+00:00, dag_hash=b7cb26b06dc6445439e201c41d451bd8
airflow-scheduler-1  | [2024-12-13T14:42:31.408+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='pipeline', task_id='train_and_predict', run_id='manual__2024-12-13T14:42:07.082582+00:00', try_number=1, map_index=-1)
spark-worker-1-1     | 24/12/13 12:14:16 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 12:14:16 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 12:14:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 12:14:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 12:14:16 INFO Utils: Successfully started service 'sparkWorker' on port 36271.
spark-worker-1-1     | 24/12/13 12:14:16 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 12:14:16 INFO Worker: Starting Spark worker 172.18.0.6:36271 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 12:14:16 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 12:14:16 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 12:14:16 INFO ResourceUtils: ==============================================================
airflow-scheduler-1  | [2024-12-13T14:42:31.411+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=pipeline, task_id=train_and_predict, run_id=manual__2024-12-13T14:42:07.082582+00:00, map_index=-1, run_start_date=2024-12-13 14:42:23.529727+00:00, run_end_date=2024-12-13 14:42:30.888086+00:00, run_duration=7.358359, state=success, executor=CeleryExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=177, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-12-13 14:42:22.371257+00:00, queued_by_job_id=173, pid=86
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:42:56] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T14:42:58.722+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 246)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:43:26] "GET /health HTTP/1.1" 200 -
spark-worker-2-1     | 24/12/13 12:14:17 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 57 ms (0 ms spent in bootstraps)
spark-worker-2-1     | 24/12/13 12:14:17 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:15:06.88 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:15:06.88 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:15:06.88 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:15:06.88 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
postgres-1           | 
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:222 SyntaxWarning: invalid escape sequence '\d'
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
airflow-scheduler-1  | [2024-12-13T14:43:29.100+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 310)
postgres-1           | 2024-12-13 12:00:30.917 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 12:00:30.919 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 12:00:30.919 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 12:00:30.924 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 12:00:30.937 UTC [27] LOG:  database system was shut down at 2024-12-13 12:00:01 UTC
spark-worker-1-1     | 24/12/13 12:14:16 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 12:14:16 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 12:14:17 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 12:14:17 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-master-1       | [38;5;6mspark [38;5;5m12:00:30.83 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m12:00:30.83 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
airflow-worker-1     |                 .> default          exchange=default(direct) key=default
airflow-worker-1     |                 
airflow-worker-1     | 
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:1151 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:1248 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:1271 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:1406 SyntaxWarning: invalid escape sequence '\d'
spark-worker-3-1     | ========================================
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 12:00:32 INFO Worker: Started daemon with process name: 35@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 12:00:32 INFO SignalUtils: Registering signal handler for TERM
postgres-1           | 2024-12-13 12:00:30.977 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 12:02:24.287 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 12:02:24.289 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 12:02:24.291 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 12:02:24.295 UTC [28] LOG:  shutting down
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:15:06.88 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:15:06.90 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:15:06.91 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | [38;5;6mspark [38;5;5m12:00:30.83 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:00:30.84 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:00:30.84 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m12:00:30.87 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m12:00:30.88 [0m[38;5;2mINFO [0m ==> Generating Spark configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m12:00:30.90 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m12:00:30.90 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | [38;5;6mspark [38;5;5m12:00:30.91 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 12:00:32 INFO Master: Started daemon with process name: 37@f9573584fb53
spark-master-1       | 24/12/13 12:00:32 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 12:00:32 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 12:00:32 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 12:00:32 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | 24/12/13 12:00:32 INFO SecurityManager: Changing modify acls to: spark
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:43:56] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T14:43:59.415+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 374)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:44:26] "GET /health HTTP/1.1" 200 -
redis-1              | 1:M 13 Dec 2024 13:32:38.322 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 13:33:04.669 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 13:33:04.669 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 13:33:04.669 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 13:33:04.669 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 13:33:04.670 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 13:33:04.670 * Server initialized
redis-1              | 1:M 13 Dec 2024 13:33:04.670 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 13:33:04.670 * RDB age 26 seconds
redis-1              | 1:M 13 Dec 2024 13:33:04.670 * RDB memory usage when created 1.40 Mb
redis-1              | 1:M 13 Dec 2024 13:33:04.670 * Done loading RDB, keys loaded: 1, keys expired: 0.
spark-master-1       | 24/12/13 12:00:32 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 12:00:32 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 12:00:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
airflow-worker-1     | [tasks]
airflow-worker-1     |   . airflow.providers.celery.executors.celery_executor_utils.execute_command
spark-worker-1-1     | 24/12/13 12:14:17 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 12:14:17 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 12:14:17 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 44 ms (0 ms spent in bootstraps)
airflow-triggerer-1  | 
airflow-triggerer-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-triggerer-1  |   ____________       _____________
airflow-triggerer-1  |  ____    |__( )_________  __/__  /________      __
airflow-triggerer-1  | ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
airflow-triggerer-1  | ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
airflow-triggerer-1  |  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
airflow-triggerer-1  | [2024-12-13 14:57:16 +0000] [15] [INFO] Starting gunicorn 23.0.0
airflow-triggerer-1  | [2024-12-13 14:57:16 +0000] [15] [INFO] Listening at: http://[::]:8794 (15)
airflow-triggerer-1  | [2024-12-13 14:57:16 +0000] [15] [INFO] Using worker: sync
airflow-triggerer-1  | [2024-12-13 14:57:16 +0000] [16] [INFO] Booting worker with pid: 16
airflow-triggerer-1  | [2024-12-13 14:57:16 +0000] [26] [INFO] Booting worker with pid: 26
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:1439 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:1495 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:1683 SyntaxWarning: invalid escape sequence '\d'
redis-1              | 1:M 13 Dec 2024 13:33:04.670 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 13:33:04.670 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734097030) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 13:37:10.435 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 13:37:10.435 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 13:37:10.476 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 13:37:10.476 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 13:41:38.237 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 13:41:38.237 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 13:41:38.237 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 13:41:38.237 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 13:41:38.238 * Running mode=standalone, port=6379.
postgres-1           | 2024-12-13 12:02:24.326 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 12:09:58.024 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 12:09:58.025 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
spark-master-1       | 24/12/13 12:00:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 12:00:33 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
airflow-worker-1     | 
airflow-worker-1     | [2024-12-13 14:41:26,389: WARNING/MainProcess] /home/airflow/.local/lib/python3.12/site-packages/celery/worker/consumer/consumer.py:508: CPendingDeprecationWarning: The broker_connection_retry configuration setting will no longer determine
airflow-worker-1     | whether broker connection retries are made during startup in Celery 6.0 and above.
airflow-worker-1     | If you wish to retain the existing behavior for retrying connections on startup,
airflow-worker-1     | you should set broker_connection_retry_on_startup to True.
airflow-worker-1     |   warnings.warn(
airflow-triggerer-1  | [2024-12-13T14:57:17.077+0000] {triggerer_job_runner.py:181} INFO - Setting up TriggererHandlerWrapper with handler <FileTaskHandler (NOTSET)>
airflow-triggerer-1  | [2024-12-13T14:57:17.078+0000] {triggerer_job_runner.py:237} INFO - Setting up logging queue listener with handlers [<RedirectStdHandler <stdout> (NOTSET)>, <TriggererHandlerWrapper (NOTSET)>]
airflow-triggerer-1  | [2024-12-13T14:57:17.083+0000] {triggerer_job_runner.py:338} INFO - Starting the triggerer
redis-1              | 1:M 13 Dec 2024 13:41:38.238 * Server initialized
redis-1              | 1:M 13 Dec 2024 13:41:38.238 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 13:41:38.238 * RDB age 268 seconds
redis-1              | 1:M 13 Dec 2024 13:41:38.238 * RDB memory usage when created 1.40 Mb
redis-1              | 1:M 13 Dec 2024 13:41:38.238 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 13:41:38.238 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 13:41:38.238 * Ready to accept connections tcp
airflow-worker-1     | 
airflow-worker-1     | [2024-12-13 14:41:26,399: INFO/MainProcess] Connected to redis://redis:6379/0
airflow-worker-1     | [2024-12-13 14:41:26,401: WARNING/MainProcess] /home/airflow/.local/lib/python3.12/site-packages/celery/worker/consumer/consumer.py:508: CPendingDeprecationWarning: The broker_connection_retry configuration setting will no longer determine
airflow-worker-1     | whether broker connection retries are made during startup in Celery 6.0 and above.
airflow-worker-1     | If you wish to retain the existing behavior for retrying connections on startup,
spark-worker-1-1     | 24/12/13 12:14:17 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:15:06.87 [0m[38;5;2mINFO [0m ==> 
airflow-triggerer-1  | [2024-12-13T14:58:17.286+0000] {triggerer_job_runner.py:510} INFO - 0 triggers currently running
airflow-triggerer-1  | [2024-12-13T14:58:19.261+0000] {triggerer_job_runner.py:605} INFO - Triggerer's async thread was blocked for 0.31 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.
airflow-triggerer-1  | [2024-12-13T14:58:28.333+0000] {triggerer_job_runner.py:605} INFO - Triggerer's async thread was blocked for 0.22 seconds, likely by a badly-written trigger. Set PYTHONASYNCIODEBUG=1 to get more information on overrunning coroutines.
airflow-triggerer-1  | [2024-12-13T14:59:17.655+0000] {triggerer_job_runner.py:510} INFO - 0 triggers currently running
airflow-triggerer-1  | [2024-12-13T15:00:17.836+0000] {triggerer_job_runner.py:510} INFO - 0 triggers currently running
airflow-triggerer-1  | [2024-12-13T15:01:17.999+0000] {triggerer_job_runner.py:510} INFO - 0 triggers currently running
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:1750 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:2025 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:2110 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:2407 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:2468 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:3987 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:4096 SyntaxWarning: invalid escape sequence '\d'
redis-1              | 1:M 13 Dec 2024 13:49:07.465 * 100 changes in 300 seconds. Saving...
redis-1              | 1:M 13 Dec 2024 13:49:07.466 * Background saving started by pid 287
redis-1              | 287:C 13 Dec 2024 13:49:07.469 * DB saved on disk
spark-worker-3-1     | 24/12/13 12:00:32 INFO SignalUtils: Registering signal handler for HUP
redis-1              | 287:C 13 Dec 2024 13:49:07.469 * Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB
redis-1              | 1:M 13 Dec 2024 13:49:07.567 * Background saving terminated with success
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | 
airflow-worker-1     | you should set broker_connection_retry_on_startup to True.
airflow-worker-1     |   warnings.warn(
airflow-worker-1     | 
airflow-worker-1     | [2024-12-13 14:41:26,406: INFO/MainProcess] mingle: searching for neighbors
airflow-worker-1     | [2024-12-13 14:41:27,417: INFO/MainProcess] mingle: all alone
airflow-worker-1     | [2024-12-13 14:41:27,430: INFO/MainProcess] celery@65b6e979e5f1 ready.
airflow-worker-1     | [2024-12-13 14:42:07,574: INFO/MainProcess] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[39e1513b-69f2-4b79-9b57-55902906e741] received
airflow-worker-1     | [2024-12-13 14:42:07,650: INFO/ForkPoolWorker-15] [39e1513b-69f2-4b79-9b57-55902906e741] Executing command in Celery: ['airflow', 'tasks', 'run', 'pipeline', 'download_and_unwrap', 'manual__2024-12-13T14:42:07.082582+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-worker-1     | [2024-12-13 14:42:08,681: INFO/ForkPoolWorker-15] Filling up the DagBag from /opt/airflow/dags/pipeline.py
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:15:06.92 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
airflow-worker-1     | [2024-12-13 14:42:09,066: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: pipeline>
airflow-worker-1     | [2024-12-13 14:42:09,066: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcomargs.py
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:15:06.87 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:15:06.88 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:15:06.88 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:15:06.88 [0m[38;5;2mINFO [0m ==> 
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:4127 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:4186 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:4225 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:4275 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:5209 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:5265 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:5302 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:5358 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:6437 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:6497 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:6538 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:6588 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:7023 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:7096 SyntaxWarning: invalid escape sequence '\d'
spark-master-1       | 24/12/13 12:00:33 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 12:00:33 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 12:00:33 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 12:00:33 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 12:00:33 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 12:00:33 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 12:00:34 INFO Master: Registering worker 172.18.0.7:42235 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 12:00:34 INFO Master: Registering worker 172.18.0.8:45411 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 12:00:34 INFO Master: Registering worker 172.18.0.6:33905 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 12:02:29 INFO Master: 172.18.0.7:52312 got disassociated, removing it.
spark-master-1       | 24/12/13 12:02:29 INFO Master: 172.18.0.8:40364 got disassociated, removing it.
spark-master-1       | 24/12/13 12:02:29 INFO Master: 172.18.0.7:42235 got disassociated, removing it.
spark-master-1       | 24/12/13 12:02:29 INFO Master: Removing worker worker-20241213120033-172.18.0.7-42235 on 172.18.0.7:42235
spark-master-1       | 24/12/13 12:02:29 INFO Master: Telling app of lost worker: worker-20241213120033-172.18.0.7-42235
spark-master-1       | 24/12/13 12:02:29 INFO Master: 172.18.0.8:45411 got disassociated, removing it.
spark-master-1       | 24/12/13 12:02:29 INFO Master: Removing worker worker-20241213120033-172.18.0.8-45411 on 172.18.0.8:45411
spark-master-1       | 24/12/13 12:02:29 INFO Master: Telling app of lost worker: worker-20241213120033-172.18.0.8-45411
spark-master-1       | 24/12/13 12:02:29 INFO Master: 172.18.0.6:36958 got disassociated, removing it.
spark-master-1       | 24/12/13 12:02:29 INFO Master: 172.18.0.6:33905 got disassociated, removing it.
spark-master-1       | 24/12/13 12:02:29 INFO Master: Removing worker worker-20241213120033-172.18.0.6-33905 on 172.18.0.6:33905
spark-master-1       | 24/12/13 12:02:29 INFO Master: Telling app of lost worker: worker-20241213120033-172.18.0.6-33905
spark-master-1       | [38;5;6mspark [38;5;5m12:14:13.71 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m12:14:13.72 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:14:13.72 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:14:13.72 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:14:13.72 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m12:14:13.74 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m12:14:13.75 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m12:14:13.76 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | [38;5;6mspark [38;5;5m12:14:13.76 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m12:14:13.77 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 12:14:16 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-master-1       | 24/12/13 12:14:16 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 12:14:16 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 12:14:16 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 12:14:16 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | 24/12/13 12:14:16 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 12:14:16 INFO SecurityManager: Changing view acls groups to: 
redis-1              | 1:signal-handler (1734097778) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 13:49:38.715 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 13:49:38.715 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 13:49:38.719 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 13:49:38.719 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 13:50:03.448 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 13:50:03.448 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 13:50:03.448 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 13:50:03.449 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 13:50:03.449 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 13:50:03.455 * Server initialized
redis-1              | 1:M 13 Dec 2024 13:50:03.455 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 13:50:03.455 * RDB age 25 seconds
redis-1              | 1:M 13 Dec 2024 13:50:03.455 * RDB memory usage when created 1.40 Mb
redis-1              | 1:M 13 Dec 2024 13:50:03.455 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 13:50:03.455 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 13:50:03.455 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734098049) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 13:54:09.193 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 13:54:09.193 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 13:54:09.203 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 13:54:09.203 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 13:54:40.003 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 13:54:40.003 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 13:54:40.003 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 13:54:40.003 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 13:54:40.004 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 13:54:40.005 * Server initialized
redis-1              | 1:M 13 Dec 2024 13:54:40.005 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 13:54:40.005 * RDB age 31 seconds
redis-1              | 1:M 13 Dec 2024 13:54:40.005 * RDB memory usage when created 1.40 Mb
redis-1              | 1:M 13 Dec 2024 13:54:40.005 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 13:54:40.005 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 13:54:40.005 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734098227) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 13:57:07.288 * User requested shutdown...
spark-master-1       | 24/12/13 12:14:16 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 12:14:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 12:14:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 12:14:16 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 12:14:16 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 12:14:16 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 12:14:16 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 12:14:17 INFO Utils: Successfully started service 'MasterUI' on port 9080.
redis-1              | 1:M 13 Dec 2024 13:57:07.288 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 13:57:07.292 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 13:57:07.292 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 13:57:29.956 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 13:57:29.956 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 13:57:29.956 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 13:57:29.956 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 13:57:29.957 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 13:57:29.957 * Server initialized
redis-1              | 1:M 13 Dec 2024 13:57:29.957 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 13:57:29.957 * RDB age 22 seconds
postgres-1           | 2024-12-13 12:09:58.025 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 12:09:58.026 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 12:09:58.028 UTC [27] LOG:  database system was shut down at 2024-12-13 12:02:24 UTC
postgres-1           | 2024-12-13 12:09:58.039 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 12:14:43.762 UTC [1] LOG:  received fast shutdown request
airflow-triggerer-1  | [2024-12-13T15:02:02.692+0000] {triggerer_job_runner.py:351} INFO - Waiting for triggers to clean up
airflow-triggerer-1  | [2024-12-13 15:02:02 +0000] [26] [INFO] Worker exiting (pid: 26)
airflow-triggerer-1  | [2024-12-13 15:02:02 +0000] [16] [INFO] Worker exiting (pid: 16)
airflow-triggerer-1  | [2024-12-13 15:02:02 +0000] [15] [INFO] Handling signal: term
airflow-triggerer-1  | [2024-12-13 15:02:02 +0000] [15] [INFO] Shutting down: Master
airflow-triggerer-1  | [2024-12-13T15:02:03.167+0000] {triggerer_job_runner.py:357} INFO - Exited trigger loop
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:7267 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:7333 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:7378 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:7440 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:8222 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:8282 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:8322 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:8368 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:9475 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:9531 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:10532 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:10588 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:10625 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:10680 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:10923 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:10979 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:11016 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:11072 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:11231 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:11282 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:11420 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:11477 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:12300 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:12383 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:12439 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:12524 SyntaxWarning: invalid escape sequence '\d'
airflow-worker-1     | [2024-12-13 14:42:09,079: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom_args_with_operators>
airflow-worker-1     | [2024-12-13 14:42:09,079: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom_args>
airflow-worker-1     | [2024-12-13 14:42:09,079: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensors.py
airflow-worker-1     | [2024-12-13 14:42:09,085: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sensors>
airflow-worker-1     | [2024-12-13 14:42:09,085: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
airflow-worker-1     | [2024-12-13 14:42:09,087: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: latest_only_with_trigger>
airflow-worker-1     | [2024-12-13 14:42:09,087: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:42:09,089: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_local_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:42:09,090: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_nested_branch_dag.py
airflow-worker-1     | [2024-12-13 14:42:09,090: DEBUG/ForkPoolWorker-15] Failed to find locale C
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:15:06.89 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:15:06.91 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:15:06.91 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:15:06.91 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:15:06.92 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:13019 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:13090 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:13139 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:13200 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:13886 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:13941 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:13977 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:14026 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:14318 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:14388 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:14436 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:14492 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:14744 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:14810 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:15415 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:15482 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:15527 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:15578 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:15897 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:15953 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:16617 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:16674 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:16712 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:16780 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:16932 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:16988 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:17025 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:17090 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:17228 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:17284 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:20234 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:20289 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:20862 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:20921 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:20960 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:21014 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:21316 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:21377 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:21418 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:21478 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:21612 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:21668 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:22155 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:22218 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:22559 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:22626 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:22671 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:22722 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:23061 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:23128 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:23173 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:23224 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:23511 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:23567 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:23968 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:24018 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:25343 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:25398 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:25434 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:25485 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:26723 SyntaxWarning: invalid escape sequence '\d'
postgres-1           | 2024-12-13 12:14:43.763 UTC [1] LOG:  aborting any active transactions
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:15:06.92 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:15:06.93 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 12:15:08 INFO Worker: Started daemon with process name: 34@e92f47d0559d
spark-worker-2-1     | 24/12/13 12:15:08 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 12:15:08 INFO SignalUtils: Registering signal handler for HUP
spark-worker-2-1     | 24/12/13 12:15:08 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 12:15:08 INFO SecurityManager: Changing view acls to: spark
redis-1              | 1:M 13 Dec 2024 13:57:29.958 * RDB memory usage when created 1.40 Mb
spark-worker-3-1     | 24/12/13 12:00:32 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 12:00:32 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 12:00:32 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 12:00:32 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 12:00:32 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 12:00:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 12:00:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 12:00:33 INFO Utils: Successfully started service 'sparkWorker' on port 33905.
spark-worker-3-1     | 24/12/13 12:00:33 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 12:00:33 INFO Worker: Starting Spark worker 172.18.0.6:33905 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 12:00:33 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 12:00:33 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 12:00:33 INFO ResourceUtils: ==============================================================
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:26779 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:27121 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:27177 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:27461 SyntaxWarning: invalid escape sequence '\d'
spark-worker-2-1     | 24/12/13 12:15:08 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 12:15:08 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 12:15:08 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 12:15:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 12:15:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:27517 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:28053 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:28109 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:28536 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:28592 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:28932 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:28982 SyntaxWarning: invalid escape sequence '\d'
postgres-1           | 2024-12-13 12:14:43.765 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 12:14:43.766 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 12:14:43.783 UTC [1] LOG:  database system is shut down
airflow-worker-1     | [2024-12-13 14:42:09,091: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,091: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,092: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,092: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,092: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
redis-1              | 1:M 13 Dec 2024 13:57:29.958 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 13:57:29.958 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 13:57:29.958 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734098349) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 13:59:09.663 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 13:59:09.663 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 13:59:09.665 * DB saved on disk
spark-worker-2-1     | 24/12/13 12:15:08 INFO Utils: Successfully started service 'sparkWorker' on port 35637.
spark-worker-2-1     | 24/12/13 12:15:08 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 12:15:09 INFO Worker: Starting Spark worker 172.18.0.9:35637 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 12:15:09 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 12:15:09 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 12:15:09 INFO ResourceUtils: ==============================================================
redis-1              | 1:M 13 Dec 2024 13:59:09.665 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 14:00:09.818 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 14:00:09.818 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 14:00:09.818 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 14:00:09.818 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 14:00:09.822 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 14:00:09.823 * Server initialized
redis-1              | 1:M 13 Dec 2024 14:00:09.823 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 14:00:09.823 * RDB age 60 seconds
redis-1              | 1:M 13 Dec 2024 14:00:09.823 * RDB memory usage when created 1.40 Mb
spark-master-1       | 24/12/13 12:14:17 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 12:14:17 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 12:14:17 INFO Master: Registering worker 172.18.0.7:40817 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 12:00:33 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 12:00:33 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 12:00:33 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 12:00:33 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 12:00:33 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-3-1     | 24/12/13 12:00:33 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 12:00:33 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 41 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 12:00:34 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
airflow-worker-1     | [2024-12-13 14:42:09,092: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_nested_branch_dag>
airflow-worker-1     | [2024-12-13 14:42:09,093: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py
airflow-worker-1     | [2024-12-13 14:42:09,094: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dynamic_task_mapping_with_no_taskflow_operators>
airflow-worker-1     | [2024-12-13 14:42:09,094: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_operator.py
spark-worker-2-1     | 24/12/13 12:15:09 INFO ResourceUtils: No custom resources configured for spark.worker.
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:30727 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:30783 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:31296 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:31344 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:31617 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:31673 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:32120 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:32176 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:32339 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:32395 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:32432 SyntaxWarning: invalid escape sequence '\d'
airflow-worker-1     | [2024-12-13 14:42:09,095: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:32487 SyntaxWarning: invalid escape sequence '\d'
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:14:13.96 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:14:13.97 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:14:13.97 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:14:13.97 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:14:13.97 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:14:13.98 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:14:14.00 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:14:14.00 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:14:14.00 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:14:14.01 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 12:14:16 INFO Worker: Started daemon with process name: 34@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 12:14:16 INFO SignalUtils: Registering signal handler for TERM
spark-worker-3-1     | 24/12/13 12:14:16 INFO SignalUtils: Registering signal handler for HUP
airflow-scheduler-1  | [2024-12-13T14:44:29.709+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 438)
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-3-1     | 24/12/13 12:14:16 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 12:14:16 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 12:14:16 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 12:15:09 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 12:15:09 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 12:15:09 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 12:15:09 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 12:15:09 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 12:15:09 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 38 ms (0 ms spent in bootstraps)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:44:56] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T14:45:00.121+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 502)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:45:26] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T14:45:30.479+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 566)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:45:56] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T14:46:00.776+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 632)
airflow-scheduler-1  | [2024-12-13T14:46:26.088+0000] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:46:26] "GET /health HTTP/1.1" 200 -
spark-worker-2-1     | 24/12/13 12:15:09 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 12:15:57 INFO Worker: Asked to launch executor app-20241213121557-0000/1 for MyAwesomeSpark
spark-worker-2-1     | 24/12/13 12:15:57 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 12:15:57 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 12:15:57 INFO SecurityManager: Changing view acls groups to: 
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:33502 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:33558 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:33869 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:33928 SyntaxWarning: invalid escape sequence '\d'
spark-master-1       | 24/12/13 12:14:17 INFO Master: Registering worker 172.18.0.6:36271 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 12:14:17 INFO Master: Registering worker 172.18.0.8:39261 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 12:14:47 INFO Master: 172.18.0.7:41958 got disassociated, removing it.
spark-master-1       | 24/12/13 12:14:47 INFO Master: 172.18.0.8:41880 got disassociated, removing it.
spark-master-1       | 24/12/13 12:14:47 INFO Master: 172.18.0.7:40817 got disassociated, removing it.
spark-master-1       | 24/12/13 12:14:47 INFO Master: Removing worker worker-20241213121416-172.18.0.7-40817 on 172.18.0.7:40817
spark-master-1       | 24/12/13 12:14:47 INFO Master: Telling app of lost worker: worker-20241213121416-172.18.0.7-40817
spark-master-1       | 24/12/13 12:14:47 INFO Master: 172.18.0.6:48862 got disassociated, removing it.
spark-master-1       | 24/12/13 12:14:47 INFO Master: 172.18.0.6:36271 got disassociated, removing it.
spark-master-1       | 24/12/13 12:14:47 INFO Master: Removing worker worker-20241213121416-172.18.0.6-36271 on 172.18.0.6:36271
spark-master-1       | 24/12/13 12:14:47 INFO Master: Telling app of lost worker: worker-20241213121416-172.18.0.6-36271
spark-master-1       | 24/12/13 12:14:47 INFO Master: 172.18.0.8:39261 got disassociated, removing it.
spark-master-1       | 24/12/13 12:14:47 INFO Master: Removing worker worker-20241213121416-172.18.0.8-39261 on 172.18.0.8:39261
spark-master-1       | 24/12/13 12:14:47 INFO Master: Telling app of lost worker: worker-20241213121416-172.18.0.8-39261
spark-master-1       | [38;5;6mspark [38;5;5m12:15:06.64 [0m[38;5;2mINFO [0m ==> 
airflow-worker-1     | [2024-12-13 14:42:09,095: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,095: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-worker-2-1     | 24/12/13 12:15:57 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 12:15:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 12:15:57 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33895" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@116cd888dcb9:33895" "--executor-id" "1" "--hostname" "172.18.0.9" "--cores" "10" "--app-id" "app-20241213121557-0000" "--worker-url" "spark://Worker@172.18.0.9:35637" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 12:15:59 INFO Worker: Asked to kill executor app-20241213121557-0000/1
spark-worker-3-1     | 24/12/13 12:14:16 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 12:14:16 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 12:14:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 12:14:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 12:14:16 INFO Utils: Successfully started service 'sparkWorker' on port 39261.
spark-worker-3-1     | 24/12/13 12:14:16 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 12:14:16 INFO Worker: Starting Spark worker 172.18.0.8:39261 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 12:14:16 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 12:14:16 INFO Worker: Spark home: /opt/bitnami/spark
airflow-scheduler-1  | [2024-12-13T14:46:31.913+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 694)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:46:56] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T14:46:58.807+0000] {scheduler_job_runner.py:260} INFO - Exiting gracefully upon receiving signal 15
airflow-scheduler-1  | [2024-12-13T14:46:59.818+0000] {process_utils.py:132} INFO - Sending 15 to group 47. PIDs of all processes in the group: [47]
airflow-scheduler-1  | [2024-12-13T14:46:59.820+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 47
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:33967 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:34021 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:34739 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:34806 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:34852 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:34908 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:36915 SyntaxWarning: invalid escape sequence '\d'
redis-1              | 1:M 13 Dec 2024 14:00:09.823 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 14:00:09.823 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 14:00:09.823 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734098471) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 14:01:11.475 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 14:01:11.475 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 14:01:11.482 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 14:01:11.482 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 14:02:40.070 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 14:02:40.070 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 14:02:40.070 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 14:02:40.071 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 14:02:40.072 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 14:02:40.072 * Server initialized
redis-1              | 1:M 13 Dec 2024 14:02:40.072 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 14:02:40.072 * RDB age 89 seconds
redis-1              | 1:M 13 Dec 2024 14:02:40.072 * RDB memory usage when created 1.40 Mb
redis-1              | 1:M 13 Dec 2024 14:02:40.072 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 14:02:40.073 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 14:02:40.073 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734098664) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 14:04:24.688 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 14:04:24.688 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 14:04:24.692 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 14:04:24.692 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 14:05:10.123 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 14:05:10.123 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 14:05:10.123 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 14:05:10.124 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 14:05:10.125 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 14:05:10.126 * Server initialized
redis-1              | 1:M 13 Dec 2024 14:05:10.126 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 14:05:10.126 * RDB age 46 seconds
redis-1              | 1:M 13 Dec 2024 14:05:10.126 * RDB memory usage when created 1.31 Mb
redis-1              | 1:M 13 Dec 2024 14:05:10.126 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 14:05:10.126 * DB loaded from disk: 0.000 seconds
airflow-scheduler-1  | [2024-12-13T14:47:00.758+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=47, status='terminated', exitcode=0, started='14:41:25') (47) terminated with exit code 0
airflow-scheduler-1  | [2024-12-13T14:47:00.761+0000] {process_utils.py:132} INFO - Sending 15 to group 47. PIDs of all processes in the group: []
airflow-scheduler-1  | [2024-12-13T14:47:00.761+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 47
airflow-scheduler-1  | [2024-12-13T14:47:00.761+0000] {process_utils.py:101} INFO - Sending the signal 15 to process 47 as process group is missing.
airflow-scheduler-1  | [2024-12-13T14:47:00.761+0000] {scheduler_job_runner.py:1017} INFO - Exited execute loop
airflow-scheduler-1  | 
airflow-scheduler-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-scheduler-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-scheduler-1  | BACKEND=redis
airflow-scheduler-1  | DB_HOST=redis
airflow-scheduler-1  | DB_PORT=6379
airflow-scheduler-1  | 
airflow-scheduler-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-scheduler-1  |   ____________       _____________
airflow-scheduler-1  |  ____    |__( )_________  __/__  /________      __
airflow-scheduler-1  | ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
airflow-scheduler-1  | ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
airflow-scheduler-1  |  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
airflow-scheduler-1  | [2024-12-13T14:57:22.234+0000] {_client.py:1026} INFO - HTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.3&python_version=3.12&platform=Linux&arch=aarch64&database=postgresql&db_version=13.18&executor=CeleryExecutor "HTTP/1.1 200 OK"
airflow-scheduler-1  | [2024-12-13T14:57:23.119+0000] {executor_loader.py:254} INFO - Loaded executor: CeleryExecutor
airflow-scheduler-1  | [2024-12-13T14:57:23.158+0000] {scheduler_job_runner.py:938} INFO - Starting the scheduler
airflow-scheduler-1  | [2024-12-13T14:57:23.160+0000] {scheduler_job_runner.py:945} INFO - Processing each file at most -1 times
airflow-scheduler-1  | [2024-12-13T14:57:23.169+0000] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 46
airflow-scheduler-1  | [2024-12-13T14:57:23.173+0000] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
airflow-scheduler-1  | [2024-12-13T14:57:23.179+0000] {settings.py:63} INFO - Configured default timezone UTC
airflow-scheduler-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-scheduler-1  | [2024-12-13T14:57:23.846+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 47)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:57:27] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T14:57:54.858+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 111)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:57:57] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | Dag run  in running state
airflow-scheduler-1  | Dag information Queued at: 2024-12-13 14:58:11.763585+00:00 hash info: b7cb26b06dc6445439e201c41d451bd8
airflow-scheduler-1  | [2024-12-13T14:58:12.677+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
airflow-scheduler-1  | 	<TaskInstance: pipeline.download_and_unwrap manual__2024-12-13T14:58:11.753084+00:00 [scheduled]>
spark-master-1       | [38;5;6mspark [38;5;5m12:15:06.65 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:15:06.65 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:15:06.65 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:15:06.65 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m12:15:06.66 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
airflow-scheduler-1  | [2024-12-13T14:58:12.678+0000] {scheduler_job_runner.py:495} INFO - DAG pipeline has 0/16 running and queued tasks
airflow-scheduler-1  | [2024-12-13T14:58:12.678+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
airflow-scheduler-1  | 	<TaskInstance: pipeline.download_and_unwrap manual__2024-12-13T14:58:11.753084+00:00 [scheduled]>
airflow-scheduler-1  | [2024-12-13T14:58:12.680+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: pipeline.download_and_unwrap manual__2024-12-13T14:58:11.753084+00:00 [scheduled]>] for executor: CeleryExecutor(parallelism=32)
airflow-scheduler-1  | [2024-12-13T14:58:12.680+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='pipeline', task_id='download_and_unwrap', run_id='manual__2024-12-13T14:58:11.753084+00:00', try_number=1, map_index=-1) to CeleryExecutor with priority 4 and queue default
airflow-scheduler-1  | [2024-12-13T14:58:12.681+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'pipeline', 'download_and_unwrap', 'manual__2024-12-13T14:58:11.753084+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-scheduler-1  | [2024-12-13T14:58:12.763+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state queued for task instance TaskInstanceKey(dag_id='pipeline', task_id='download_and_unwrap', run_id='manual__2024-12-13T14:58:11.753084+00:00', try_number=1, map_index=-1)
airflow-scheduler-1  | [2024-12-13T14:58:12.769+0000] {scheduler_job_runner.py:791} INFO - Setting external_id for <TaskInstance: pipeline.download_and_unwrap manual__2024-12-13T14:58:11.753084+00:00 [queued]> to 9b98a5c2-6d78-4f62-bdd3-7cc49598db0e
airflow-scheduler-1  | [2024-12-13T14:58:18.612+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
airflow-scheduler-1  | 	<TaskInstance: pipeline.split manual__2024-12-13T14:58:11.753084+00:00 [scheduled]>
spark-master-1       | [38;5;6mspark [38;5;5m12:15:06.67 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m12:15:06.67 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | 
airflow-scheduler-1  | [2024-12-13T14:58:18.613+0000] {scheduler_job_runner.py:495} INFO - DAG pipeline has 0/16 running and queued tasks
airflow-scheduler-1  | [2024-12-13T14:58:18.613+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
airflow-scheduler-1  | 	<TaskInstance: pipeline.split manual__2024-12-13T14:58:11.753084+00:00 [scheduled]>
airflow-scheduler-1  | [2024-12-13T14:58:18.614+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: pipeline.split manual__2024-12-13T14:58:11.753084+00:00 [scheduled]>] for executor: CeleryExecutor(parallelism=32)
airflow-worker-1     | [2024-12-13 14:42:09,096: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:42:09,096: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:42:09,097: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator.py
spark-worker-3-1     | 24/12/13 12:14:16 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 12:14:16 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 12:14:16 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 12:14:17 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 12:14:17 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 12:14:17 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-3-1     | 24/12/13 12:14:17 INFO Worker: Connecting to master spark-master:7077...
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 12:15:06.627 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
spark-worker-2-1     | 24/12/13 12:15:59 INFO ExecutorRunner: Runner thread for executor app-20241213121557-0000/1 interrupted
spark-worker-2-1     | 24/12/13 12:15:59 INFO ExecutorRunner: Killing process!
spark-worker-2-1     | 24/12/13 12:15:59 INFO Worker: Executor app-20241213121557-0000/1 finished with state KILLED exitStatus 143
spark-worker-3-1     | 24/12/13 12:14:17 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 71 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 12:14:17 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:15:06.88 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:15:06.88 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:15:06.67 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | [38;5;6mspark [38;5;5m12:15:06.68 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 12:15:08 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-master-1       | 24/12/13 12:15:08 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 12:15:08 INFO SignalUtils: Registering signal handler for HUP
redis-1              | 1:M 13 Dec 2024 14:05:10.126 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734099049) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 14:10:49.410 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 14:10:49.410 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 14:10:49.413 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 14:10:49.413 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 14:40:53.697 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 14:40:53.697 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 14:40:53.697 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 14:40:53.697 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 14:40:53.701 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 14:40:53.702 * Server initialized
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:15:06.88 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:15:06.88 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:15:06.88 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:15:06.89 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
airflow-scheduler-1  | [2024-12-13T14:58:18.615+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='pipeline', task_id='split', run_id='manual__2024-12-13T14:58:11.753084+00:00', try_number=1, map_index=-1) to CeleryExecutor with priority 3 and queue default
airflow-scheduler-1  | [2024-12-13T14:58:18.615+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'pipeline', 'split', 'manual__2024-12-13T14:58:11.753084+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-scheduler-1  | [2024-12-13T14:58:18.628+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state queued for task instance TaskInstanceKey(dag_id='pipeline', task_id='split', run_id='manual__2024-12-13T14:58:11.753084+00:00', try_number=1, map_index=-1)
airflow-scheduler-1  | [2024-12-13T14:58:18.629+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='pipeline', task_id='download_and_unwrap', run_id='manual__2024-12-13T14:58:11.753084+00:00', try_number=1, map_index=-1)
airflow-scheduler-1  | [2024-12-13T14:58:18.633+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=pipeline, task_id=download_and_unwrap, run_id=manual__2024-12-13T14:58:11.753084+00:00, map_index=-1, run_start_date=2024-12-13 14:58:14.258784+00:00, run_end_date=2024-12-13 14:58:17.724751+00:00, run_duration=3.465967, state=success, executor=CeleryExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=180, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-12-13 14:58:12.679073+00:00, queued_by_job_id=179, pid=77
airflow-scheduler-1  | [2024-12-13T14:58:18.634+0000] {scheduler_job_runner.py:791} INFO - Setting external_id for <TaskInstance: pipeline.split manual__2024-12-13T14:58:11.753084+00:00 [queued]> to e007c70a-3a5f-41f7-a4b0-ed22818811d7
airflow-scheduler-1  | [2024-12-13T14:58:21.894+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
airflow-scheduler-1  | 	<TaskInstance: pipeline.upload_to_minio manual__2024-12-13T14:58:11.753084+00:00 [scheduled]>
airflow-scheduler-1  | [2024-12-13T14:58:21.895+0000] {scheduler_job_runner.py:495} INFO - DAG pipeline has 0/16 running and queued tasks
airflow-scheduler-1  | [2024-12-13T14:58:21.895+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
airflow-scheduler-1  | 	<TaskInstance: pipeline.upload_to_minio manual__2024-12-13T14:58:11.753084+00:00 [scheduled]>
airflow-worker-1     | [2024-12-13 14:42:09,097: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-scheduler-1  | [2024-12-13T14:58:21.897+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: pipeline.upload_to_minio manual__2024-12-13T14:58:11.753084+00:00 [scheduled]>] for executor: CeleryExecutor(parallelism=32)
airflow-scheduler-1  | [2024-12-13T14:58:21.897+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='pipeline', task_id='upload_to_minio', run_id='manual__2024-12-13T14:58:11.753084+00:00', try_number=1, map_index=-1) to CeleryExecutor with priority 2 and queue default
airflow-scheduler-1  | [2024-12-13T14:58:21.898+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'pipeline', 'upload_to_minio', 'manual__2024-12-13T14:58:11.753084+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-scheduler-1  | [2024-12-13T14:58:21.916+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state queued for task instance TaskInstanceKey(dag_id='pipeline', task_id='upload_to_minio', run_id='manual__2024-12-13T14:58:11.753084+00:00', try_number=1, map_index=-1)
airflow-scheduler-1  | [2024-12-13T14:58:21.917+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='pipeline', task_id='split', run_id='manual__2024-12-13T14:58:11.753084+00:00', try_number=1, map_index=-1)
airflow-scheduler-1  | [2024-12-13T14:58:21.920+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=pipeline, task_id=split, run_id=manual__2024-12-13T14:58:11.753084+00:00, map_index=-1, run_start_date=2024-12-13 14:58:21.094654+00:00, run_end_date=2024-12-13 14:58:21.462050+00:00, run_duration=0.367396, state=success, executor=CeleryExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=181, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-12-13 14:58:18.613985+00:00, queued_by_job_id=179, pid=80
airflow-scheduler-1  | [2024-12-13T14:58:21.922+0000] {scheduler_job_runner.py:791} INFO - Setting external_id for <TaskInstance: pipeline.upload_to_minio manual__2024-12-13T14:58:11.753084+00:00 [queued]> to 1cb55bd1-bdad-4269-8673-d5be69eb2fb7
airflow-scheduler-1  | [2024-12-13T14:58:23.674+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
airflow-scheduler-1  | 	<TaskInstance: pipeline.train_and_predict manual__2024-12-13T14:58:11.753084+00:00 [scheduled]>
airflow-scheduler-1  | [2024-12-13T14:58:23.674+0000] {scheduler_job_runner.py:495} INFO - DAG pipeline has 0/16 running and queued tasks
airflow-scheduler-1  | [2024-12-13T14:58:23.674+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
airflow-scheduler-1  | 	<TaskInstance: pipeline.train_and_predict manual__2024-12-13T14:58:11.753084+00:00 [scheduled]>
airflow-scheduler-1  | [2024-12-13T14:58:23.675+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: pipeline.train_and_predict manual__2024-12-13T14:58:11.753084+00:00 [scheduled]>] for executor: CeleryExecutor(parallelism=32)
airflow-scheduler-1  | [2024-12-13T14:58:23.676+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='pipeline', task_id='train_and_predict', run_id='manual__2024-12-13T14:58:11.753084+00:00', try_number=1, map_index=-1) to CeleryExecutor with priority 1 and queue default
airflow-scheduler-1  | [2024-12-13T14:58:23.676+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'pipeline', 'train_and_predict', 'manual__2024-12-13T14:58:11.753084+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-scheduler-1  | [2024-12-13T14:58:23.685+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state queued for task instance TaskInstanceKey(dag_id='pipeline', task_id='train_and_predict', run_id='manual__2024-12-13T14:58:11.753084+00:00', try_number=1, map_index=-1)
airflow-scheduler-1  | [2024-12-13T14:58:23.689+0000] {scheduler_job_runner.py:791} INFO - Setting external_id for <TaskInstance: pipeline.train_and_predict manual__2024-12-13T14:58:11.753084+00:00 [queued]> to 62403189-caef-43d6-a734-72b5db033277
airflow-worker-1     | [2024-12-13 14:42:09,097: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
postgres-1           | 2024-12-13 12:15:06.629 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 12:15:06.629 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 12:15:06.638 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 12:15:06.653 UTC [27] LOG:  database system was shut down at 2024-12-13 12:14:43 UTC
postgres-1           | 2024-12-13 12:15:06.670 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 12:21:44.338 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 12:21:44.339 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 12:21:44.342 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 12:21:44.342 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 12:21:44.354 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 12:22:21.737 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 12:22:21.737 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 12:22:21.737 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 12:22:21.743 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
airflow-scheduler-1  | [2024-12-13T14:58:23.741+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='pipeline', task_id='upload_to_minio', run_id='manual__2024-12-13T14:58:11.753084+00:00', try_number=1, map_index=-1)
redis-1              | 1:M 13 Dec 2024 14:40:53.702 * Loading RDB produced by version 7.2.6
airflow-scheduler-1  | [2024-12-13T14:58:23.744+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=pipeline, task_id=upload_to_minio, run_id=manual__2024-12-13T14:58:11.753084+00:00, map_index=-1, run_start_date=2024-12-13 14:58:23.256994+00:00, run_end_date=2024-12-13 14:58:23.622471+00:00, run_duration=0.365477, state=success, executor=CeleryExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=182, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-12-13 14:58:21.896254+00:00, queued_by_job_id=179, pid=83
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-master-1       | 24/12/13 12:15:08 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 12:15:08 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | 24/12/13 12:15:08 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 12:15:08 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 12:15:59 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-worker-2-1     | 24/12/13 12:15:59 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213121557-0000, execId=1)
spark-worker-2-1     | 24/12/13 12:15:59 INFO ExternalShuffleBlockResolver: Application app-20241213121557-0000 removed, cleanupLocalDirs = true
spark-worker-2-1     | 24/12/13 12:15:59 INFO Worker: Cleaning up local directories for application app-20241213121557-0000
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:22:21.93 [0m[38;5;2mINFO [0m ==> 
airflow-scheduler-1  | [2024-12-13T14:58:26.174+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 178)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:58:28] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T14:58:36.006+0000] {dagrun.py:854} INFO - Marking run <DagRun pipeline @ 2024-12-13 14:58:11.753084+00:00: manual__2024-12-13T14:58:11.753084+00:00, state:running, queued_at: 2024-12-13 14:58:11.763585+00:00. externally triggered: True> successful
airflow-scheduler-1  | Dag run in success state
airflow-scheduler-1  | Dag run start:2024-12-13 14:58:12.642406+00:00 end:2024-12-13 14:58:36.008604+00:00
airflow-scheduler-1  | [2024-12-13T14:58:36.008+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=pipeline, execution_date=2024-12-13 14:58:11.753084+00:00, run_id=manual__2024-12-13T14:58:11.753084+00:00, run_start_date=2024-12-13 14:58:12.642406+00:00, run_end_date=2024-12-13 14:58:36.008604+00:00, run_duration=23.366198, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-12-13 14:58:11.753084+00:00, data_interval_end=2024-12-13 14:58:11.753084+00:00, dag_hash=b7cb26b06dc6445439e201c41d451bd8
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 12:15:08 INFO Worker: Started daemon with process name: 34@f484bf7bba3e
spark-worker-1-1     | 24/12/13 12:15:08 INFO SignalUtils: Registering signal handler for TERM
airflow-scheduler-1  | [2024-12-13T14:58:36.020+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='pipeline', task_id='train_and_predict', run_id='manual__2024-12-13T14:58:11.753084+00:00', try_number=1, map_index=-1)
redis-1              | 1:M 13 Dec 2024 14:40:53.702 * RDB age 1804 seconds
redis-1              | 1:M 13 Dec 2024 14:40:53.702 * RDB memory usage when created 1.40 Mb
redis-1              | 1:M 13 Dec 2024 14:40:53.702 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 14:40:53.702 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 14:40:53.702 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734101223) Received SIGTERM scheduling shutdown...
redis-1              | 1:M 13 Dec 2024 14:47:03.904 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 14:47:03.904 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 14:47:03.924 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 14:47:03.924 # Redis is now ready to exit, bye bye...
redis-1              | 1:C 13 Dec 2024 14:56:59.413 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
redis-1              | 1:C 13 Dec 2024 14:56:59.413 * Redis version=7.2.6, bits=64, commit=00000000, modified=0, pid=1, just started
redis-1              | 1:C 13 Dec 2024 14:56:59.413 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
redis-1              | 1:M 13 Dec 2024 14:56:59.413 * monotonic clock: POSIX clock_gettime
redis-1              | 1:M 13 Dec 2024 14:56:59.428 * Running mode=standalone, port=6379.
redis-1              | 1:M 13 Dec 2024 14:56:59.429 * Server initialized
redis-1              | 1:M 13 Dec 2024 14:56:59.430 * Loading RDB produced by version 7.2.6
redis-1              | 1:M 13 Dec 2024 14:56:59.430 * RDB age 596 seconds
redis-1              | 1:M 13 Dec 2024 14:56:59.430 * RDB memory usage when created 1.40 Mb
redis-1              | 1:M 13 Dec 2024 14:56:59.430 * Done loading RDB, keys loaded: 1, keys expired: 0.
redis-1              | 1:M 13 Dec 2024 14:56:59.430 * DB loaded from disk: 0.000 seconds
redis-1              | 1:M 13 Dec 2024 14:56:59.430 * Ready to accept connections tcp
redis-1              | 1:signal-handler (1734102127) Received SIGTERM scheduling shutdown...
spark-worker-1-1     | 24/12/13 12:15:08 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 12:15:08 INFO SignalUtils: Registering signal handler for INT
postgres-1           | 2024-12-13 12:22:21.752 UTC [27] LOG:  database system was shut down at 2024-12-13 12:21:44 UTC
postgres-1           | 2024-12-13 12:22:21.798 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 12:29:23.860 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 12:29:23.862 UTC [1] LOG:  aborting any active transactions
spark-master-1       | 24/12/13 12:15:08 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 12:15:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 12:15:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 12:15:08 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
postgres-1           | 2024-12-13 12:29:23.865 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 12:29:23.865 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 12:29:23.879 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 12:30:09.742 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
redis-1              | 1:M 13 Dec 2024 15:02:07.463 * User requested shutdown...
redis-1              | 1:M 13 Dec 2024 15:02:07.463 * Saving the final RDB snapshot before exiting.
redis-1              | 1:M 13 Dec 2024 15:02:07.470 * DB saved on disk
redis-1              | 1:M 13 Dec 2024 15:02:07.470 # Redis is now ready to exit, bye bye...
postgres-1           | 2024-12-13 12:30:09.743 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 12:30:09.743 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 12:30:09.749 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 12:30:09.786 UTC [27] LOG:  database system was shut down at 2024-12-13 12:29:23 UTC
postgres-1           | 2024-12-13 12:30:09.798 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 12:38:57.954 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 12:38:57.957 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 12:38:57.962 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 12:38:57.965 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 12:38:57.987 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 13:02:08.905 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 13:02:08.905 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
airflow-worker-1     | [2024-12-13 14:42:09,097: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,100: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,100: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,100: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,100: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_operator>
airflow-worker-1     | [2024-12-13 14:42:09,100: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_passing_params_via_test_command.py
airflow-worker-1     | [2024-12-13 14:42:09,101: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,101: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:36971 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:37417 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:37473 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:37845 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:37901 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:38208 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:38264 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:38301 SyntaxWarning: invalid escape sequence '\d'
airflow-worker-1     | [2024-12-13 14:42:09,101: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,102: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_passing_params_via_test_command>
airflow-worker-1     | [2024-12-13 14:42:09,102: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_objectstorage.py
airflow-worker-1     | [2024-12-13 14:42:09,104: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_objectstorage>
airflow-worker-1     | [2024-12-13 14:42:09,104: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_decorator.py
airflow-worker-1     | [2024-12-13 14:42:09,106: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_python_decorator>
airflow-worker-1     | [2024-12-13 14:42:09,106: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_trigger_ui.py
airflow-worker-1     | [2024-12-13 14:42:09,109: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_params_trigger_ui>
airflow-worker-1     | [2024-12-13 14:42:09,109: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_ui_tutorial.py
airflow-worker-1     | [2024-12-13 14:42:09,111: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_params_ui_tutorial>
postgres-1           | 2024-12-13 13:02:08.905 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 13:02:08.911 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 13:02:08.919 UTC [27] LOG:  database system was shut down at 2024-12-13 12:38:57 UTC
postgres-1           | 2024-12-13 13:02:08.928 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 13:02:12.217 UTC [34] LOG:  could not receive data from client: Connection reset by peer
postgres-1           | 2024-12-13 13:02:12.217 UTC [34] LOG:  unexpected EOF on client connection with an open transaction
postgres-1           | 2024-12-13 13:06:50.234 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 13:06:50.240 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 13:06:50.242 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 13:06:50.243 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 13:06:50.277 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 13:07:15.817 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 13:07:15.817 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 13:07:15.817 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 13:07:15.834 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 13:07:15.847 UTC [27] LOG:  database system was shut down at 2024-12-13 13:06:50 UTC
postgres-1           | 2024-12-13 13:07:15.877 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 13:13:39.799 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 13:13:39.816 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 13:13:39.839 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 13:13:39.839 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 13:13:39.859 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 13:22:10.645 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 13:22:10.646 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 13:22:10.646 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 13:22:10.653 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 13:22:10.691 UTC [27] LOG:  database system was shut down at 2024-12-13 13:13:39 UTC
postgres-1           | 2024-12-13 13:22:10.712 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 13:32:38.231 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 13:32:38.233 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 13:32:38.235 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 13:32:38.237 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 13:32:38.263 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 13:33:04.791 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 13:33:04.793 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 13:33:04.793 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 13:33:04.804 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 13:33:04.811 UTC [27] LOG:  database system was shut down at 2024-12-13 13:32:38 UTC
postgres-1           | 2024-12-13 13:33:04.827 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 13:37:10.367 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 13:37:10.376 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 13:37:10.383 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 13:37:10.385 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 13:37:10.515 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
spark-master-1       | 24/12/13 12:15:08 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 12:15:08 INFO Master: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 12:15:08 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 12:15:08 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 12:15:08 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 12:15:08 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 12:15:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 12:15:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 12:15:08 INFO Utils: Successfully started service 'sparkWorker' on port 33653.
spark-worker-1-1     | 24/12/13 12:15:08 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 12:15:09 INFO Worker: Starting Spark worker 172.18.0.6:33653 with 10 cores, 2.0 GiB RAM
my_spark             | 
my_spark             | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
my_spark             | 
my_spark             | airflow command error: the following arguments are required: GROUP_OR_COMMAND, see help above.
my_spark             | Usage: airflow [-h] GROUP_OR_COMMAND ...
my_spark             | 
my_spark             | Positional Arguments:
my_spark             |   GROUP_OR_COMMAND
my_spark             | 
my_spark             |     Groups
my_spark             |       config            View configuration
my_spark             |       connections       Manage connections
my_spark             |       dags              Manage DAGs
my_spark             |       db                Database operations
my_spark             |       jobs              Manage jobs
my_spark             |       pools             Manage pools
my_spark             |       providers         Display providers
my_spark             |       roles             Manage roles
my_spark             |       tasks             Manage tasks
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:38351 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:38690 SyntaxWarning: invalid escape sequence '\d'
airflow-scheduler-1  | [2024-12-13T14:58:36.023+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=pipeline, task_id=train_and_predict, run_id=manual__2024-12-13T14:58:11.753084+00:00, map_index=-1, run_start_date=2024-12-13 14:58:25.035923+00:00, run_end_date=2024-12-13 14:58:35.650867+00:00, run_duration=10.614944, state=success, executor=CeleryExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=183, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-12-13 14:58:23.675203+00:00, queued_by_job_id=179, pid=86
airflow-scheduler-1  | [2024-12-13T14:58:57.350+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 244)
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:22:21.94 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:38758 SyntaxWarning: invalid escape sequence '\d'
my_spark             |       users             Manage users
my_spark             |       variables         Manage variables
my_spark             | 
my_spark             |     Commands:
my_spark             |       cheat-sheet       Display cheat sheet
my_spark             |       dag-processor     Start a standalone Dag Processor instance
my_spark             |       info              Show information about current Airflow and environment
my_spark             |       kerberos          Start a kerberos ticket renewer
my_spark             |       plugins           Dump information about loaded plugins
my_spark             |       rotate-fernet-key
my_spark             |                         Rotate encrypted connection credentials and variables
my_spark             |       scheduler         Start a scheduler instance
my_spark             |       standalone        Run an all-in-one copy of Airflow
my_spark             |       sync-perm         Update permissions for existing roles and optionally
my_spark             |                         DAGs
my_spark             |       triggerer         Start a triggerer instance
my_spark             |       version           Show the version
airflow-worker-1     | [2024-12-13 14:42:09,111: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown.py
airflow-worker-1     | [2024-12-13 14:42:09,112: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_setup_teardown>
airflow-worker-1     | [2024-12-13 14:42:09,112: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sla_dag.py
airflow-worker-1     | [2024-12-13 14:42:09,113: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,113: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,113: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,114: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sla_dag>
airflow-worker-1     | [2024-12-13 14:42:09,114: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator_decorator.py
spark-master-1       | 24/12/13 12:15:08 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 12:15:08 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 12:15:08 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:39141 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:39192 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:39419 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:39487 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:39683 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:39738 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:40019 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:40084 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:41138 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:41196 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:41359 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:41415 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:41452 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:41507 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:41875 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:41947 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:42441 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:42497 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:42534 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:42590 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:42627 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:42692 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:43020 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:43079 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:43118 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:43168 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:43514 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:43573 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:43612 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:43666 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:43989 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:44045 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:44386 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:44442 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:45003 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:45058 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:45444 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:45499 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:46042 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:46098 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:46702 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:46758 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:47000 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:47050 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:47648 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:47704 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:48113 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:48190 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:48244 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:48321 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:49499 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:49555 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:49932 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:50002 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:50052 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:50107 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:50379 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:50449 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:50499 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:50554 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:50590 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:50660 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:50710 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:50770 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:51046 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:51116 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:51166 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:51226 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:51485 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:51540 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:51787 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:51851 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:51895 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:51958 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:52212 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:52275 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:52463 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:52532 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:53040 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:53111 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:53334 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:53407 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:53961 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:54059 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:54322 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:54398 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:55022 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:55078 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:55310 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:55365 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:56196 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:56248 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:56508 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:56564 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:57087 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:57147 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:57187 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:57237 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:57560 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:57620 SyntaxWarning: invalid escape sequence '\d'
airflow-worker-1     | [2024-12-13 14:42:09,115: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,115: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,115: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,118: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,118: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,119: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,119: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_python_operator_decorator>
airflow-worker-1     | [2024-12-13 14:42:09,119: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_datasets.py
airflow-worker-1     | [2024-12-13 14:42:09,121: DEBUG/ForkPoolWorker-15] Initializing Providers Manager[dataset_uris]
airflow-worker-1     | [2024-12-13 14:42:09,860: DEBUG/ForkPoolWorker-15] Initialization of Providers Manager[dataset_uris] took 0.74 seconds
airflow-worker-1     | [2024-12-13 14:42:09,860: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,860: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,860: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,863: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,863: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,863: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,863: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,863: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,863: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,864: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_produces_1>
airflow-worker-1     | [2024-12-13 14:42:09,864: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1>
airflow-worker-1     | [2024-12-13 14:42:09,864: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_produces_2>
airflow-worker-1     | [2024-12-13 14:42:09,864: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: conditional_dataset_and_time_based_timetable>
airflow-worker-1     | [2024-12-13 14:42:09,864: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_or_2_with_dataset_expressions>
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:58:58] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T14:59:28.217+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 308)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:59:28] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 14:59:58] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T14:59:58.657+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 378)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 15:00:28] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T15:00:29.107+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 442)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 15:00:58] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T15:00:59.930+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 506)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 15:01:28] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T15:01:30.258+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 571)
airflow-scheduler-1  | 127.0.0.1 - - [13/Dec/2024 15:01:58] "GET /health HTTP/1.1" 200 -
airflow-scheduler-1  | [2024-12-13T15:02:01.079+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 635)
airflow-scheduler-1  | [2024-12-13T15:02:02.692+0000] {scheduler_job_runner.py:260} INFO - Exiting gracefully upon receiving signal 15
airflow-scheduler-1  | [2024-12-13T15:02:03.698+0000] {process_utils.py:132} INFO - Sending 15 to group 46. PIDs of all processes in the group: [46]
airflow-scheduler-1  | [2024-12-13T15:02:03.699+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 46
airflow-scheduler-1  | [2024-12-13T15:02:04.269+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=46, status='terminated', exitcode=0, started='14:57:22') (46) terminated with exit code 0
airflow-scheduler-1  | [2024-12-13T15:02:04.271+0000] {process_utils.py:132} INFO - Sending 15 to group 46. PIDs of all processes in the group: []
airflow-scheduler-1  | [2024-12-13T15:02:04.271+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 46
airflow-scheduler-1  | [2024-12-13T15:02:04.271+0000] {process_utils.py:101} INFO - Sending the signal 15 to process 46 as process group is missing.
airflow-scheduler-1  | [2024-12-13T15:02:04.272+0000] {scheduler_job_runner.py:1017} INFO - Exited execute loop
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:22:21.94 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:22:21.94 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:22:21.94 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:22:21.95 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:22:21.98 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:22:21.98 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:22:21.98 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:22:22.00 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 12:22:23 INFO Worker: Started daemon with process name: 34@e92f47d0559d
spark-worker-2-1     | 24/12/13 12:22:23 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 12:22:23 INFO SignalUtils: Registering signal handler for HUP
spark-worker-2-1     | 24/12/13 12:22:23 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 12:22:23 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 12:22:23 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 12:22:23 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 12:22:23 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 12:22:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
my_spark             |       webserver         Start a Airflow webserver instance
my_spark             | 
my_spark             | Options:
my_spark             |   -h, --help            show this help message and exit
my_spark             | 
my_spark             | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
my_spark             | Usage: airflow [-h] GROUP_OR_COMMAND ...
my_spark             | 
my_spark             | Positional Arguments:
my_spark             |   GROUP_OR_COMMAND
my_spark             | 
my_spark             |     Groups
airflow-worker-1     | [2024-12-13 14:42:09,864: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1_and_2>
airflow-worker-1     | [2024-12-13 14:42:09,864: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_unknown_never_scheduled>
airflow-worker-1     | [2024-12-13 14:42:09,864: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_or_both_2_and_3_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:42:09,864: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1_never_scheduled>
airflow-worker-1     | [2024-12-13 14:42:09,864: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_and_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:42:09,865: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_outlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:42:09,866: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,867: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,867: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,867: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,867: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,867: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,868: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,868: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,868: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,868: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,868: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,868: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,868: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_by_context>
airflow-worker-1     | [2024-12-13 14:42:09,868: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,869: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,869: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,869: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_by_yield>
airflow-worker-1     | [2024-12-13 14:42:09,869: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,869: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,869: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,869: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_from_classic_operator>
airflow-worker-1     | [2024-12-13 14:42:09,869: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_decorator.py
airflow-worker-1     | [2024-12-13 14:42:09,871: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_decorator>
airflow-worker-1     | [2024-12-13 14:42:09,872: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_decorator.py
airflow-worker-1     | [2024-12-13 14:42:09,873: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_short_circuit_decorator>
airflow-worker-1     | [2024-12-13 14:42:09,873: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_controller_dag.py
airflow-worker-1     | [2024-12-13 14:42:09,878: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_trigger_controller_dag>
airflow-worker-1     | [2024-12-13 14:42:09,878: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only.py
airflow-worker-1     | [2024-12-13 14:42:09,879: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: latest_only>
airflow-worker-1     | [2024-12-13 14:42:09,880: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_time_delta_sensor_async.py
airflow-worker-1     | [2024-12-13 14:42:09,881: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_time_delta_sensor_async>
airflow-worker-1     | [2024-12-13 14:42:09,881: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_templates.py
airflow-worker-1     | [2024-12-13 14:42:09,881: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,881: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:09,881: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,883: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:09,883: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
spark-master-1       | 24/12/13 12:15:09 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 12:15:09 INFO Master: Registering worker 172.18.0.7:40295 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 12:15:09 INFO Master: Registering worker 172.18.0.6:33653 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 12:15:09 INFO Master: Registering worker 172.18.0.9:35637 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 12:15:57 INFO Master: Registering app MyAwesomeSpark
spark-master-1       | 24/12/13 12:15:57 INFO Master: Registered app MyAwesomeSpark with ID app-20241213121557-0000
spark-master-1       | 24/12/13 12:15:57 INFO Master: Start scheduling for app app-20241213121557-0000 with rpId: 0
spark-master-1       | 24/12/13 12:15:57 INFO Master: Launching executor app-20241213121557-0000/0 on worker worker-20241213121508-172.18.0.7-40295
spark-master-1       | 24/12/13 12:15:57 INFO Master: Launching executor app-20241213121557-0000/1 on worker worker-20241213121508-172.18.0.9-35637
spark-master-1       | 24/12/13 12:15:57 INFO Master: Launching executor app-20241213121557-0000/2 on worker worker-20241213121508-172.18.0.6-33653
spark-master-1       | 24/12/13 12:15:57 INFO Master: Start scheduling for app app-20241213121557-0000 with rpId: 0
spark-master-1       | 24/12/13 12:15:57 INFO Master: Start scheduling for app app-20241213121557-0000 with rpId: 0
spark-master-1       | 24/12/13 12:15:57 INFO Master: Start scheduling for app app-20241213121557-0000 with rpId: 0
spark-master-1       | 24/12/13 12:15:59 INFO Master: Received unregister request from application app-20241213121557-0000
spark-master-1       | 24/12/13 12:15:59 INFO Master: Removing app app-20241213121557-0000
spark-master-1       | 24/12/13 12:15:59 INFO Master: 172.18.0.8:44386 got disassociated, removing it.
spark-master-1       | 24/12/13 12:15:59 INFO Master: 116cd888dcb9:33895 got disassociated, removing it.
spark-master-1       | 24/12/13 12:15:59 WARN Master: Got status update for unknown executor app-20241213121557-0000/1
spark-master-1       | 24/12/13 12:15:59 WARN Master: Got status update for unknown executor app-20241213121557-0000/0
spark-master-1       | 24/12/13 12:15:59 WARN Master: Got status update for unknown executor app-20241213121557-0000/2
spark-master-1       | 24/12/13 12:21:50 INFO Master: 172.18.0.9:33160 got disassociated, removing it.
my_spark             |       config            View configuration
my_spark             |       connections       Manage connections
my_spark             |       dags              Manage DAGs
my_spark             |       db                Database operations
my_spark             |       jobs              Manage jobs
my_spark             |       pools             Manage pools
postgres-1           | 
postgres-1           | 2024-12-13 13:41:38.634 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 13:41:38.637 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 13:41:38.637 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 13:41:38.658 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 13:41:38.669 UTC [27] LOG:  database system was shut down at 2024-12-13 13:37:10 UTC
postgres-1           | 2024-12-13 13:41:38.704 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 13:49:38.710 UTC [1] LOG:  received fast shutdown request
spark-worker-2-1     | 24/12/13 12:22:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 12:22:24 INFO Utils: Successfully started service 'sparkWorker' on port 40751.
spark-worker-2-1     | 24/12/13 12:22:24 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 12:22:24 INFO Worker: Starting Spark worker 172.18.0.8:40751 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 12:22:24 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 12:22:24 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 12:22:24 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 12:22:24 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 12:22:24 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 12:22:24 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 12:22:24 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:57660 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:57710 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:59106 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:59162 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:59316 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:59396 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:59452 SyntaxWarning: invalid escape sequence '\d'
my_spark             |       providers         Display providers
my_spark             |       roles             Manage roles
airflow-worker-1     | [2024-12-13 14:42:09,883: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:09,883: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_templates>
spark-master-1       | 24/12/13 12:21:50 INFO Master: 172.18.0.9:35637 got disassociated, removing it.
spark-master-1       | 24/12/13 12:21:50 INFO Master: Removing worker worker-20241213121508-172.18.0.9-35637 on 172.18.0.9:35637
spark-master-1       | 24/12/13 12:21:50 INFO Master: Telling app of lost worker: worker-20241213121508-172.18.0.9-35637
spark-master-1       | 24/12/13 12:21:50 INFO Master: 172.18.0.7:33804 got disassociated, removing it.
spark-master-1       | 24/12/13 12:21:50 INFO Master: 172.18.0.7:40295 got disassociated, removing it.
spark-master-1       | 24/12/13 12:21:50 INFO Master: Removing worker worker-20241213121508-172.18.0.7-40295 on 172.18.0.7:40295
spark-master-1       | 24/12/13 12:21:50 INFO Master: Telling app of lost worker: worker-20241213121508-172.18.0.7-40295
spark-master-1       | 24/12/13 12:21:50 INFO Master: 172.18.0.6:42976 got disassociated, removing it.
spark-master-1       | 24/12/13 12:21:50 INFO Master: 172.18.0.6:33653 got disassociated, removing it.
spark-master-1       | 24/12/13 12:21:50 INFO Master: Removing worker worker-20241213121508-172.18.0.6-33653 on 172.18.0.6:33653
spark-master-1       | 24/12/13 12:21:50 INFO Master: Telling app of lost worker: worker-20241213121508-172.18.0.6-33653
spark-master-1       | [38;5;6mspark [38;5;5m12:22:21.66 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m12:22:21.66 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:22:21.67 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:22:21.68 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:22:21.68 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m12:22:21.70 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m12:22:21.71 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m12:22:21.71 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | [38;5;6mspark [38;5;5m12:22:21.72 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | 
spark-worker-1-1     | 24/12/13 12:15:09 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 12:15:09 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 12:15:09 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 12:15:09 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 12:15:09 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 12:15:09 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 12:15:09 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 12:22:24 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 12:22:24 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 12:22:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 95 ms (0 ms spent in bootstraps)
spark-worker-2-1     | 24/12/13 12:22:24 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 12:23:08 INFO Worker: Asked to launch executor app-20241213122308-0000/2 for MyAwesomeSpark
spark-worker-2-1     | 24/12/13 12:23:08 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 12:23:08 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 12:23:08 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 12:23:08 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 12:23:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 12:23:08 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=37609" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@116cd888dcb9:37609" "--executor-id" "2" "--hostname" "172.18.0.8" "--cores" "10" "--app-id" "app-20241213122308-0000" "--worker-url" "spark://Worker@172.18.0.8:40751" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 12:23:10 INFO Worker: Asked to kill executor app-20241213122308-0000/2
spark-worker-2-1     | 24/12/13 12:23:10 INFO ExecutorRunner: Runner thread for executor app-20241213122308-0000/2 interrupted
spark-worker-2-1     | 24/12/13 12:23:10 INFO ExecutorRunner: Killing process!
spark-worker-2-1     | 24/12/13 12:23:10 INFO Worker: Executor app-20241213122308-0000/2 finished with state KILLED exitStatus 143
spark-worker-2-1     | 24/12/13 12:23:10 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
spark-worker-2-1     | 24/12/13 12:23:10 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213122308-0000, execId=2)
spark-worker-2-1     | 24/12/13 12:23:10 INFO Worker: Cleaning up local directories for application app-20241213122308-0000
spark-worker-2-1     | 24/12/13 12:23:10 INFO ExternalShuffleBlockResolver: Application app-20241213122308-0000 removed, cleanupLocalDirs = true
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:30:10.00 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:30:10.01 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:30:10.01 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:30:10.01 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:30:10.01 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:30:10.02 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:30:10.03 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:30:10.04 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:30:10.05 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m12:30:10.05 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 12:30:11 INFO Worker: Started daemon with process name: 34@e92f47d0559d
spark-worker-2-1     | 24/12/13 12:30:11 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 12:30:11 INFO SignalUtils: Registering signal handler for HUP
spark-worker-2-1     | 24/12/13 12:30:11 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 12:30:11 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 12:30:11 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 12:30:11 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 12:30:11 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 12:30:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 12:30:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 12:30:12 INFO Utils: Successfully started service 'sparkWorker' on port 36763.
spark-worker-2-1     | 24/12/13 12:30:12 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 12:30:12 INFO Worker: Starting Spark worker 172.18.0.8:36763 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 12:30:12 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 12:30:12 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 12:30:12 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 12:30:12 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 12:30:12 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 12:30:12 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 12:30:12 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 12:30:12 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 12:30:12 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 12:30:12 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 25 ms (0 ms spent in bootstraps)
spark-worker-2-1     | 24/12/13 12:30:12 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 12:30:59 INFO Worker: Asked to launch executor app-20241213123059-0000/1 for MyAwesomeSpark
spark-worker-2-1     | 24/12/13 12:30:59 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 12:30:59 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 12:30:59 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 12:30:59 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 12:30:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 12:30:59 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45651" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@116cd888dcb9:45651" "--executor-id" "1" "--hostname" "172.18.0.8" "--cores" "10" "--app-id" "app-20241213123059-0000" "--worker-url" "spark://Worker@172.18.0.8:36763" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 12:31:01 INFO Worker: Asked to kill executor app-20241213123059-0000/1
spark-worker-2-1     | 24/12/13 12:31:01 INFO ExecutorRunner: Runner thread for executor app-20241213123059-0000/1 interrupted
spark-worker-2-1     | 24/12/13 12:31:01 INFO ExecutorRunner: Killing process!
spark-worker-2-1     | 24/12/13 12:31:01 INFO Worker: Executor app-20241213123059-0000/1 finished with state KILLED exitStatus 143
spark-worker-2-1     | 24/12/13 12:31:01 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-worker-2-1     | 24/12/13 12:31:01 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213123059-0000, execId=1)
spark-worker-2-1     | 24/12/13 12:31:01 INFO ExternalShuffleBlockResolver: Application app-20241213123059-0000 removed, cleanupLocalDirs = true
spark-worker-2-1     | 24/12/13 12:31:01 INFO Worker: Cleaning up local directories for application app-20241213123059-0000
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:02:14.21 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:02:14.21 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:02:14.21 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:02:14.21 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:02:14.21 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:02:14.23 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:02:14.25 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:02:14.26 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:02:14.26 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:02:14.28 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 13:02:17 INFO Worker: Started daemon with process name: 33@e92f47d0559d
spark-worker-1-1     | 24/12/13 12:15:09 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
airflow-worker-1     | [2024-12-13 14:42:09,883: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api_virtualenv.py
airflow-worker-1     | [2024-12-13 14:42:09,885: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_api_virtualenv>
airflow-init-1       | 
airflow-init-1       | [1;33mWARNING!!!: AIRFLOW_UID not set![0m
airflow-init-1       | If you are on Linux, you SHOULD follow the instructions below to set 
airflow-init-1       | AIRFLOW_UID environment variable, otherwise files will be owned by root.
airflow-init-1       | For other operating systems you can get rid of the warning with manually created .env file:
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:59535 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:59594 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:59677 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:59733 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:59818 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:60639 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:60722 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:60778 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:60863 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:61167 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:61250 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:61306 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:61387 SyntaxWarning: invalid escape sequence '\d'
spark-master-1       | [38;5;6mspark [38;5;5m12:22:21.73 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 12:22:23 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-master-1       | 24/12/13 12:22:23 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 12:22:23 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 12:22:23 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 12:22:23 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | 24/12/13 12:22:23 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 12:22:23 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 12:22:23 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 12:22:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 12:22:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 12:15:09 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 12:15:09 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 32 ms (1 ms spent in bootstraps)
spark-worker-1-1     | 24/12/13 12:15:09 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 12:15:57 INFO Worker: Asked to launch executor app-20241213121557-0000/2 for MyAwesomeSpark
spark-master-1       | 24/12/13 12:22:24 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 12:22:24 INFO Master: Starting Spark master at spark://f9573584fb53:7077
postgres-1           | 2024-12-13 13:49:38.713 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 13:49:38.717 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 13:49:38.722 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 13:49:38.785 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 13:50:03.557 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
airflow-init-1       |     See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user
airflow-init-1       | 
postgres-1           | 2024-12-13 13:50:03.558 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 13:50:03.558 UTC [1] LOG:  listening on IPv6 address "::", port 5432
spark-master-1       | 24/12/13 12:22:24 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 12:22:24 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
postgres-1           | 2024-12-13 13:50:03.565 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 13:50:03.584 UTC [27] LOG:  database system was shut down at 2024-12-13 13:49:38 UTC
postgres-1           | 2024-12-13 13:50:03.604 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 13:54:09.165 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 13:54:09.170 UTC [1] LOG:  aborting any active transactions
spark-worker-1-1     | 24/12/13 12:15:57 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 12:15:57 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 12:15:57 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 12:15:57 INFO SecurityManager: Changing modify acls groups to: 
airflow-worker-1     | [2024-12-13 14:42:09,885: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensor_decorator.py
airflow-worker-1     | [2024-12-13 14:42:09,886: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sensor_decorator>
airflow-worker-1     | [2024-12-13 14:42:09,886: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_operator.py
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:61696 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:61752 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:61954 SyntaxWarning: invalid escape sequence '\d'
spark-worker-1-1     | 24/12/13 12:15:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 12:15:57 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33895" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@116cd888dcb9:33895" "--executor-id" "2" "--hostname" "172.18.0.6" "--cores" "10" "--app-id" "app-20241213121557-0000" "--worker-url" "spark://Worker@172.18.0.6:33653" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 12:15:59 INFO Worker: Asked to kill executor app-20241213121557-0000/2
spark-worker-1-1     | 24/12/13 12:15:59 INFO ExecutorRunner: Runner thread for executor app-20241213121557-0000/2 interrupted
spark-worker-1-1     | 24/12/13 12:15:59 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 12:15:59 INFO Worker: Executor app-20241213121557-0000/2 finished with state KILLED exitStatus 143
spark-worker-1-1     | 24/12/13 12:15:59 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
spark-worker-1-1     | 24/12/13 12:15:59 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213121557-0000, execId=2)
spark-worker-1-1     | 24/12/13 12:15:59 INFO ExternalShuffleBlockResolver: Application app-20241213121557-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | 24/12/13 12:15:59 INFO Worker: Cleaning up local directories for application app-20241213121557-0000
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:22:21.92 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:22:21.92 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:22:21.93 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:22:21.93 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:22:21.93 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:22:21.94 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:22:21.96 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:22:21.97 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:22:21.97 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:22:21.99 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-master-1       | 24/12/13 12:22:24 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 12:22:24 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
airflow-worker-1     | [2024-12-13 14:42:09,888: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_short_circuit_operator>
airflow-worker-1     | [2024-12-13 14:42:09,888: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcom.py
airflow-worker-1     | [2024-12-13 14:42:09,890: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom>
airflow-worker-1     | [2024-12-13 14:42:09,890: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias.py
airflow-worker-1     | [2024-12-13 14:42:10,045: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_producer>
airflow-worker-1     | [2024-12-13 14:42:10,046: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_producer>
airflow-worker-1     | [2024-12-13 14:42:10,046: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_consumer>
airflow-worker-1     | [2024-12-13 14:42:10,046: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_consumer>
airflow-worker-1     | [2024-12-13 14:42:10,046: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown_taskflow.py
airflow-worker-1     | [2024-12-13 14:42:10,048: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_setup_teardown_taskflow>
airflow-worker-1     | [2024-12-13 14:42:10,048: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api.py
airflow-worker-1     | [2024-12-13 14:42:10,049: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_api>
airflow-worker-1     | [2024-12-13 14:42:10,049: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_labels.py
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:61992 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:62660 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:62715 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:63882 SyntaxWarning: invalid escape sequence '\d'
airflow-worker-1     | [2024-12-13 14:42:10,050: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,050: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,050: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,051: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,051: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,051: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 12:22:23 INFO Worker: Started daemon with process name: 34@f484bf7bba3e
spark-worker-1-1     | 24/12/13 12:22:23 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 13:02:17 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 13:02:17 INFO SignalUtils: Registering signal handler for HUP
postgres-1           | 2024-12-13 13:54:09.172 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 13:54:09.172 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 13:54:09.218 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:63946 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:64582 SyntaxWarning: invalid escape sequence '\d'
spark-worker-2-1     | 24/12/13 13:02:17 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 13:02:18 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:02:18 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:02:18 INFO SecurityManager: Changing view acls groups to: 
postgres-1           | 
postgres-1           | 2024-12-13 13:54:40.190 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
airflow-worker-1     | [2024-12-13 14:42:10,051: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_labels>
airflow-worker-1     | [2024-12-13 14:42:10,051: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:42:10,056: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:42:10,056: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_display_name.py
airflow-worker-1     | [2024-12-13 14:42:10,056: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_display_name>
airflow-worker-1     | [2024-12-13 14:42:10,056: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
my_spark             |       tasks             Manage tasks
my_spark             |       users             Manage users
my_spark             |       variables         Manage variables
my_spark             | 
my_spark             |     Commands:
my_spark             |       cheat-sheet       Display cheat sheet
my_spark             |       dag-processor     Start a standalone Dag Processor instance
my_spark             |       info              Show information about current Airflow and environment
my_spark             |       kerberos          Start a kerberos ticket renewer
my_spark             |       plugins           Dump information about loaded plugins
my_spark             |       rotate-fernet-key
my_spark             | 
spark-master-1       | 24/12/13 12:22:24 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 12:22:24 INFO Master: Registering worker 172.18.0.7:39807 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 12:22:24 INFO Master: Registering worker 172.18.0.8:40751 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 12:22:24 INFO Master: Registering worker 172.18.0.9:44977 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 12:23:08 INFO Master: Registering app MyAwesomeSpark
spark-master-1       | 24/12/13 12:23:08 INFO Master: Registered app MyAwesomeSpark with ID app-20241213122308-0000
spark-master-1       | 24/12/13 12:23:08 INFO Master: Start scheduling for app app-20241213122308-0000 with rpId: 0
spark-master-1       | 24/12/13 12:23:08 INFO Master: Launching executor app-20241213122308-0000/0 on worker worker-20241213122224-172.18.0.7-39807
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:64674 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:64940 SyntaxWarning: invalid escape sequence '\d'
spark-master-1       | 24/12/13 12:23:08 INFO Master: Launching executor app-20241213122308-0000/1 on worker worker-20241213122224-172.18.0.9-44977
spark-master-1       | 24/12/13 12:23:08 INFO Master: Launching executor app-20241213122308-0000/2 on worker worker-20241213122224-172.18.0.8-40751
spark-master-1       | 24/12/13 12:23:08 INFO Master: Start scheduling for app app-20241213122308-0000 with rpId: 0
my_spark             | airflow command error: the following arguments are required: GROUP_OR_COMMAND, see help above.
my_spark             |                         Rotate encrypted connection credentials and variables
my_spark             |       scheduler         Start a scheduler instance
my_spark             |       standalone        Run an all-in-one copy of Airflow
my_spark             |       sync-perm         Update permissions for existing roles and optionally
my_spark             |                         DAGs
my_spark             |       triggerer         Start a triggerer instance
my_spark             |       version           Show the version
my_spark             |       webserver         Start a Airflow webserver instance
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65014 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65117 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65199 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65385 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65441 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65874 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65950 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66003 SyntaxWarning: invalid escape sequence '\d'
airflow-init-1       | The container is run as root user. For security, consider using a regular user account.
airflow-init-1       | 
airflow-init-1       | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:15:06.89 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:15:06.90 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:15:06.90 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:15:06.91 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 12:15:08 INFO Worker: Started daemon with process name: 33@ed56f9ca93ac
postgres-1           | 2024-12-13 13:54:40.191 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 13:54:40.191 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 13:54:40.227 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 13:54:40.239 UTC [26] LOG:  database system was shut down at 2024-12-13 13:54:09 UTC
postgres-1           | 2024-12-13 13:54:40.289 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 13:57:07.187 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 13:57:07.188 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 13:57:07.191 UTC [1] LOG:  background worker "logical replication launcher" (PID 32) exited with exit code 1
postgres-1           | 2024-12-13 13:57:07.191 UTC [27] LOG:  shutting down
postgres-1           | 2024-12-13 13:57:07.218 UTC [1] LOG:  database system is shut down
postgres-1           | 
my_spark             | 
my_spark             | Options:
my_spark             |   -h, --help            show this help message and exit
spark-worker-1-1     | 24/12/13 12:22:23 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 12:22:23 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 12:22:23 INFO SecurityManager: Changing view acls to: spark
airflow-init-1       | DB: postgresql+psycopg2://airflow:***@postgres/airflow
airflow-init-1       | Performing upgrade to the metadata database postgresql+psycopg2://airflow:***@postgres/airflow
airflow-init-1       | [2024-12-13T14:41:05.826+0000] {migration.py:207} INFO - Context impl PostgresqlImpl.
airflow-init-1       | [2024-12-13T14:41:05.826+0000] {migration.py:210} INFO - Will assume transactional DDL.
airflow-init-1       | [2024-12-13T14:41:05.829+0000] {migration.py:207} INFO - Context impl PostgresqlImpl.
airflow-init-1       | [2024-12-13T14:41:05.829+0000] {migration.py:210} INFO - Will assume transactional DDL.
airflow-init-1       | [2024-12-13T14:41:05.829+0000] {db.py:1675} INFO - Creating tables
airflow-init-1       | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
spark-worker-2-1     | 24/12/13 13:02:18 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:02:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 13:02:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 13:02:19 INFO Utils: Successfully started service 'sparkWorker' on port 38289.
spark-worker-2-1     | 24/12/13 13:02:19 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 13:02:19 INFO Worker: Starting Spark worker 172.18.0.6:38289 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 13:02:19 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 13:02:19 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 13:02:19 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:02:19 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 13:02:19 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:02:19 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-master-1       | 24/12/13 12:23:08 INFO Master: Start scheduling for app app-20241213122308-0000 with rpId: 0
spark-master-1       | 24/12/13 12:23:08 INFO Master: Start scheduling for app app-20241213122308-0000 with rpId: 0
spark-master-1       | 24/12/13 12:23:10 INFO Master: Received unregister request from application app-20241213122308-0000
spark-master-1       | 24/12/13 12:23:10 INFO Master: Removing app app-20241213122308-0000
spark-master-1       | 24/12/13 12:23:10 WARN Master: Got status update for unknown executor app-20241213122308-0000/1
spark-master-1       | 24/12/13 12:23:10 INFO Master: 172.18.0.6:34542 got disassociated, removing it.
spark-master-1       | 24/12/13 12:23:10 INFO Master: 116cd888dcb9:37609 got disassociated, removing it.
spark-master-1       | 24/12/13 12:23:10 WARN Master: Got status update for unknown executor app-20241213122308-0000/2
spark-master-1       | 24/12/13 12:23:10 WARN Master: Got status update for unknown executor app-20241213122308-0000/0
spark-master-1       | 24/12/13 12:29:31 INFO Master: 172.18.0.7:44834 got disassociated, removing it.
spark-master-1       | 24/12/13 12:29:31 INFO Master: 172.18.0.7:39807 got disassociated, removing it.
spark-master-1       | 24/12/13 12:29:31 INFO Master: Removing worker worker-20241213122224-172.18.0.7-39807 on 172.18.0.7:39807
spark-master-1       | 24/12/13 12:29:31 INFO Master: Telling app of lost worker: worker-20241213122224-172.18.0.7-39807
spark-worker-3-1     | 24/12/13 12:15:08 INFO SignalUtils: Registering signal handler for TERM
spark-worker-3-1     | 24/12/13 12:15:08 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 12:15:08 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 12:15:08 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 12:15:08 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 12:15:08 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 12:15:08 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 12:15:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 12:15:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 12:15:08 INFO Utils: Successfully started service 'sparkWorker' on port 40295.
spark-worker-3-1     | 24/12/13 12:15:08 INFO Worker: Worker decommissioning not enabled.
spark-master-1       | 24/12/13 12:29:31 INFO Master: 172.18.0.9:43302 got disassociated, removing it.
spark-master-1       | 24/12/13 12:29:31 INFO Master: 172.18.0.9:44977 got disassociated, removing it.
spark-master-1       | 24/12/13 12:29:31 INFO Master: Removing worker worker-20241213122224-172.18.0.9-44977 on 172.18.0.9:44977
spark-master-1       | 24/12/13 12:29:31 INFO Master: Telling app of lost worker: worker-20241213122224-172.18.0.9-44977
spark-master-1       | 24/12/13 12:29:31 INFO Master: 172.18.0.8:45502 got disassociated, removing it.
spark-master-1       | 24/12/13 12:29:31 INFO Master: 172.18.0.8:40751 got disassociated, removing it.
spark-master-1       | 24/12/13 12:29:31 INFO Master: Removing worker worker-20241213122224-172.18.0.8-40751 on 172.18.0.8:40751
spark-master-1       | 24/12/13 12:29:31 INFO Master: Telling app of lost worker: worker-20241213122224-172.18.0.8-40751
spark-master-1       | [38;5;6mspark [38;5;5m12:30:09.65 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m12:30:09.65 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:30:09.65 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:30:09.65 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m12:30:09.65 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m12:30:09.68 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m12:30:09.70 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m12:30:09.70 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m12:30:09.70 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | [38;5;6mspark [38;5;5m12:30:09.71 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
airflow-worker-1     | [2024-12-13 14:42:10,062: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:10,062: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:10,062: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:10,062: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:10,063: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group_decorator.py
airflow-worker-1     | [2024-12-13 14:42:10,066: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_task_group_decorator>
airflow-worker-1     | [2024-12-13 14:42:10,066: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_target_dag.py
airflow-worker-1     | [2024-12-13 14:42:10,067: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_trigger_target_dag>
airflow-worker-1     | [2024-12-13 14:42:10,067: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_external_task_marker_dag.py
airflow-worker-1     | [2024-12-13 14:42:10,070: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_external_task_marker_child>
airflow-worker-1     | [2024-12-13 14:42:10,070: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_external_task_marker_parent>
airflow-worker-1     | [2024-12-13 14:42:10,070: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_skip_dag.py
airflow-worker-1     | [2024-12-13 14:42:10,071: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_skip_dag>
airflow-worker-1     | [2024-12-13 14:42:10,071: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_workday_timetable.py
airflow-worker-1     | [2024-12-13 14:42:10,072: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_workday_timetable>
airflow-worker-1     | [2024-12-13 14:42:10,072: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_dag.py
airflow-worker-1     | [2024-12-13 14:42:10,073: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_dag>
spark-worker-1-1     | 24/12/13 12:22:23 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 12:22:23 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 12:22:23 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 12:22:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 13:57:30.100 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 13:57:30.102 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 13:57:30.102 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 13:57:30.115 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 12:30:11 INFO Master: Started daemon with process name: 35@f9573584fb53
spark-master-1       | 24/12/13 12:30:11 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 12:30:11 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 12:30:11 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 12:30:11 INFO SecurityManager: Changing view acls to: spark
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66087 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66686 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66779 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66918 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66964 SyntaxWarning: invalid escape sequence '\d'
spark-worker-2-1     | 24/12/13 13:02:20 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 13:02:20 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-master-1       | 24/12/13 12:30:11 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 12:30:11 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 12:30:11 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 12:30:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 12:30:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 12:30:12 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 12:30:12 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 12:30:12 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 12:30:12 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 12:30:12 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 12:30:12 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 12:30:12 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 12:30:12 INFO Master: Registering worker 172.18.0.7:43835 with 10 cores, 2.0 GiB RAM
airflow-init-1       | INFO  [alembic.runtime.migration] Will assume transactional DDL.
airflow-init-1       | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
airflow-init-1       | INFO  [alembic.runtime.migration] Will assume transactional DDL.
spark-worker-2-1     | 24/12/13 13:02:20 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 13:02:20 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.3:7077 after 58 ms (0 ms spent in bootstraps)
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:67735 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:67791 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:68043 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:68097 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:68532 SyntaxWarning: invalid escape sequence '\d'
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:68588 SyntaxWarning: invalid escape sequence '\d'
airflow-init-1       | Database migrating done!
airflow-init-1       | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-init-1       | /home/airflow/.local/lib/python3.12/site-packages/flask_limiter/extension.py:333 UserWarning: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. This is not recommended for production use. See: https://flask-limiter.readthedocs.io#configuring-a-storage-backend for documentation about configuring the storage backend.
airflow-init-1       | airflow already exist in the db
postgres-1           | 2024-12-13 13:57:30.121 UTC [27] LOG:  database system was shut down at 2024-12-13 13:57:07 UTC
postgres-1           | 2024-12-13 13:57:30.136 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 13:59:09.591 UTC [1] LOG:  received fast shutdown request
spark-worker-2-1     | 24/12/13 13:02:20 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 13:03:23 INFO Worker: Asked to launch executor app-20241213130322-0000/0 for MyAwesomeSpark
spark-worker-2-1     | 24/12/13 13:03:23 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 12:15:09 INFO Worker: Starting Spark worker 172.18.0.7:40295 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 12:15:09 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 12:15:09 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 12:15:09 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 12:15:09 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 12:15:09 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 12:15:09 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 12:15:09 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 12:15:09 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-master-1       | 24/12/13 12:30:12 INFO Master: Registering worker 172.18.0.8:36763 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 12:30:12 INFO Master: Registering worker 172.18.0.9:44719 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 12:30:59 INFO Master: Registering app MyAwesomeSpark
spark-master-1       | 24/12/13 12:30:59 INFO Master: Registered app MyAwesomeSpark with ID app-20241213123059-0000
spark-master-1       | 24/12/13 12:30:59 INFO Master: Start scheduling for app app-20241213123059-0000 with rpId: 0
spark-master-1       | 24/12/13 12:30:59 INFO Master: Launching executor app-20241213123059-0000/0 on worker worker-20241213123012-172.18.0.9-44719
spark-worker-3-1     | 24/12/13 12:15:09 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 12:15:09 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 25 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 12:15:09 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 12:15:57 INFO Worker: Asked to launch executor app-20241213121557-0000/0 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 12:15:57 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 12:15:57 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 12:15:57 INFO SecurityManager: Changing view acls groups to: 
airflow-init-1       | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-init-1       | 2.10.3
airflow-init-1       | 
airflow-init-1       | [1;33mWARNING!!!: AIRFLOW_UID not set![0m
airflow-init-1       | If you are on Linux, you SHOULD follow the instructions below to set 
airflow-init-1       | AIRFLOW_UID environment variable, otherwise files will be owned by root.
spark-master-1       | 24/12/13 12:30:59 INFO Master: Launching executor app-20241213123059-0000/1 on worker worker-20241213123012-172.18.0.8-36763
spark-master-1       | 24/12/13 12:30:59 INFO Master: Launching executor app-20241213123059-0000/2 on worker worker-20241213123012-172.18.0.7-43835
spark-master-1       | 24/12/13 12:31:00 INFO Master: Start scheduling for app app-20241213123059-0000 with rpId: 0
spark-master-1       | 24/12/13 12:31:00 INFO Master: Start scheduling for app app-20241213123059-0000 with rpId: 0
spark-master-1       | 24/12/13 12:31:00 INFO Master: Start scheduling for app app-20241213123059-0000 with rpId: 0
spark-master-1       | 24/12/13 12:31:01 INFO Master: Received unregister request from application app-20241213123059-0000
spark-worker-2-1     | 24/12/13 13:03:23 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 12:31:01 INFO Master: Removing app app-20241213123059-0000
airflow-worker-1     | [2024-12-13 14:42:10,073: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_operator.py
postgres-1           | 2024-12-13 13:59:09.599 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 13:59:09.602 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 13:59:09.608 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 13:59:09.652 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 14:00:09.967 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 14:00:09.976 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 14:00:09.976 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 14:00:09.982 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 14:00:10.003 UTC [28] LOG:  database system was shut down at 2024-12-13 13:59:09 UTC
postgres-1           | 2024-12-13 14:00:10.034 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 14:01:11.419 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 14:01:11.423 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 14:01:11.425 UTC [1] LOG:  background worker "logical replication launcher" (PID 34) exited with exit code 1
postgres-1           | 2024-12-13 14:01:11.425 UTC [29] LOG:  shutting down
postgres-1           | 2024-12-13 14:01:11.450 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 14:02:40.170 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 14:02:40.177 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 14:02:40.177 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 14:02:40.182 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 14:02:40.191 UTC [26] LOG:  database system was shut down at 2024-12-13 14:01:11 UTC
postgres-1           | 2024-12-13 14:02:40.242 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 14:04:24.629 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 14:04:24.632 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 14:04:24.636 UTC [1] LOG:  background worker "logical replication launcher" (PID 32) exited with exit code 1
postgres-1           | 2024-12-13 14:04:24.638 UTC [27] LOG:  shutting down
postgres-1           | 2024-12-13 14:04:24.670 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 14:05:10.201 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 14:05:10.201 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
postgres-1           | 2024-12-13 14:05:10.202 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 14:05:10.214 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 14:05:10.221 UTC [27] LOG:  database system was shut down at 2024-12-13 14:04:24 UTC
airflow-init-1       | For other operating systems you can get rid of the warning with manually created .env file:
airflow-init-1       |     See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user
airflow-init-1       | 
airflow-init-1       | The container is run as root user. For security, consider using a regular user account.
postgres-1           | 2024-12-13 14:05:10.238 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 14:10:49.339 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 14:10:49.365 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 14:10:49.368 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 14:10:49.370 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 14:10:49.386 UTC [1] LOG:  database system is shut down
spark-worker-2-1     | 24/12/13 13:03:23 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 13:03:23 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:03:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 13:03:23 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=38277" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@e6bc8ed3f281:38277" "--executor-id" "0" "--hostname" "172.18.0.6" "--cores" "10" "--app-id" "app-20241213130322-0000" "--worker-url" "spark://Worker@172.18.0.6:38289" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 13:03:25 INFO Worker: Asked to kill executor app-20241213130322-0000/0
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/snowflake/sqlalchemy/base.py:1068 SAWarning: The GenericFunction 'flatten' is already registered and is going to be overridden.
airflow-webserver-1  | [2024-12-13 14:41:33 +0000] [19] [INFO] Listening at: http://0.0.0.0:8080 (19)
spark-worker-1-1     | 24/12/13 12:22:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 12:22:24 INFO Utils: Successfully started service 'sparkWorker' on port 39807.
spark-worker-1-1     | 24/12/13 12:22:24 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 12:22:24 INFO Worker: Starting Spark worker 172.18.0.7:39807 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 12:22:24 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 12:22:24 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 12:22:24 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 12:22:24 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 12:22:24 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 12:22:24 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 12:22:24 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-1-1     | 24/12/13 12:22:24 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 12:22:24 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 12:22:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 53 ms (0 ms spent in bootstraps)
spark-worker-1-1     | 24/12/13 12:22:24 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 12:23:08 INFO Worker: Asked to launch executor app-20241213122308-0000/0 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 12:23:08 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 12:23:08 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 12:23:08 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 13:03:25 INFO ExecutorRunner: Runner thread for executor app-20241213130322-0000/0 interrupted
airflow-webserver-1  | [2024-12-13 14:41:33 +0000] [19] [INFO] Using worker: sync
airflow-webserver-1  | [2024-12-13 14:41:33 +0000] [54] [INFO] Booting worker with pid: 54
airflow-webserver-1  | [2024-12-13 14:41:33 +0000] [55] [INFO] Booting worker with pid: 55
airflow-webserver-1  | [2024-12-13 14:41:33 +0000] [56] [INFO] Booting worker with pid: 56
airflow-webserver-1  | [2024-12-13 14:41:33 +0000] [57] [INFO] Booting worker with pid: 57
airflow-init-1       | 
airflow-init-1       | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
spark-worker-1-1     | 24/12/13 12:23:08 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 12:23:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 12:23:08 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=37609" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@116cd888dcb9:37609" "--executor-id" "0" "--hostname" "172.18.0.7" "--cores" "10" "--app-id" "app-20241213122308-0000" "--worker-url" "spark://Worker@172.18.0.7:39807" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 12:23:10 INFO Worker: Asked to kill executor app-20241213122308-0000/0
spark-worker-1-1     | 24/12/13 12:23:10 INFO ExecutorRunner: Runner thread for executor app-20241213122308-0000/0 interrupted
spark-worker-3-1     | 24/12/13 12:15:57 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 12:15:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 12:15:57 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33895" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@116cd888dcb9:33895" "--executor-id" "0" "--hostname" "172.18.0.7" "--cores" "10" "--app-id" "app-20241213121557-0000" "--worker-url" "spark://Worker@172.18.0.7:40295" "--resourceProfileId" "0"
spark-worker-3-1     | 24/12/13 12:15:59 INFO Worker: Asked to kill executor app-20241213121557-0000/0
spark-worker-3-1     | 24/12/13 12:15:59 INFO ExecutorRunner: Runner thread for executor app-20241213121557-0000/0 interrupted
spark-worker-3-1     | 24/12/13 12:15:59 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 12:23:10 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 12:23:10 INFO Worker: Executor app-20241213122308-0000/0 finished with state KILLED exitStatus 143
spark-worker-1-1     | 24/12/13 12:23:10 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-1-1     | 24/12/13 12:23:10 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213122308-0000, execId=0)
spark-worker-1-1     | 24/12/13 12:23:10 INFO ExternalShuffleBlockResolver: Application app-20241213122308-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | 24/12/13 12:23:10 INFO Worker: Cleaning up local directories for application app-20241213122308-0000
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:30:09.95 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:30:09.95 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:30:09.98 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:30:09.98 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:30:09.98 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:30:09.99 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:30:10.00 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:30:10.00 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:30:10.00 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | [38;5;6mspark [38;5;5m12:30:10.02 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 12:30:11 INFO Worker: Started daemon with process name: 33@f484bf7bba3e
spark-worker-3-1     | 24/12/13 12:15:59 INFO Worker: Executor app-20241213121557-0000/0 finished with state KILLED exitStatus 143
spark-worker-3-1     | 24/12/13 12:15:59 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-3-1     | 24/12/13 12:15:59 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213121557-0000, execId=0)
spark-worker-3-1     | 24/12/13 12:15:59 INFO Worker: Cleaning up local directories for application app-20241213121557-0000
spark-worker-3-1     | 24/12/13 12:15:59 INFO ExternalShuffleBlockResolver: Application app-20241213121557-0000 removed, cleanupLocalDirs = true
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:22:22.00 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:22:22.00 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:22:22.01 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:22:22.01 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:22:22.03 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:22:22.06 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:22:22.07 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:22:22.07 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:22:22.07 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:22:22.08 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 12:22:23 INFO Worker: Started daemon with process name: 34@ed56f9ca93ac
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:41:36 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/www/security_manager.py:257 AirflowProviderDeprecationWarning: is_authorized_dataset will be renamed as is_authorized_asset in Airflow 3 and will be removed when the minimum Airflow version is set to 3.0 for the fab provider
spark-master-1       | 24/12/13 12:31:01 INFO Master: 172.18.0.10:59926 got disassociated, removing it.
spark-worker-3-1     | 24/12/13 12:22:23 INFO SignalUtils: Registering signal handler for TERM
spark-worker-3-1     | 24/12/13 12:22:23 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 12:22:23 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 12:22:23 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 12:22:23 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 12:22:23 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 12:22:23 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 12:22:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 12:22:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 12:22:24 INFO Utils: Successfully started service 'sparkWorker' on port 44977.
airflow-init-1       | DB: postgresql+psycopg2://airflow:***@postgres/airflow
spark-worker-2-1     | 24/12/13 13:03:25 INFO ExecutorRunner: Killing process!
spark-worker-2-1     | 24/12/13 13:03:25 INFO Worker: Executor app-20241213130322-0000/0 finished with state KILLED exitStatus 143
spark-worker-2-1     | 24/12/13 13:03:25 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-2-1     | 24/12/13 13:03:25 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213130322-0000, execId=0)
spark-worker-2-1     | 24/12/13 13:03:25 INFO ExternalShuffleBlockResolver: Application app-20241213130322-0000 removed, cleanupLocalDirs = true
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
spark-worker-3-1     | 24/12/13 12:22:24 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 12:22:24 INFO Worker: Starting Spark worker 172.18.0.9:44977 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 12:22:24 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 12:22:24 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 12:22:24 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 12:22:24 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 12:22:24 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 12:22:24 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 12:22:24 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 12:22:24 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
airflow-init-1       | Performing upgrade to the metadata database postgresql+psycopg2://airflow:***@postgres/airflow
airflow-init-1       | [2024-12-13T14:57:08.104+0000] {migration.py:207} INFO - Context impl PostgresqlImpl.
airflow-init-1       | [2024-12-13T14:57:08.104+0000] {migration.py:210} INFO - Will assume transactional DDL.
airflow-init-1       | [2024-12-13T14:57:08.107+0000] {migration.py:207} INFO - Context impl PostgresqlImpl.
airflow-init-1       | [2024-12-13T14:57:08.107+0000] {migration.py:210} INFO - Will assume transactional DDL.
airflow-init-1       | [2024-12-13T14:57:08.108+0000] {db.py:1675} INFO - Creating tables
airflow-init-1       | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
airflow-init-1       | INFO  [alembic.runtime.migration] Will assume transactional DDL.
airflow-init-1       | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
airflow-init-1       | INFO  [alembic.runtime.migration] Will assume transactional DDL.
airflow-init-1       | Database migrating done!
airflow-init-1       | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-init-1       | /home/airflow/.local/lib/python3.12/site-packages/flask_limiter/extension.py:333 UserWarning: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. This is not recommended for production use. See: https://flask-limiter.readthedocs.io#configuring-a-storage-backend for documentation about configuring the storage backend.
airflow-init-1       | airflow already exist in the db
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:05 +0000] "GET /dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A05%3A42.971044%2B00%3A00&tab=logs&task_id=train_and_predict HTTP/1.1" 200 9343 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A00%3A41.133625%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/www/auth.py:291 AirflowProviderDeprecationWarning: is_authorized_dataset will be renamed as is_authorized_asset in Airflow 3 and will be removed when the minimum Airflow version is set to 3.0 for the fab provider
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:05 +0000] "GET /object/next_run_datasets/pipeline HTTP/1.1" 200 39 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A05%3A42.971044%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-worker-3-1     | 24/12/13 12:22:24 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 12:22:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 47 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 12:22:24 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 12:23:08 INFO Worker: Asked to launch executor app-20241213122308-0000/1 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 12:23:08 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 12:23:08 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 12:30:11 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 12:31:01 INFO Master: 116cd888dcb9:45651 got disassociated, removing it.
spark-master-1       | 24/12/13 12:31:01 WARN Master: Got status update for unknown executor app-20241213123059-0000/0
spark-master-1       | 24/12/13 12:31:01 WARN Master: Got status update for unknown executor app-20241213123059-0000/2
spark-master-1       | 24/12/13 12:31:01 WARN Master: Got status update for unknown executor app-20241213123059-0000/1
spark-master-1       | 24/12/13 12:39:03 INFO Master: 172.18.0.9:36776 got disassociated, removing it.
spark-master-1       | 24/12/13 12:39:03 INFO Master: 172.18.0.9:44719 got disassociated, removing it.
spark-master-1       | 24/12/13 12:39:03 INFO Master: Removing worker worker-20241213123012-172.18.0.9-44719 on 172.18.0.9:44719
spark-master-1       | 24/12/13 12:39:03 INFO Master: Telling app of lost worker: worker-20241213123012-172.18.0.9-44719
spark-master-1       | 24/12/13 12:39:03 INFO Master: 172.18.0.8:34852 got disassociated, removing it.
spark-master-1       | 24/12/13 12:39:03 INFO Master: 172.18.0.8:36763 got disassociated, removing it.
airflow-worker-1     | [2024-12-13 14:42:10,075: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_python_operator>
airflow-worker-1     | [2024-12-13 14:42:10,076: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group.py
spark-worker-3-1     | 24/12/13 12:23:08 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 12:23:08 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 12:23:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 12:23:08 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=37609" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@116cd888dcb9:37609" "--executor-id" "1" "--hostname" "172.18.0.9" "--cores" "10" "--app-id" "app-20241213122308-0000" "--worker-url" "spark://Worker@172.18.0.9:44977" "--resourceProfileId" "0"
spark-worker-3-1     | 24/12/13 12:23:10 INFO Worker: Asked to kill executor app-20241213122308-0000/1
spark-worker-3-1     | 24/12/13 12:23:10 INFO ExecutorRunner: Runner thread for executor app-20241213122308-0000/1 interrupted
spark-worker-3-1     | 24/12/13 12:23:10 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 12:23:10 INFO Worker: Executor app-20241213122308-0000/1 finished with state KILLED exitStatus 143
spark-worker-3-1     | 24/12/13 12:23:10 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-worker-3-1     | 24/12/13 12:23:10 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213122308-0000, execId=1)
spark-worker-3-1     | 24/12/13 12:23:10 INFO ExternalShuffleBlockResolver: Application app-20241213122308-0000 removed, cleanupLocalDirs = true
postgres-1           | 2024-12-13 14:40:53.848 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 14:40:53.849 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:05 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40792 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A05%3A42.971044%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:06 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:05:42.971044+00:00/taskInstances/train_and_predict HTTP/1.1" 200 921 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A05%3A42.971044%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-worker-1-1     | 24/12/13 12:30:11 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 12:30:11 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 12:30:11 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 12:30:11 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 12:30:11 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 12:30:11 INFO SecurityManager: Changing modify acls groups to: 
airflow-init-1       | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-init-1       | 2.10.3
spark-worker-2-1     | 24/12/13 13:03:25 INFO Worker: Cleaning up local directories for application app-20241213130322-0000
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:07:15.96 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | 24/12/13 12:30:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 12:30:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 12:30:12 INFO Utils: Successfully started service 'sparkWorker' on port 43835.
spark-worker-1-1     | 24/12/13 12:30:12 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 12:23:10 INFO Worker: Cleaning up local directories for application app-20241213122308-0000
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:30:10.00 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:30:10.00 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
postgres-1           | 2024-12-13 14:40:53.849 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 14:40:53.854 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 14:40:53.865 UTC [27] LOG:  database system was shut down at 2024-12-13 14:10:49 UTC
postgres-1           | 2024-12-13 14:40:53.872 UTC [1] LOG:  database system is ready to accept connections
postgres-1           | 2024-12-13 14:47:03.862 UTC [1] LOG:  received fast shutdown request
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:06 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:05:42.971044+00:00/taskInstances/train_and_predict/logs/1?full_content=false HTTP/1.1" 200 7936 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A05%3A42.971044%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:42:06 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:07 +0000] "POST /dags/pipeline/trigger?origin=%2Fdags%2Fpipeline%2Fgrid%3Fdag_run_id%3Dmanual__2024-12-13T14%253A05%253A42.971044%252B00%253A00%26tab%3Dlogs%26task_id%3Dtrain_and_predict HTTP/1.1" 302 473 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A05%3A42.971044%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/www/security_manager.py:257 AirflowProviderDeprecationWarning: is_authorized_dataset will be renamed as is_authorized_asset in Airflow 3 and will be removed when the minimum Airflow version is set to 3.0 for the fab provider
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:07 +0000] "GET /dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict HTTP/1.1" 200 9420 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A05%3A42.971044%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/www/auth.py:291 AirflowProviderDeprecationWarning: is_authorized_dataset will be renamed as is_authorized_asset in Airflow 3 and will be removed when the minimum Airflow version is set to 3.0 for the fab provider
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:07 +0000] "GET /object/next_run_datasets/pipeline HTTP/1.1" 200 39 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:07 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40462 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:07 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict HTTP/1.1" 200 742 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:30:10.01 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:07 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict/logs/1?full_content=false HTTP/1.1" 200 2 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:10 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40493 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-master-1       | 24/12/13 12:39:03 INFO Master: Removing worker worker-20241213123012-172.18.0.8-36763 on 172.18.0.8:36763
spark-master-1       | 24/12/13 12:39:03 INFO Master: Telling app of lost worker: worker-20241213123012-172.18.0.8-36763
spark-master-1       | 24/12/13 12:39:03 INFO Master: 172.18.0.7:34594 got disassociated, removing it.
spark-master-1       | 24/12/13 12:39:03 INFO Master: 172.18.0.7:43835 got disassociated, removing it.
spark-master-1       | 24/12/13 12:39:03 INFO Master: Removing worker worker-20241213123012-172.18.0.7-43835 on 172.18.0.7:43835
spark-master-1       | 24/12/13 12:39:03 INFO Master: Telling app of lost worker: worker-20241213123012-172.18.0.7-43835
spark-master-1       | [38;5;6mspark [38;5;5m13:02:13.98 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:02:14.00 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:02:14.01 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:02:14.01 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:07:15.96 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:07:15.96 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:07:15.96 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-hnz0xkyr5budp2j55kvnoy9wj9kibs/?location=": dial tcp 172.18.0.4:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-xznfaei2cdngbs1kv6ev2qixfkw2ea/?location=": dial tcp 172.18.0.5:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-5dhnscnlniotiunnu6biiwcnxzv6vz/?location=": dial tcp 172.18.0.5:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
postgres-1           | 2024-12-13 14:47:03.868 UTC [1] LOG:  aborting any active transactions
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:10 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict HTTP/1.1" 200 742 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:30:10.01 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:30:10.01 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:30:10.03 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:30:10.05 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:30:10.05 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:30:10.06 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m12:30:10.06 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | 24/12/13 12:30:12 INFO Worker: Starting Spark worker 172.18.0.7:43835 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 12:30:12 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 12:30:12 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 12:30:12 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 12:30:12 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 12:30:12 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 12:30:12 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 12:30:12 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-1-1     | 24/12/13 12:30:12 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 12:30:12 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 12:30:12 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 20 ms (0 ms spent in bootstraps)
spark-worker-1-1     | 24/12/13 12:30:12 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
airflow-worker-1     | [2024-12-13 14:42:10,077: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_task_group>
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:07:15.97 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:07:15.97 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:07:15.99 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:07:15.99 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:07:15.99 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:07:16.01 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
postgres-1           | 2024-12-13 14:47:03.870 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
postgres-1           | 2024-12-13 14:47:03.871 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 14:47:03.885 UTC [1] LOG:  database system is shut down
postgres-1           | 
postgres-1           | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres-1           | 
postgres-1           | 2024-12-13 14:56:59.533 UTC [1] LOG:  starting PostgreSQL 13.18 (Debian 13.18-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
postgres-1           | 2024-12-13 14:56:59.533 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-ky90zqju6isaqujfz21qg52ptgmckl/?location=": dial tcp 172.18.0.2:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-4xofgwkjsgixbnm5eadnfoaycmprhw/?location=": dial tcp 172.18.0.3:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-4fvk66mze3w6cxba1tq64g0iwtaxrq/?location=": dial tcp 172.18.0.4:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | Added `myminio` successfully.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:13 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40493 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:13 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict HTTP/1.1" 200 742 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:16 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40557 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:16 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict HTTP/1.1" 200 742 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:19 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40652 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:19 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict HTTP/1.1" 200 742 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:22 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict HTTP/1.1" 200 776 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-master-1       | [38;5;6mspark [38;5;5m13:02:14.02 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:02:14.06 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m13:02:14.07 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
postgres-1           | 2024-12-13 14:56:59.534 UTC [1] LOG:  listening on IPv6 address "::", port 5432
postgres-1           | 2024-12-13 14:56:59.537 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
postgres-1           | 2024-12-13 14:56:59.547 UTC [27] LOG:  database system was shut down at 2024-12-13 14:47:03 UTC
postgres-1           | 2024-12-13 14:56:59.570 UTC [1] LOG:  database system is ready to accept connections
airflow-worker-1     | [2024-12-13 14:42:10,077: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_datetime_operator.py
airflow-worker-1     | [2024-12-13 14:42:10,078: DEBUG/ForkPoolWorker-15] Failed to find locale C
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m13:02:14.08 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | [38;5;6mspark [38;5;5m13:02:14.08 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | 
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:22 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40747 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:25 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40778 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:25 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict HTTP/1.1" 200 887 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | [2024-12-13T14:42:26.295+0000] {executor_loader.py:254} INFO - Loaded executor: CeleryExecutor
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:26 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict/logs/1?full_content=false HTTP/1.1" 200 3680 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:26 +0000] "GET /dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict HTTP/1.1" 200 9338 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A05%3A42.971044%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 13:07:17 INFO Worker: Started daemon with process name: 34@e92f47d0559d
spark-worker-2-1     | 24/12/13 13:07:17 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 13:07:17 INFO SignalUtils: Registering signal handler for HUP
spark-worker-2-1     | 24/12/13 13:07:17 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 13:07:17 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:07:17 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:07:17 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 13:07:17 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:07:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 13:07:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 13:07:18 INFO Utils: Successfully started service 'sparkWorker' on port 34437.
postgres-1           | 2024-12-13 15:02:07.458 UTC [1] LOG:  received fast shutdown request
postgres-1           | 2024-12-13 15:02:07.462 UTC [1] LOG:  aborting any active transactions
postgres-1           | 2024-12-13 15:02:07.464 UTC [1] LOG:  background worker "logical replication launcher" (PID 33) exited with exit code 1
spark-worker-2-1     | 24/12/13 13:07:18 INFO Worker: Worker decommissioning not enabled.
spark-master-1       | [38;5;6mspark [38;5;5m13:02:14.10 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 13:02:17 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-master-1       | 24/12/13 13:02:17 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 13:02:17 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 13:02:17 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 13:02:18 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | 24/12/13 13:02:18 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 13:02:18 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 13:02:18 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 13:02:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 13:02:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 13:02:19 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 13:02:19 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 13:02:19 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 13:02:19 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 13:02:20 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 13:02:20 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 13:02:20 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 13:02:20 INFO Master: Registering worker 172.18.0.9:46243 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:02:20 INFO Master: Registering worker 172.18.0.8:45079 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:02:20 INFO Master: Registering worker 172.18.0.6:38289 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:03:22 INFO Master: Registering app MyAwesomeSpark
spark-master-1       | 24/12/13 13:03:22 INFO Master: Registered app MyAwesomeSpark with ID app-20241213130322-0000
spark-master-1       | 24/12/13 13:03:22 INFO Master: Start scheduling for app app-20241213130322-0000 with rpId: 0
spark-master-1       | 24/12/13 13:03:22 INFO Master: Launching executor app-20241213130322-0000/0 on worker worker-20241213130219-172.18.0.6-38289
spark-master-1       | 24/12/13 13:03:22 INFO Master: Launching executor app-20241213130322-0000/1 on worker worker-20241213130219-172.18.0.8-45079
spark-master-1       | 24/12/13 13:03:22 INFO Master: Launching executor app-20241213130322-0000/2 on worker worker-20241213130219-172.18.0.9-46243
spark-master-1       | 24/12/13 13:03:23 INFO Master: Start scheduling for app app-20241213130322-0000 with rpId: 0
spark-master-1       | 24/12/13 13:03:23 INFO Master: Start scheduling for app app-20241213130322-0000 with rpId: 0
spark-master-1       | 24/12/13 13:03:23 INFO Master: Start scheduling for app app-20241213130322-0000 with rpId: 0
spark-master-1       | 24/12/13 13:03:25 INFO Master: Received unregister request from application app-20241213130322-0000
spark-master-1       | 24/12/13 13:03:25 INFO Master: Removing app app-20241213130322-0000
spark-master-1       | 24/12/13 13:03:25 WARN Master: Got status update for unknown executor app-20241213130322-0000/1
spark-master-1       | 24/12/13 13:03:25 WARN Master: Got status update for unknown executor app-20241213130322-0000/2
spark-master-1       | 24/12/13 13:03:25 WARN Master: Got status update for unknown executor app-20241213130322-0000/0
spark-master-1       | 24/12/13 13:03:25 INFO Master: 172.18.0.10:48384 got disassociated, removing it.
spark-master-1       | 24/12/13 13:03:25 INFO Master: e6bc8ed3f281:38277 got disassociated, removing it.
spark-master-1       | 24/12/13 13:06:55 INFO Master: 172.18.0.9:47248 got disassociated, removing it.
spark-master-1       | 24/12/13 13:06:55 INFO Master: 172.18.0.8:58716 got disassociated, removing it.
spark-master-1       | 24/12/13 13:06:55 INFO Master: 172.18.0.8:45079 got disassociated, removing it.
spark-master-1       | 24/12/13 13:06:55 INFO Master: Removing worker worker-20241213130219-172.18.0.8-45079 on 172.18.0.8:45079
spark-master-1       | 24/12/13 13:06:55 INFO Master: Telling app of lost worker: worker-20241213130219-172.18.0.8-45079
spark-master-1       | 24/12/13 13:06:55 INFO Master: 172.18.0.9:46243 got disassociated, removing it.
spark-master-1       | 24/12/13 13:06:55 INFO Master: Removing worker worker-20241213130219-172.18.0.9-46243 on 172.18.0.9:46243
spark-master-1       | 24/12/13 13:06:55 INFO Master: Telling app of lost worker: worker-20241213130219-172.18.0.9-46243
spark-master-1       | 24/12/13 13:06:55 INFO Master: 172.18.0.6:38278 got disassociated, removing it.
spark-master-1       | 24/12/13 13:06:55 INFO Master: 172.18.0.6:38289 got disassociated, removing it.
spark-master-1       | 24/12/13 13:06:55 INFO Master: Removing worker worker-20241213130219-172.18.0.6-38289 on 172.18.0.6:38289
spark-master-1       | 24/12/13 13:06:55 INFO Master: Telling app of lost worker: worker-20241213130219-172.18.0.6-38289
spark-master-1       | [38;5;6mspark [38;5;5m13:07:15.69 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:07:15.69 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:07:15.69 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:07:15.69 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:07:15.69 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:07:15.71 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m13:07:15.72 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m13:07:15.73 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | [38;5;6mspark [38;5;5m13:07:15.73 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m13:07:15.74 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 13:07:17 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-master-1       | 24/12/13 13:07:17 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 13:07:17 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 13:07:17 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 13:07:17 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | 24/12/13 13:07:17 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 13:07:17 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 13:07:17 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 13:07:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 13:07:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 13:07:18 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 13:07:18 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 13:07:18 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 13:07:19 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 13:07:19 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 13:07:19 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 13:07:19 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 13:07:19 INFO Master: Registering worker 172.18.0.7:42905 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:07:19 INFO Master: Registering worker 172.18.0.8:42093 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:07:19 INFO Master: Registering worker 172.18.0.6:34437 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:08:16 INFO Master: Registering app MyAwesomeSpark
spark-master-1       | 24/12/13 13:08:17 INFO Master: Registered app MyAwesomeSpark with ID app-20241213130816-0000
spark-master-1       | 24/12/13 13:08:17 INFO Master: Start scheduling for app app-20241213130816-0000 with rpId: 0
spark-master-1       | 24/12/13 13:08:17 INFO Master: Launching executor app-20241213130816-0000/0 on worker worker-20241213130718-172.18.0.7-42905
spark-master-1       | 24/12/13 13:08:17 INFO Master: Launching executor app-20241213130816-0000/1 on worker worker-20241213130718-172.18.0.6-34437
spark-master-1       | 24/12/13 13:08:17 INFO Master: Launching executor app-20241213130816-0000/2 on worker worker-20241213130718-172.18.0.8-42093
spark-master-1       | 24/12/13 13:08:17 INFO Master: Start scheduling for app app-20241213130816-0000 with rpId: 0
spark-master-1       | 24/12/13 13:08:17 INFO Master: Start scheduling for app app-20241213130816-0000 with rpId: 0
spark-master-1       | 24/12/13 13:08:17 INFO Master: Start scheduling for app app-20241213130816-0000 with rpId: 0
spark-master-1       | 24/12/13 13:08:22 INFO Master: Received unregister request from application app-20241213130816-0000
spark-master-1       | 24/12/13 13:08:22 INFO Master: Removing app app-20241213130816-0000
spark-master-1       | 24/12/13 13:08:22 INFO Master: 172.18.0.9:39106 got disassociated, removing it.
spark-master-1       | 24/12/13 13:08:22 INFO Master: e538db3bcc6c:46711 got disassociated, removing it.
spark-master-1       | 24/12/13 13:08:22 WARN Master: Got status update for unknown executor app-20241213130816-0000/0
spark-master-1       | 24/12/13 13:08:22 WARN Master: Got status update for unknown executor app-20241213130816-0000/2
spark-master-1       | 24/12/13 13:08:22 WARN Master: Got status update for unknown executor app-20241213130816-0000/1
spark-master-1       | 24/12/13 13:13:44 INFO Master: 172.18.0.7:45494 got disassociated, removing it.
spark-master-1       | 24/12/13 13:13:44 INFO Master: 172.18.0.7:42905 got disassociated, removing it.
spark-master-1       | 24/12/13 13:13:44 INFO Master: Removing worker worker-20241213130718-172.18.0.7-42905 on 172.18.0.7:42905
spark-master-1       | 24/12/13 13:13:44 INFO Master: Telling app of lost worker: worker-20241213130718-172.18.0.7-42905
spark-master-1       | 24/12/13 13:13:44 INFO Master: 172.18.0.8:59392 got disassociated, removing it.
spark-master-1       | 24/12/13 13:13:44 INFO Master: 172.18.0.8:42093 got disassociated, removing it.
spark-master-1       | 24/12/13 13:13:44 INFO Master: Removing worker worker-20241213130718-172.18.0.8-42093 on 172.18.0.8:42093
spark-master-1       | 24/12/13 13:13:44 INFO Master: Telling app of lost worker: worker-20241213130718-172.18.0.8-42093
spark-master-1       | 24/12/13 13:13:44 INFO Master: 172.18.0.6:56530 got disassociated, removing it.
spark-master-1       | 24/12/13 13:13:44 INFO Master: 172.18.0.6:34437 got disassociated, removing it.
spark-master-1       | 24/12/13 13:13:44 INFO Master: Removing worker worker-20241213130718-172.18.0.6-34437 on 172.18.0.6:34437
spark-master-1       | 24/12/13 13:13:44 INFO Master: Telling app of lost worker: worker-20241213130718-172.18.0.6-34437
spark-master-1       | [38;5;6mspark [38;5;5m13:22:10.50 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:22:10.50 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:22:10.50 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:22:10.51 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:22:10.51 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:22:10.55 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m13:22:10.57 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m13:22:10.58 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | [38;5;6mspark [38;5;5m13:22:10.59 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m13:22:10.60 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 13:22:13 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 12:30:11 INFO Worker: Started daemon with process name: 34@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 12:30:11 INFO SignalUtils: Registering signal handler for TERM
spark-worker-3-1     | 24/12/13 12:30:11 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 12:30:11 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 12:30:11 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 12:30:11 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 12:30:11 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 12:30:11 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 12:30:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 12:30:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 12:30:12 INFO Utils: Successfully started service 'sparkWorker' on port 44719.
spark-worker-3-1     | 24/12/13 12:30:12 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 12:30:12 INFO Worker: Starting Spark worker 172.18.0.9:44719 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 12:30:12 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 12:30:12 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 12:30:12 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 12:30:12 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 12:30:12 INFO ResourceUtils: ==============================================================
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-263xj4msjch33f5x4enbgrbl2csqiw/?location=": dial tcp 172.18.0.4:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
airflow-worker-1     | [2024-12-13 14:42:10,078: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,078: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,078: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,079: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,079: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,079: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,079: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,079: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,080: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,080: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,080: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,080: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator>
airflow-worker-1     | [2024-12-13 14:42:10,080: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,080: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,080: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,081: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator_3>
airflow-worker-1     | [2024-12-13 14:42:10,081: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,081: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,081: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,081: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator_2>
airflow-worker-1     | [2024-12-13 14:42:10,081: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dag_decorator.py
airflow-worker-1     | [2024-12-13 14:42:10,275: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dag_decorator>
airflow-worker-1     | [2024-12-13 14:42:10,275: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_day_of_week_operator.py
airflow-worker-1     | [2024-12-13 14:42:10,276: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,276: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,277: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,277: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,277: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,277: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,278: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_weekday_branch_operator>
airflow-worker-1     | [2024-12-13 14:42:10,278: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_inlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:42:10,279: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,279: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,279: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,279: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,279: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,279: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,280: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,280: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,280: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,280: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: read_dataset_event_from_classic>
airflow-worker-1     | [2024-12-13 14:42:10,280: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,280: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,280: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,281: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: read_dataset_event>
airflow-worker-1     | [2024-12-13 14:42:10,281: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping.py
spark-worker-1-1     | 24/12/13 12:30:59 INFO Worker: Asked to launch executor app-20241213123059-0000/2 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 12:30:59 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 12:30:59 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:07:19 INFO Worker: Starting Spark worker 172.18.0.6:34437 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 13:07:19 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 13:07:19 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 13:07:19 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 12:30:12 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 12:30:12 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 12:30:12 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-cj9ah32jm43oyfgdu1tyh2mdyp346a/?location=": dial tcp 172.18.0.3:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-59lbfxvolggeca2dzuckxtx1tukzmu/?location=": dial tcp 172.18.0.4:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-o3i0wpo1qqsybis01g4oym1kpk200o/?location=": dial tcp 172.18.0.5:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-q1ontmbogxdgn443as5q111if2ql32/?location=": dial tcp 172.18.0.4:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-yu9pb1k4bnqilgfi4ozk1o59y5jzbc/?location=": dial tcp 172.18.0.4:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-rvjghn14fjiup9m1wp3n30pumyk15d/?location=": dial tcp 172.18.0.4:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-zrv0h413kaozshswt1b60tulw5fyam/?location=": dial tcp 172.18.0.4:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-s563rxxzh9pewevwqkojpm1609yzvk/?location=": dial tcp 172.18.0.3:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:26 +0000] "GET /object/next_run_datasets/pipeline HTTP/1.1" 200 39 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:26 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40778 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:26 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict HTTP/1.1" 200 887 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | [2024-12-13T14:42:26.846+0000] {executor_loader.py:254} INFO - Loaded executor: CeleryExecutor
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:26 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict/logs/1?full_content=false HTTP/1.1" 200 3680 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:28 +0000] "GET /dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict HTTP/1.1" 200 9340 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A05%3A42.971044%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-worker-3-1     | 24/12/13 12:30:12 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 12:30:12 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 38 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 12:30:12 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 12:30:59 INFO Worker: Asked to launch executor app-20241213123059-0000/0 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 12:30:59 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 12:30:59 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 12:30:59 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 12:30:59 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 12:30:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 12:30:59 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45651" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@116cd888dcb9:45651" "--executor-id" "0" "--hostname" "172.18.0.9" "--cores" "10" "--app-id" "app-20241213123059-0000" "--worker-url" "spark://Worker@172.18.0.9:44719" "--resourceProfileId" "0"
spark-worker-3-1     | 24/12/13 12:31:01 INFO Worker: Asked to kill executor app-20241213123059-0000/0
spark-worker-3-1     | 24/12/13 12:31:01 INFO ExecutorRunner: Runner thread for executor app-20241213123059-0000/0 interrupted
spark-worker-3-1     | 24/12/13 12:31:01 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 12:31:01 INFO Worker: Executor app-20241213123059-0000/0 finished with state KILLED exitStatus 143
spark-worker-3-1     | 24/12/13 12:31:01 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-3-1     | 24/12/13 12:31:01 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213123059-0000, execId=0)
spark-worker-3-1     | 24/12/13 12:31:01 INFO ExternalShuffleBlockResolver: Application app-20241213123059-0000 removed, cleanupLocalDirs = true
spark-worker-3-1     | 24/12/13 12:31:01 INFO Worker: Cleaning up local directories for application app-20241213123059-0000
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:02:14.36 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:02:14.36 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:02:14.36 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:02:14.36 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:02:14.36 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:02:14.39 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | 24/12/13 12:30:59 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 12:30:59 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:02:14.41 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:02:14.42 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:02:14.44 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:02:14.46 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/www/auth.py:291 AirflowProviderDeprecationWarning: is_authorized_dataset will be renamed as is_authorized_asset in Airflow 3 and will be removed when the minimum Airflow version is set to 3.0 for the fab provider
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:28 +0000] "GET /object/next_run_datasets/pipeline HTTP/1.1" 200 39 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:28 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40778 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:28 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict HTTP/1.1" 200 887 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:28 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict/logs/1?full_content=false HTTP/1.1" 200 4256 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:31 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40838 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-cr2h5vwke5vgvz9ak2f9mpju1frlnc/?location=": dial tcp 172.18.0.3:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
createbuckets-1      | mc: <ERROR> Unable to initialize new alias from the provided credentials. Get "http://minio:9000/probe-bsign-zphugklpv491tycelzhlwn9ewrg3f3/?location=": dial tcp 172.18.0.3:9000: connect: connection refused.
createbuckets-1      | Bucket created successfully `myminio/movielens`.
createbuckets-1      | mc: Please use 'mc anonymous'
postgres-1           | 2024-12-13 15:02:07.497 UTC [28] LOG:  shutting down
postgres-1           | 2024-12-13 15:02:07.512 UTC [1] LOG:  database system is shut down
spark-worker-2-1     | 24/12/13 13:07:19 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 12:30:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 12:31:00 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45651" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@116cd888dcb9:45651" "--executor-id" "2" "--hostname" "172.18.0.7" "--cores" "10" "--app-id" "app-20241213123059-0000" "--worker-url" "spark://Worker@172.18.0.7:43835" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 12:31:01 INFO Worker: Asked to kill executor app-20241213123059-0000/2
spark-worker-2-1     | 24/12/13 13:07:19 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:07:19 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 13:07:19 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 13:07:19 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 13:07:19 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 13:07:19 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 64 ms (0 ms spent in bootstraps)
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:42:36 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:56 +0000] "GET /dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict HTTP/1.1" 200 9342 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A05%3A42.971044%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:56 +0000] "GET /object/next_run_datasets/pipeline HTTP/1.1" 200 39 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:56 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40838 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:56 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict HTTP/1.1" 200 921 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-master-1       | 24/12/13 13:22:13 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 13:22:13 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 13:22:13 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 13:22:13 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | 24/12/13 13:22:13 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:07:19 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 13:08:17 INFO Worker: Asked to launch executor app-20241213130816-0000/1 for MyAwesomeSpark
spark-master-1       | 24/12/13 13:22:13 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 13:22:13 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 13:22:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 12:31:01 INFO ExecutorRunner: Runner thread for executor app-20241213123059-0000/2 interrupted
spark-worker-1-1     | 24/12/13 12:31:01 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 12:31:01 INFO Worker: Executor app-20241213123059-0000/2 finished with state KILLED exitStatus 143
spark-worker-1-1     | 24/12/13 12:31:01 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
spark-worker-1-1     | 24/12/13 12:31:01 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213123059-0000, execId=2)
spark-worker-1-1     | 24/12/13 12:31:01 INFO ExternalShuffleBlockResolver: Application app-20241213123059-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | 24/12/13 12:31:01 INFO Worker: Cleaning up local directories for application app-20241213123059-0000
spark-worker-2-1     | 24/12/13 13:08:17 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:08:17 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:08:17 INFO SecurityManager: Changing view acls groups to: 
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:42:56 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict/logs/1?full_content=false HTTP/1.1" 200 8164 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:43:06 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:43:36 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:43:43 +0000] "GET /api/v1/dags/pipeline/tasks/train_and_predict HTTP/1.1" 200 957 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:44:06 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:44:24 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40838 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T12%3A15%3A44.075828%2B00%3A00&tab=details" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:44:36 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:44:40 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40838 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T12%3A22%3A53.401610%2B00%3A00&tab=details" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-master-1       | 24/12/13 13:22:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 13:22:14 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 13:22:14 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 13:22:14 INFO Master: Running Spark version 3.5.0
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 13:02:17 INFO Worker: Started daemon with process name: 33@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 13:02:17 INFO SignalUtils: Registering signal handler for TERM
spark-worker-3-1     | 24/12/13 13:02:17 INFO SignalUtils: Registering signal handler for HUP
airflow-worker-1     | [2024-12-13 14:42:10,282: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dynamic_task_mapping>
airflow-worker-1     | [2024-12-13 14:42:10,282: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_subdag_operator.py
airflow-worker-1     | [2024-12-13 14:42:10,284: DEBUG/ForkPoolWorker-15] Failed to find locale C
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:02:14.33 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:02:14.34 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:02:14.34 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:02:14.34 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:02:14.35 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:02:14.40 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:02:14.42 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:02:14.42 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:02:14.43 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:02:14.46 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 13:02:17 INFO Worker: Started daemon with process name: 33@f484bf7bba3e
spark-worker-1-1     | 24/12/13 13:02:17 INFO SignalUtils: Registering signal handler for TERM
spark-worker-1-1     | 24/12/13 13:02:17 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 13:02:17 INFO SignalUtils: Registering signal handler for INT
airflow-worker-1     | [2024-12-13 14:42:10,284: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
spark-worker-1-1     | 24/12/13 13:02:18 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 13:02:18 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:02:18 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:02:18 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:02:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:02:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 13:02:19 INFO Utils: Successfully started service 'sparkWorker' on port 46243.
spark-worker-1-1     | 24/12/13 13:02:19 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 13:02:19 INFO Worker: Starting Spark worker 172.18.0.9:46243 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 13:02:19 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 13:02:19 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 13:02:19 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:02:19 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 13:02:19 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:02:20 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 13:02:20 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-1-1     | 24/12/13 13:02:20 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-2-1     | 24/12/13 13:08:17 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:08:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 13:08:17 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=46711" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@e538db3bcc6c:46711" "--executor-id" "1" "--hostname" "172.18.0.6" "--cores" "10" "--app-id" "app-20241213130816-0000" "--worker-url" "spark://Worker@172.18.0.6:34437" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 13:08:22 INFO Worker: Asked to kill executor app-20241213130816-0000/1
spark-worker-2-1     | 24/12/13 13:08:22 INFO ExecutorRunner: Runner thread for executor app-20241213130816-0000/1 interrupted
spark-worker-2-1     | 24/12/13 13:08:22 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 13:02:20 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 13:02:20 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.3:7077 after 46 ms (0 ms spent in bootstraps)
spark-worker-1-1     | 24/12/13 13:02:20 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 13:03:23 INFO Worker: Asked to launch executor app-20241213130322-0000/2 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 13:03:23 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 13:03:23 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:03:23 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 13:22:14 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 13:22:14 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-worker-2-1     | 24/12/13 13:08:22 INFO Worker: Executor app-20241213130816-0000/1 finished with state KILLED exitStatus 143
airflow-worker-1     | [2024-12-13 14:42:10,284: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-master-1       | 24/12/13 13:22:14 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 13:22:15 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 13:22:15 INFO Master: Registering worker 172.18.0.6:43299 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:22:15 INFO Master: Registering worker 172.18.0.9:44609 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:22:15 INFO Master: Registering worker 172.18.0.7:35133 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:23:00 INFO Master: Registering app MyAwesomeSpark
spark-master-1       | 24/12/13 13:23:00 INFO Master: Registered app MyAwesomeSpark with ID app-20241213132300-0000
spark-master-1       | 24/12/13 13:23:00 INFO Master: Start scheduling for app app-20241213132300-0000 with rpId: 0
spark-master-1       | 24/12/13 13:23:01 INFO Master: Launching executor app-20241213132300-0000/0 on worker worker-20241213132214-172.18.0.6-43299
spark-master-1       | 24/12/13 13:23:01 INFO Master: Launching executor app-20241213132300-0000/1 on worker worker-20241213132214-172.18.0.7-35133
airflow-worker-1     | [2024-12-13 14:42:10,289: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,289: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:10,289: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,293: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator.section-1>
spark-worker-1-1     | 24/12/13 13:03:23 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:03:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:03:23 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=38277" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@e6bc8ed3f281:38277" "--executor-id" "2" "--hostname" "172.18.0.9" "--cores" "10" "--app-id" "app-20241213130322-0000" "--worker-url" "spark://Worker@172.18.0.9:46243" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 13:03:25 INFO Worker: Asked to kill executor app-20241213130322-0000/2
spark-worker-1-1     | 24/12/13 13:03:25 INFO ExecutorRunner: Runner thread for executor app-20241213130322-0000/2 interrupted
spark-worker-1-1     | 24/12/13 13:03:25 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 13:03:25 INFO Worker: Executor app-20241213130322-0000/2 finished with state KILLED exitStatus 143
spark-worker-1-1     | 24/12/13 13:03:25 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
spark-worker-1-1     | 24/12/13 13:03:25 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213130322-0000, execId=2)
spark-worker-1-1     | 24/12/13 13:03:25 INFO ExternalShuffleBlockResolver: Application app-20241213130322-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | 24/12/13 13:03:25 INFO Worker: Cleaning up local directories for application app-20241213130322-0000
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:07:15.98 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:07:15.98 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:07:15.99 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | 24/12/13 13:23:01 INFO Master: Launching executor app-20241213132300-0000/2 on worker worker-20241213132214-172.18.0.9-44609
spark-master-1       | 24/12/13 13:23:01 INFO Master: Start scheduling for app app-20241213132300-0000 with rpId: 0
spark-master-1       | 24/12/13 13:23:01 INFO Master: Start scheduling for app app-20241213132300-0000 with rpId: 0
spark-master-1       | 24/12/13 13:23:01 INFO Master: Start scheduling for app app-20241213132300-0000 with rpId: 0
spark-master-1       | 24/12/13 13:23:04 INFO Master: Received unregister request from application app-20241213132300-0000
spark-master-1       | 24/12/13 13:23:04 INFO Master: Removing app app-20241213132300-0000
spark-master-1       | 24/12/13 13:23:04 INFO Master: 172.18.0.8:35010 got disassociated, removing it.
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:07:15.99 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:07:15.99 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:07:16.00 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:07:16.01 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | 24/12/13 13:08:22 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-worker-2-1     | 24/12/13 13:08:22 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213130816-0000, execId=1)
spark-worker-2-1     | 24/12/13 13:08:22 INFO Worker: Cleaning up local directories for application app-20241213130816-0000
spark-worker-2-1     | 24/12/13 13:08:22 INFO ExternalShuffleBlockResolver: Application app-20241213130816-0000 removed, cleanupLocalDirs = true
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:22:10.86 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:22:10.86 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:22:10.87 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:44:41 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T12:22:53.401610+00:00/taskInstances/train_and_predict/dependencies HTTP/1.1" 200 25 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T12%3A22%3A53.401610%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:07:16.02 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:07:16.02 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:07:16.05 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-master-1       | 24/12/13 13:23:04 INFO Master: e538db3bcc6c:33895 got disassociated, removing it.
spark-master-1       | 24/12/13 13:23:04 WARN Master: Got status update for unknown executor app-20241213132300-0000/0
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:22:10.87 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:22:10.87 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:22:10.89 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:22:10.90 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:22:10.91 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:22:10.91 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:22:10.99 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 13:02:17 INFO SignalUtils: Registering signal handler for INT
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:44:41 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T12:22:53.401610+00:00/taskInstances/train_and_predict HTTP/1.1" 200 921 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T12%3A22%3A53.401610%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:45:06 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:45:36 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:46:06 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:46:36 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
spark-master-1       | 24/12/13 13:23:04 WARN Master: Got status update for unknown executor app-20241213132300-0000/2
spark-master-1       | 24/12/13 13:23:04 WARN Master: Got status update for unknown executor app-20241213132300-0000/1
spark-master-1       | 24/12/13 13:32:43 INFO Master: 172.18.0.9:54738 got disassociated, removing it.
spark-master-1       | 24/12/13 13:32:43 INFO Master: 172.18.0.7:52718 got disassociated, removing it.
spark-master-1       | 24/12/13 13:32:43 INFO Master: 172.18.0.7:35133 got disassociated, removing it.
spark-master-1       | 24/12/13 13:32:43 INFO Master: Removing worker worker-20241213132214-172.18.0.7-35133 on 172.18.0.7:35133
spark-master-1       | 24/12/13 13:32:43 INFO Master: Telling app of lost worker: worker-20241213132214-172.18.0.7-35133
spark-master-1       | 24/12/13 13:32:43 INFO Master: 172.18.0.6:57932 got disassociated, removing it.
spark-master-1       | 24/12/13 13:32:43 INFO Master: 172.18.0.6:43299 got disassociated, removing it.
spark-master-1       | 24/12/13 13:32:43 INFO Master: Removing worker worker-20241213132214-172.18.0.6-43299 on 172.18.0.6:43299
spark-master-1       | 24/12/13 13:32:43 INFO Master: Telling app of lost worker: worker-20241213132214-172.18.0.6-43299
spark-master-1       | 24/12/13 13:32:43 INFO Master: 172.18.0.9:44609 got disassociated, removing it.
airflow-worker-1     | [2024-12-13 14:42:10,293: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator.section-2>
airflow-worker-1     | [2024-12-13 14:42:10,293: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator>
airflow-worker-1     | [2024-12-13 14:42:10,293: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial.py
airflow-worker-1     | [2024-12-13 14:42:10,294: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial>
airflow-webserver-1  | [2024-12-13 14:46:58 +0000] [55] [INFO] Worker exiting (pid: 55)
airflow-webserver-1  | [2024-12-13 14:46:58 +0000] [57] [INFO] Worker exiting (pid: 57)
airflow-webserver-1  | [2024-12-13 14:46:58 +0000] [56] [INFO] Worker exiting (pid: 56)
airflow-webserver-1  | [2024-12-13 14:46:58 +0000] [19] [INFO] Handling signal: term
airflow-webserver-1  | [2024-12-13 14:46:58 +0000] [54] [INFO] Worker exiting (pid: 54)
airflow-webserver-1  |   ____________       _____________
airflow-webserver-1  |  ____    |__( )_________  __/__  /________      __
spark-worker-2-1     | 24/12/13 13:22:13 INFO Worker: Started daemon with process name: 33@e92f47d0559d
spark-worker-2-1     | 24/12/13 13:22:13 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 13:22:13 INFO SignalUtils: Registering signal handler for HUP
airflow-webserver-1  | ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
airflow-webserver-1  | ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
airflow-webserver-1  |  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
airflow-webserver-1  | Running the Gunicorn Server with:
spark-master-1       | 24/12/13 13:32:43 INFO Master: Removing worker worker-20241213132214-172.18.0.9-44609 on 172.18.0.9:44609
spark-master-1       | 24/12/13 13:32:43 INFO Master: Telling app of lost worker: worker-20241213132214-172.18.0.9-44609
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 13:22:13 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 13:22:13 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:22:13 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:22:13 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 13:22:13 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:07:17 INFO Worker: Started daemon with process name: 33@f484bf7bba3e
spark-worker-1-1     | 24/12/13 13:07:17 INFO SignalUtils: Registering signal handler for TERM
spark-worker-1-1     | 24/12/13 13:07:17 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 13:07:17 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 13:22:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 13:22:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 13:07:17 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 13:07:17 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:07:17 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:07:17 INFO SecurityManager: Changing modify acls groups to: 
airflow-worker-1     | [2024-12-13 14:42:10,294: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_complex.py
airflow-worker-1     | [2024-12-13 14:42:10,298: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_complex>
airflow-worker-1     | [2024-12-13 14:42:10,299: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_python_dop_operator_3.py
airflow-worker-1     | [2024-12-13 14:42:10,299: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:10,299: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
spark-worker-1-1     | 24/12/13 13:07:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:07:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 13:07:18 INFO Utils: Successfully started service 'sparkWorker' on port 42093.
spark-worker-1-1     | 24/12/13 13:07:18 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 13:07:19 INFO Worker: Starting Spark worker 172.18.0.8:42093 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 13:02:18 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:02:18 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 13:02:18 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:02:18 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:02:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
airflow-worker-1     | [2024-12-13 14:42:10,299: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:10,300: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_dop_operator_v3>
spark-worker-1-1     | 24/12/13 13:07:19 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 13:07:19 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 13:07:19 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:07:19 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 13:07:19 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:07:19 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 13:07:19 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-1-1     | 24/12/13 13:07:19 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-3-1     | 24/12/13 13:02:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 13:02:19 INFO Utils: Successfully started service 'sparkWorker' on port 45079.
spark-worker-3-1     | 24/12/13 13:02:19 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 13:02:19 INFO Worker: Starting Spark worker 172.18.0.8:45079 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 13:02:19 INFO Worker: Running Spark version 3.5.0
airflow-webserver-1  | Workers: 4 sync
airflow-webserver-1  | Host: 0.0.0.0:8080
airflow-webserver-1  | Timeout: 120
airflow-webserver-1  | Logfiles: - -
airflow-webserver-1  | Access Logformat: 
airflow-webserver-1  | =================================================================
spark-worker-2-1     | 24/12/13 13:22:14 INFO Utils: Successfully started service 'sparkWorker' on port 44609.
spark-worker-2-1     | 24/12/13 13:22:14 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 13:22:14 INFO Worker: Starting Spark worker 172.18.0.9:44609 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 13:22:14 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 13:22:14 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 13:22:14 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:22:14 INFO ResourceUtils: No custom resources configured for spark.worker.
airflow-worker-1     | [2024-12-13 14:42:10,300: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/subdags/subdag.py
airflow-worker-1     | [2024-12-13 14:42:10,301: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/event_listener.py
airflow-worker-1     | [2024-12-13 14:42:10,301: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/listener_plugin.py
airflow-worker-1     | [2024-12-13 14:42:10,301: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/decreasing_priority_weight_strategy.py
spark-worker-2-1     | 24/12/13 13:22:14 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:22:14 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 13:22:14 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 13:22:14 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-1-1     | 24/12/13 13:07:19 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 13:07:19 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 42 ms (0 ms spent in bootstraps)
spark-worker-1-1     | 24/12/13 13:07:19 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 13:08:17 INFO Worker: Asked to launch executor app-20241213130816-0000/2 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 13:08:17 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | [38;5;6mspark [38;5;5m13:33:04.67 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:33:04.67 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:33:04.67 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | 24/12/13 13:02:19 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 13:02:19 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 13:02:19 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 13:02:19 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 13:02:20 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-master-1       | [38;5;6mspark [38;5;5m13:33:04.67 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:33:04.67 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:33:04.69 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | 24/12/13 13:08:17 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:08:17 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:08:17 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:08:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 13:22:14 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 13:22:15 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 83 ms (0 ms spent in bootstraps)
spark-master-1       | [38;5;6mspark [38;5;5m13:33:04.70 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m13:33:04.71 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | [38;5;6mspark [38;5;5m13:33:04.71 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | 
airflow-worker-1     | [2024-12-13 14:42:10,302: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/workday.py
airflow-worker-1     | [2024-12-13 14:42:10,318: DEBUG/ForkPoolWorker-15] Plugins are already loaded. Skipping.
airflow-worker-1     | [2024-12-13 14:42:10,318: DEBUG/ForkPoolWorker-15] Integrate DAG plugins
airflow-worker-1     | [2024-12-13 14:42:10,320: DEBUG/ForkPoolWorker-15] previous_execution_date was called
spark-master-1       | [38;5;6mspark [38;5;5m13:33:04.75 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-worker-1-1     | 24/12/13 13:08:17 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=46711" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@e538db3bcc6c:46711" "--executor-id" "2" "--hostname" "172.18.0.8" "--cores" "10" "--app-id" "app-20241213130816-0000" "--worker-url" "spark://Worker@172.18.0.8:42093" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 13:08:22 INFO Worker: Asked to kill executor app-20241213130816-0000/2
spark-worker-2-1     | 24/12/13 13:22:15 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 13:23:01 INFO Worker: Asked to launch executor app-20241213132300-0000/2 for MyAwesomeSpark
spark-worker-2-1     | 24/12/13 13:23:01 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:23:01 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:23:01 INFO SecurityManager: Changing view acls groups to: 
airflow-webserver-1  | [2024-12-13T14:46:58.810+0000] {webserver_command.py:430} INFO - Received signal: 15. Closing gunicorn.
airflow-webserver-1  | [2024-12-13 14:47:01 +0000] [19] [INFO] Shutting down: Master
airflow-webserver-1  | 
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/flask_limiter/extension.py:333 UserWarning: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. This is not recommended for production use. See: https://flask-limiter.readthedocs.io#configuring-a-storage-backend for documentation about configuring the storage backend.
spark-worker-1-1     | 24/12/13 13:08:22 INFO ExecutorRunner: Runner thread for executor app-20241213130816-0000/2 interrupted
spark-worker-1-1     | 24/12/13 13:08:22 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 13:08:22 INFO Worker: Executor app-20241213130816-0000/2 finished with state KILLED exitStatus 143
spark-worker-1-1     | 24/12/13 13:08:22 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
spark-worker-1-1     | 24/12/13 13:08:22 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213130816-0000, execId=2)
spark-worker-1-1     | 24/12/13 13:08:22 INFO ExternalShuffleBlockResolver: Application app-20241213130816-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | 24/12/13 13:08:22 INFO Worker: Cleaning up local directories for application app-20241213130816-0000
spark-worker-2-1     | 24/12/13 13:23:01 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:23:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:22:10.78 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:22:10.78 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:22:10.78 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:22:10.79 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:22:10.79 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | 24/12/13 13:23:01 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33895" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@e538db3bcc6c:33895" "--executor-id" "2" "--hostname" "172.18.0.9" "--cores" "10" "--app-id" "app-20241213132300-0000" "--worker-url" "spark://Worker@172.18.0.9:44609" "--resourceProfileId" "0"
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:22:10.81 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:22:10.84 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:22:10.85 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:22:10.85 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | 
spark-worker-2-1     | 24/12/13 13:23:04 INFO Worker: Asked to kill executor app-20241213132300-0000/2
spark-worker-2-1     | 24/12/13 13:23:04 INFO ExecutorRunner: Runner thread for executor app-20241213132300-0000/2 interrupted
spark-worker-2-1     | 24/12/13 13:23:04 INFO ExecutorRunner: Killing process!
spark-worker-2-1     | 24/12/13 13:23:04 INFO Worker: Executor app-20241213132300-0000/2 finished with state KILLED exitStatus 143
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/providers/fab/auth_manager/fab_auth_manager.py:558 FutureWarning: section/key [webserver/update_fab_perms] has been deprecated, you should use[fab/update_fab_perms] instead. Please update your `conf.get*` call to use the new name
airflow-webserver-1  | [2024-12-13 14:57:20 +0000] [15] [INFO] Starting gunicorn 23.0.0
airflow-webserver-1  | [2024-12-13T14:57:23.254+0000] {providers_manager.py:287} INFO - Optional provider feature disabled when importing 'airflow.providers.google.leveldb.hooks.leveldb.LevelDBHook' from 'apache-airflow-providers-google' package
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/snowflake/sqlalchemy/base.py:1068 SAWarning: The GenericFunction 'flatten' is already registered and is going to be overridden.
airflow-webserver-1  | [2024-12-13 14:57:24 +0000] [15] [INFO] Listening at: http://0.0.0.0:8080 (15)
spark-worker-2-1     | 24/12/13 13:23:04 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
spark-worker-2-1     | 24/12/13 13:23:04 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213132300-0000, execId=2)
spark-worker-2-1     | 24/12/13 13:23:04 INFO ExternalShuffleBlockResolver: Application app-20241213132300-0000 removed, cleanupLocalDirs = true
spark-worker-2-1     | 24/12/13 13:23:04 INFO Worker: Cleaning up local directories for application app-20241213132300-0000
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:33:04.95 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:33:04.95 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:33:04.96 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
airflow-worker-1     | [2024-12-13 14:42:10,326: INFO/ForkPoolWorker-15] Running <TaskInstance: pipeline.download_and_unwrap manual__2024-12-13T14:42:07.082582+00:00 [queued]> on host 65b6e979e5f1
airflow-worker-1     | [2024-12-13 14:42:10,326: DEBUG/ForkPoolWorker-15] Disposing DB connection pool (PID 76)
airflow-worker-1     | [2024-12-13 14:42:10,326: DEBUG/ForkPoolWorker-15] Setting up DB connection pool (PID 76)
airflow-worker-1     | [2024-12-13 14:42:10,326: DEBUG/ForkPoolWorker-15] settings.prepare_engine_args(): Using NullPool
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:22:10.87 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
airflow-webserver-1  | [2024-12-13 14:57:24 +0000] [15] [INFO] Using worker: sync
airflow-webserver-1  | [2024-12-13 14:57:24 +0000] [45] [INFO] Booting worker with pid: 45
airflow-webserver-1  | [2024-12-13 14:57:25 +0000] [46] [INFO] Booting worker with pid: 46
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:33:04.96 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:33:04.96 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | 24/12/13 13:02:20 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 13:02:20 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-3-1     | 24/12/13 13:02:20 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 13:02:20 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.3:7077 after 31 ms (0 ms spent in bootstraps)
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 13:22:13 INFO Worker: Started daemon with process name: 33@f484bf7bba3e
spark-worker-1-1     | 24/12/13 13:22:13 INFO SignalUtils: Registering signal handler for TERM
spark-worker-1-1     | 24/12/13 13:22:13 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 13:22:13 INFO SignalUtils: Registering signal handler for INT
airflow-webserver-1  | [2024-12-13 14:57:25 +0000] [47] [INFO] Booting worker with pid: 47
airflow-webserver-1  | [2024-12-13 14:57:25 +0000] [48] [INFO] Booting worker with pid: 48
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:57:27 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
spark-worker-1-1     | 24/12/13 13:22:13 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:33:04.97 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:33:04.99 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:33:04.99 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:33:05.00 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | 
airflow-worker-1     | [2024-12-13 14:42:15,697: DEBUG/ForkPoolWorker-15] Calling callbacks: []
airflow-worker-1     | [2024-12-13 14:42:15,734: INFO/ForkPoolWorker-15] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[39e1513b-69f2-4b79-9b57-55902906e741] succeeded in 8.151043920999655s: None
spark-master-1       | 24/12/13 13:33:07 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-master-1       | 24/12/13 13:33:07 INFO SignalUtils: Registering signal handler for TERM
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/www/security_manager.py:257 AirflowProviderDeprecationWarning: is_authorized_dataset will be renamed as is_authorized_asset in Airflow 3 and will be removed when the minimum Airflow version is set to 3.0 for the fab provider
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:57:40 +0000] "GET /dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=details&task_id=train_and_predict HTTP/1.1" 200 9340 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A05%3A42.971044%2B00%3A00&tab=logs&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-worker-3-1     | 24/12/13 13:02:20 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 13:03:23 INFO Worker: Asked to launch executor app-20241213130322-0000/1 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 13:03:23 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:03:23 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 13:03:23 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:03:23 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:03:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 13:03:23 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=38277" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@e6bc8ed3f281:38277" "--executor-id" "1" "--hostname" "172.18.0.8" "--cores" "10" "--app-id" "app-20241213130322-0000" "--worker-url" "spark://Worker@172.18.0.8:45079" "--resourceProfileId" "0"
spark-worker-3-1     | 24/12/13 13:03:25 INFO Worker: Asked to kill executor app-20241213130322-0000/1
spark-worker-3-1     | 24/12/13 13:03:25 INFO ExecutorRunner: Runner thread for executor app-20241213130322-0000/1 interrupted
spark-worker-3-1     | 24/12/13 13:03:25 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 13:03:25 INFO Worker: Executor app-20241213130322-0000/1 finished with state KILLED exitStatus 143
spark-worker-3-1     | 24/12/13 13:03:25 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-worker-3-1     | 24/12/13 13:03:25 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213130322-0000, execId=1)
spark-worker-3-1     | 24/12/13 13:03:25 INFO ExternalShuffleBlockResolver: Application app-20241213130322-0000 removed, cleanupLocalDirs = true
spark-worker-3-1     | 24/12/13 13:03:25 INFO Worker: Cleaning up local directories for application app-20241213130322-0000
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:07:15.98 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:07:15.98 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:07:15.98 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:07:15.99 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:07:15.99 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:07:16.00 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
airflow-worker-1     | [2024-12-13 14:42:16,229: INFO/MainProcess] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[6789a300-d913-4a6d-aee0-508369a38049] received
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:07:16.02 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | 24/12/13 13:22:13 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:22:13 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:22:13 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 13:33:07 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 13:33:07 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 13:22:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:22:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 13:22:14 INFO Utils: Successfully started service 'sparkWorker' on port 43299.
spark-worker-1-1     | 24/12/13 13:22:14 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:07:16.02 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:07:16.03 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:07:16.06 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-master-1       | 24/12/13 13:33:07 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | 24/12/13 13:33:07 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 13:33:07 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 13:33:07 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 13:33:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 13:33:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 13:33:08 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 13:33:08 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 13:33:08 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 13:33:08 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
airflow-worker-1     | [2024-12-13 14:42:16,238: INFO/ForkPoolWorker-15] [6789a300-d913-4a6d-aee0-508369a38049] Executing command in Celery: ['airflow', 'tasks', 'run', 'pipeline', 'split', 'manual__2024-12-13T14:42:07.082582+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-worker-1     | [2024-12-13 14:42:16,520: INFO/ForkPoolWorker-15] Filling up the DagBag from /opt/airflow/dags/pipeline.py
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:33:05.01 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 13:33:07 INFO Worker: Started daemon with process name: 34@e92f47d0559d
spark-master-1       | 24/12/13 13:33:08 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-worker-2-1     | 24/12/13 13:33:07 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 13:33:07 INFO SignalUtils: Registering signal handler for HUP
spark-worker-2-1     | 24/12/13 13:33:07 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 13:33:07 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:33:07 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
airflow-worker-1     | [2024-12-13 14:42:16,784: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: pipeline>
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 13:07:17 INFO Worker: Started daemon with process name: 33@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 13:07:17 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 13:33:08 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 13:33:08 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 13:33:09 INFO Master: Registering worker 172.18.0.8:39467 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:33:09 INFO Master: Registering worker 172.18.0.6:42369 with 10 cores, 2.0 GiB RAM
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/www/auth.py:291 AirflowProviderDeprecationWarning: is_authorized_dataset will be renamed as is_authorized_asset in Airflow 3 and will be removed when the minimum Airflow version is set to 3.0 for the fab provider
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:57:40 +0000] "GET /object/next_run_datasets/pipeline HTTP/1.1" 200 39 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:57:40 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40838 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-worker-1     | [2024-12-13 14:42:16,784: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcomargs.py
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:57:40 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict/dependencies HTTP/1.1" 200 25 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:57:40 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:42:07.082582+00:00/taskInstances/train_and_predict HTTP/1.1" 200 921 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:57:40 +0000] "GET /api/v1/dags/pipeline/tasks/train_and_predict HTTP/1.1" 200 957 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:57:57 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:11 +0000] "POST /dags/pipeline/trigger?origin=%2Fdags%2Fpipeline%2Fgrid%3Fdag_run_id%3Dmanual__2024-12-13T14%253A42%253A07.082582%252B00%253A00%26tab%3Ddetails%26task_id%3Dtrain_and_predict HTTP/1.1" 302 479 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/www/security_manager.py:257 AirflowProviderDeprecationWarning: is_authorized_dataset will be renamed as is_authorized_asset in Airflow 3 and will be removed when the minimum Airflow version is set to 3.0 for the fab provider
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:11 +0000] "GET /dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict HTTP/1.1" 200 9420 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A42%3A07.082582%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | /home/airflow/.local/lib/python3.12/site-packages/airflow/www/auth.py:291 AirflowProviderDeprecationWarning: is_authorized_dataset will be renamed as is_authorized_asset in Airflow 3 and will be removed when the minimum Airflow version is set to 3.0 for the fab provider
spark-worker-1-1     | 24/12/13 13:22:14 INFO Worker: Starting Spark worker 172.18.0.6:43299 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 13:22:14 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 13:22:14 INFO Worker: Spark home: /opt/bitnami/spark
spark-master-1       | 24/12/13 13:33:09 INFO Master: Registering worker 172.18.0.7:41621 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:33:53 INFO Master: Registering app MyAwesomeSpark
airflow-worker-1     | [2024-12-13 14:42:16,791: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom_args>
airflow-worker-1     | [2024-12-13 14:42:16,791: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom_args_with_operators>
airflow-worker-1     | [2024-12-13 14:42:16,792: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensors.py
airflow-worker-1     | [2024-12-13 14:42:16,795: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sensors>
airflow-worker-1     | [2024-12-13 14:42:16,795: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
spark-worker-1-1     | 24/12/13 13:22:14 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:22:14 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 13:22:14 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:22:14 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 13:22:14 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 13:07:17 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 13:07:17 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 13:07:17 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:07:17 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 13:07:17 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:22:14 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 13:22:14 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 13:22:15 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 59 ms (0 ms spent in bootstraps)
spark-worker-1-1     | 24/12/13 13:22:15 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 13:23:01 INFO Worker: Asked to launch executor app-20241213132300-0000/0 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 13:23:01 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 13:23:01 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:23:01 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:23:01 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:23:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 13:33:07 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 13:33:07 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:33:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 13:33:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 13:33:08 INFO Utils: Successfully started service 'sparkWorker' on port 42369.
spark-worker-2-1     | 24/12/13 13:33:08 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 13:33:08 INFO Worker: Starting Spark worker 172.18.0.6:42369 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 13:33:08 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 13:07:17 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:07:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:23:01 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33895" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@e538db3bcc6c:33895" "--executor-id" "0" "--hostname" "172.18.0.6" "--cores" "10" "--app-id" "app-20241213132300-0000" "--worker-url" "spark://Worker@172.18.0.6:43299" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 13:23:04 INFO Worker: Asked to kill executor app-20241213132300-0000/0
spark-worker-2-1     | 24/12/13 13:33:08 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 13:33:08 INFO ResourceUtils: ==============================================================
spark-master-1       | 24/12/13 13:33:53 INFO Master: Registered app MyAwesomeSpark with ID app-20241213133353-0000
spark-master-1       | 24/12/13 13:33:53 INFO Master: Start scheduling for app app-20241213133353-0000 with rpId: 0
spark-master-1       | 24/12/13 13:33:53 INFO Master: Launching executor app-20241213133353-0000/0 on worker worker-20241213133308-172.18.0.8-39467
spark-master-1       | 24/12/13 13:33:53 INFO Master: Launching executor app-20241213133353-0000/1 on worker worker-20241213133308-172.18.0.6-42369
spark-master-1       | 24/12/13 13:33:53 INFO Master: Launching executor app-20241213133353-0000/2 on worker worker-20241213133308-172.18.0.7-41621
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:12 +0000] "GET /object/next_run_datasets/pipeline HTTP/1.1" 200 39 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:12 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40413 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:12 +0000] "GET /api/v1/dags/pipeline/tasks/train_and_predict HTTP/1.1" 200 957 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-worker-1     | [2024-12-13 14:42:16,797: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: latest_only_with_trigger>
airflow-worker-1     | [2024-12-13 14:42:16,797: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:42:16,799: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_local_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:42:16,799: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_nested_branch_dag.py
airflow-worker-1     | [2024-12-13 14:42:16,800: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:12 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:58:11.753084+00:00/taskInstances/train_and_predict HTTP/1.1" 200 742 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:12 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:58:11.753084+00:00/taskInstances/train_and_predict/dependencies HTTP/1.1" 200 616 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-worker-3-1     | 24/12/13 13:07:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 13:07:18 INFO Utils: Successfully started service 'sparkWorker' on port 42905.
spark-worker-3-1     | 24/12/13 13:07:18 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 13:07:19 INFO Worker: Starting Spark worker 172.18.0.7:42905 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 13:07:19 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 13:23:04 INFO ExecutorRunner: Runner thread for executor app-20241213132300-0000/0 interrupted
spark-worker-1-1     | 24/12/13 13:23:04 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 13:23:04 INFO Worker: Executor app-20241213132300-0000/0 finished with state KILLED exitStatus 143
spark-worker-1-1     | 24/12/13 13:23:04 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-1-1     | 24/12/13 13:23:04 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213132300-0000, execId=0)
spark-worker-2-1     | 24/12/13 13:33:08 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 13:33:08 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:33:08 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 13:33:08 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
airflow-worker-1     | [2024-12-13 14:42:16,800: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:16,800: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:16,801: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:16,801: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
spark-worker-2-1     | 24/12/13 13:33:08 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 13:33:08 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 13:33:08 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 57 ms (0 ms spent in bootstraps)
spark-master-1       | 24/12/13 13:33:53 INFO Master: Start scheduling for app app-20241213133353-0000 with rpId: 0
spark-master-1       | 24/12/13 13:33:53 INFO Master: Start scheduling for app app-20241213133353-0000 with rpId: 0
spark-master-1       | 24/12/13 13:33:53 INFO Master: Start scheduling for app app-20241213133353-0000 with rpId: 0
spark-master-1       | 24/12/13 13:33:57 INFO Master: Received unregister request from application app-20241213133353-0000
spark-worker-3-1     | 24/12/13 13:07:19 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 13:07:19 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 13:07:19 INFO ResourceUtils: No custom resources configured for spark.worker.
airflow-worker-1     | [2024-12-13 14:42:16,801: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:16,802: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_nested_branch_dag>
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:15 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40539 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:15 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:58:11.753084+00:00/taskInstances/train_and_predict HTTP/1.1" 200 742 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:15 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:58:11.753084+00:00/taskInstances/train_and_predict/dependencies HTTP/1.1" 200 472 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:18 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40576 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:18 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:58:11.753084+00:00/taskInstances/train_and_predict HTTP/1.1" 200 742 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-master-1       | 24/12/13 13:33:57 INFO Master: Removing app app-20241213133353-0000
airflow-worker-1     | [2024-12-13 14:42:16,802: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py
airflow-worker-1     | [2024-12-13 14:42:16,803: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dynamic_task_mapping_with_no_taskflow_operators>
airflow-worker-1     | [2024-12-13 14:42:16,804: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_operator.py
airflow-worker-1     | [2024-12-13 14:42:16,804: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:16,804: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:16,804: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:16,806: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:42:16,806: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:42:16,806: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator.py
airflow-worker-1     | [2024-12-13 14:42:16,806: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:20 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:58:11.753084+00:00/taskInstances/train_and_predict/dependencies HTTP/1.1" 200 472 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:21 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40634 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-worker-1-1     | 24/12/13 13:23:04 INFO ExternalShuffleBlockResolver: Application app-20241213132300-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | 24/12/13 13:23:04 INFO Worker: Cleaning up local directories for application app-20241213132300-0000
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:33:04.99 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | 24/12/13 13:33:09 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 13:33:53 INFO Worker: Asked to launch executor app-20241213133353-0000/1 for MyAwesomeSpark
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:21 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:58:11.753084+00:00/taskInstances/train_and_predict HTTP/1.1" 200 742 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:23 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:58:11.753084+00:00/taskInstances/train_and_predict/dependencies HTTP/1.1" 200 472 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:33:04.99 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:33:04.99 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:33:04.99 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
airflow-worker-1     | [2024-12-13 14:42:16,806: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:16,806: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:24 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40793 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:24 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:58:11.753084+00:00/taskInstances/train_and_predict HTTP/1.1" 200 776 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:27 +0000] "GET /api/v1/dags/pipeline/dagRuns/manual__2024-12-13T14:58:11.753084+00:00/taskInstances/train_and_predict HTTP/1.1" 200 887 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-worker-3-1     | 24/12/13 13:07:19 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 13:07:19 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 13:07:19 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-master-1       | 24/12/13 13:33:57 INFO Master: 172.18.0.10:46358 got disassociated, removing it.
spark-master-1       | 24/12/13 13:33:57 INFO Master: e538db3bcc6c:41103 got disassociated, removing it.
spark-master-1       | 24/12/13 13:33:57 WARN Master: Got status update for unknown executor app-20241213133353-0000/2
spark-master-1       | 24/12/13 13:33:57 WARN Master: Got status update for unknown executor app-20241213133353-0000/0
spark-master-1       | 24/12/13 13:33:57 WARN Master: Got status update for unknown executor app-20241213133353-0000/1
spark-master-1       | 24/12/13 13:37:14 INFO Master: 172.18.0.7:54124 got disassociated, removing it.
spark-master-1       | 24/12/13 13:37:14 INFO Master: 172.18.0.6:55192 got disassociated, removing it.
spark-master-1       | 24/12/13 13:37:14 INFO Master: 172.18.0.7:41621 got disassociated, removing it.
spark-master-1       | 24/12/13 13:37:14 INFO Master: Removing worker worker-20241213133308-172.18.0.7-41621 on 172.18.0.7:41621
spark-master-1       | 24/12/13 13:37:14 INFO Master: Telling app of lost worker: worker-20241213133308-172.18.0.7-41621
spark-worker-3-1     | 24/12/13 13:07:19 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-3-1     | 24/12/13 13:07:19 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 13:07:19 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 41 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 13:07:19 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 13:08:17 INFO Worker: Asked to launch executor app-20241213130816-0000/0 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 13:08:17 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:08:17 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:33:05.00 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:33:05.01 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:33:05.02 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:33:05.03 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:33:05.03 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | 
airflow-worker-1     | [2024-12-13 14:42:16,809: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:16,809: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:16,809: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-worker-3-1     | 24/12/13 13:08:17 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:08:17 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:08:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
airflow-worker-1     | [2024-12-13 14:42:16,810: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_operator>
airflow-worker-1     | [2024-12-13 14:42:16,810: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_passing_params_via_test_command.py
airflow-worker-1     | [2024-12-13 14:42:16,811: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:16,811: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
spark-worker-2-1     | 24/12/13 13:33:53 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:33:53 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:33:53 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 13:33:53 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:33:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 13:08:17 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=46711" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@e538db3bcc6c:46711" "--executor-id" "0" "--hostname" "172.18.0.7" "--cores" "10" "--app-id" "app-20241213130816-0000" "--worker-url" "spark://Worker@172.18.0.7:42905" "--resourceProfileId" "0"
spark-worker-3-1     | 24/12/13 13:08:22 INFO Worker: Asked to kill executor app-20241213130816-0000/0
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:33:05.05 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-master-1       | 24/12/13 13:37:14 INFO Master: 172.18.0.6:42369 got disassociated, removing it.
spark-master-1       | 24/12/13 13:37:14 INFO Master: Removing worker worker-20241213133308-172.18.0.6-42369 on 172.18.0.6:42369
spark-master-1       | 24/12/13 13:37:14 INFO Master: Telling app of lost worker: worker-20241213133308-172.18.0.6-42369
spark-master-1       | 24/12/13 13:37:14 INFO Master: 172.18.0.8:51236 got disassociated, removing it.
spark-worker-2-1     | 24/12/13 13:33:53 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=41103" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@e538db3bcc6c:41103" "--executor-id" "1" "--hostname" "172.18.0.6" "--cores" "10" "--app-id" "app-20241213133353-0000" "--worker-url" "spark://Worker@172.18.0.6:42369" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 13:33:57 INFO Worker: Asked to kill executor app-20241213133353-0000/1
spark-master-1       | 24/12/13 13:37:14 INFO Master: 172.18.0.8:39467 got disassociated, removing it.
spark-master-1       | 24/12/13 13:37:14 INFO Master: Removing worker worker-20241213133308-172.18.0.8-39467 on 172.18.0.8:39467
spark-master-1       | 24/12/13 13:37:14 INFO Master: Telling app of lost worker: worker-20241213133308-172.18.0.8-39467
spark-worker-2-1     | 24/12/13 13:33:57 INFO ExecutorRunner: Runner thread for executor app-20241213133353-0000/1 interrupted
spark-worker-2-1     | 24/12/13 13:33:57 INFO ExecutorRunner: Killing process!
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:27 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40824 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details&task_id=train_and_predict" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:58:28 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:30 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40824 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | 24/12/13 13:33:57 INFO Worker: Executor app-20241213133353-0000/1 finished with state KILLED exitStatus 143
spark-worker-2-1     | 24/12/13 13:33:57 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-worker-2-1     | 24/12/13 13:33:57 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213133353-0000, execId=1)
spark-worker-2-1     | 24/12/13 13:33:57 INFO ExternalShuffleBlockResolver: Application app-20241213133353-0000 removed, cleanupLocalDirs = true
spark-master-1       | [38;5;6mspark [38;5;5m13:41:38.33 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:41:38.34 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
airflow-worker-1     | [2024-12-13 14:42:16,811: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:16,812: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_passing_params_via_test_command>
spark-worker-3-1     | 24/12/13 13:08:22 INFO ExecutorRunner: Runner thread for executor app-20241213130816-0000/0 interrupted
spark-worker-3-1     | 24/12/13 13:08:22 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 13:08:22 INFO Worker: Executor app-20241213130816-0000/0 finished with state KILLED exitStatus 143
spark-worker-3-1     | 24/12/13 13:08:22 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-3-1     | 24/12/13 13:08:22 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213130816-0000, execId=0)
spark-worker-3-1     | 24/12/13 13:08:22 INFO ExternalShuffleBlockResolver: Application app-20241213130816-0000 removed, cleanupLocalDirs = true
spark-worker-3-1     | 24/12/13 13:08:22 INFO Worker: Cleaning up local directories for application app-20241213130816-0000
airflow-worker-1     | [2024-12-13 14:42:16,812: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_objectstorage.py
airflow-worker-1     | [2024-12-13 14:42:16,813: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_objectstorage>
airflow-worker-1     | [2024-12-13 14:42:16,813: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_decorator.py
airflow-worker-1     | [2024-12-13 14:42:16,815: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_python_decorator>
airflow-worker-1     | [2024-12-13 14:42:16,815: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_trigger_ui.py
airflow-worker-1     | [2024-12-13 14:42:16,817: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_params_trigger_ui>
spark-worker-2-1     | 24/12/13 13:33:57 INFO Worker: Cleaning up local directories for application app-20241213133353-0000
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:41:38.84 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:41:38.84 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:41:38.84 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:41:38.85 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:33 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40824 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 172.18.0.1 - - [13/Dec/2024:14:58:36 +0000] "GET /object/grid_data?dag_id=pipeline&num_runs=25 HTTP/1.1" 200 40884 "http://localhost:8080/dags/pipeline/grid?dag_run_id=manual__2024-12-13T14%3A58%3A11.753084%2B00%3A00&tab=details" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 YaBrowser/24.7.0.0 Safari/537.36"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:58:58 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:59:28 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-worker-1     | [2024-12-13 14:42:16,817: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_ui_tutorial.py
airflow-worker-1     | [2024-12-13 14:42:16,819: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_params_ui_tutorial>
airflow-worker-1     | [2024-12-13 14:42:16,819: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown.py
airflow-worker-1     | [2024-12-13 14:42:16,820: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_setup_teardown>
spark-master-1       | [38;5;6mspark [38;5;5m13:41:38.34 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:41:38.36 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:41:38.36 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:41:38.85 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:41:38.86 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m13:41:38.46 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m13:41:38.49 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:41:38.88 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:41:38.88 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:41:38.89 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | ========================================
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 13:33:07 INFO Worker: Started daemon with process name: 34@f484bf7bba3e
spark-worker-1-1     | 24/12/13 13:33:07 INFO SignalUtils: Registering signal handler for TERM
spark-worker-1-1     | 24/12/13 13:33:07 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 13:33:07 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 13:33:07 INFO SecurityManager: Changing view acls to: spark
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:14:59:58 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:15:00:28 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:15:00:58 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:41:38.92 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:15:01:28 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | 127.0.0.1 - - [13/Dec/2024:15:01:59 +0000] "GET /health HTTP/1.1" 200 318 "-" "curl/7.88.1"
airflow-webserver-1  | [2024-12-13 15:02:02 +0000] [45] [INFO] Worker exiting (pid: 45)
airflow-webserver-1  | [2024-12-13 15:02:02 +0000] [15] [INFO] Handling signal: term
airflow-webserver-1  | [2024-12-13 15:02:02 +0000] [48] [INFO] Worker exiting (pid: 48)
airflow-worker-1     | [2024-12-13 14:42:16,820: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sla_dag.py
airflow-worker-1     | [2024-12-13 14:42:16,821: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:16,821: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:16,821: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m13:41:38.49 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | [38;5;6mspark [38;5;5m13:41:38.49 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m13:41:38.56 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
airflow-worker-1     | [2024-12-13 14:42:16,822: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sla_dag>
airflow-worker-1     | [2024-12-13 14:42:16,822: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator_decorator.py
airflow-worker-1     | [2024-12-13 14:42:16,822: DEBUG/ForkPoolWorker-15] Failed to find locale C
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:22:10.75 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:22:10.75 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:22:10.75 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:22:10.75 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:22:10.76 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 13:41:41 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-worker-1-1     | 24/12/13 13:33:07 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:33:07 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:33:07 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:33:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:33:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
airflow-worker-1     | [2024-12-13 14:42:16,822: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:16,822: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:16,826: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:16,826: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:16,826: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:22:10.77 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:22:10.79 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:22:10.79 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:22:10.80 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
airflow-worker-1     | [2024-12-13 14:42:16,827: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_python_operator_decorator>
airflow-worker-1     | [2024-12-13 14:42:16,827: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_datasets.py
airflow-worker-1     | [2024-12-13 14:42:16,827: DEBUG/ForkPoolWorker-15] Initializing Providers Manager[dataset_uris]
airflow-worker-1     | [2024-12-13 14:42:17,214: DEBUG/ForkPoolWorker-15] Initialization of Providers Manager[dataset_uris] took 0.39 seconds
airflow-webserver-1  | [2024-12-13 15:02:02 +0000] [47] [INFO] Worker exiting (pid: 47)
airflow-webserver-1  | [2024-12-13 15:02:02 +0000] [46] [INFO] Worker exiting (pid: 46)
airflow-webserver-1  |   ____________       _____________
airflow-webserver-1  |  ____    |__( )_________  __/__  /________      __
airflow-webserver-1  | ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 13:41:41 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 13:41:41 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 13:41:41 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 13:41:41 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:22:10.81 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
airflow-webserver-1  | ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
airflow-webserver-1  |  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
airflow-webserver-1  | Running the Gunicorn Server with:
airflow-worker-1     | [2024-12-13 14:42:17,215: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,215: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,215: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,217: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,217: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-webserver-1  | Workers: 4 sync
airflow-webserver-1  | Host: 0.0.0.0:8080
airflow-webserver-1  | Timeout: 120
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
airflow-worker-1     | [2024-12-13 14:42:17,217: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,221: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_or_both_2_and_3_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:42:17,221: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: conditional_dataset_and_time_based_timetable>
airflow-worker-1     | [2024-12-13 14:42:17,221: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_produces_2>
airflow-worker-1     | [2024-12-13 14:42:17,221: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,221: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,224: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,224: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_produces_1>
airflow-worker-1     | [2024-12-13 14:42:17,225: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1_and_2>
airflow-worker-1     | [2024-12-13 14:42:17,225: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1>
airflow-worker-1     | [2024-12-13 14:42:17,225: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_and_2_with_dataset_expressions>
spark-worker-2-1     | 24/12/13 13:41:41 INFO Worker: Started daemon with process name: 34@e92f47d0559d
spark-worker-2-1     | 24/12/13 13:41:41 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 13:41:41 INFO SignalUtils: Registering signal handler for HUP
spark-worker-2-1     | 24/12/13 13:41:41 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 13:41:41 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 13:22:13 INFO Worker: Started daemon with process name: 34@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 13:22:13 INFO SignalUtils: Registering signal handler for TERM
spark-worker-3-1     | 24/12/13 13:22:13 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 13:22:13 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 13:41:41 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:41:41 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 13:41:41 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 13:41:41 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:22:13 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:22:13 INFO SecurityManager: Changing modify acls to: spark
airflow-webserver-1  | Logfiles: - -
airflow-webserver-1  | Access Logformat: 
airflow-webserver-1  | =================================================================
airflow-webserver-1  | [2024-12-13T15:02:02.691+0000] {webserver_command.py:430} INFO - Received signal: 15. Closing gunicorn.
airflow-webserver-1  | [2024-12-13 15:02:05 +0000] [15] [INFO] Shutting down: Master
spark-worker-2-1     | 24/12/13 13:41:41 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:41:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 13:41:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 13:41:42 INFO Utils: Successfully started service 'sparkWorker' on port 40025.
spark-worker-2-1     | 24/12/13 13:41:42 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 13:41:42 INFO Worker: Starting Spark worker 172.18.0.9:40025 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 13:41:42 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 13:41:42 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 13:41:42 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:41:42 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 13:41:42 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:41:42 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 13:41:42 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 13:41:42 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 13:41:42 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 13:41:42 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 42 ms (0 ms spent in bootstraps)
spark-worker-2-1     | 24/12/13 13:41:42 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 13:42:39 INFO Worker: Asked to launch executor app-20241213134239-0000/0 for MyAwesomeSpark
spark-worker-2-1     | 24/12/13 13:42:39 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:42:39 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:42:39 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 13:41:41 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 13:41:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 13:42:39 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:42:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
airflow-worker-1     | [2024-12-13 14:42:17,225: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1_never_scheduled>
airflow-worker-1     | [2024-12-13 14:42:17,225: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_unknown_never_scheduled>
spark-master-1       | 24/12/13 13:41:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 13:41:42 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 13:41:42 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 13:41:42 INFO Master: Running Spark version 3.5.0
airflow-worker-1     | [2024-12-13 14:42:17,225: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_or_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:42:17,226: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_outlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:42:17,228: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,228: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,228: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,228: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,229: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,229: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,229: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,229: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,229: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,230: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,230: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,230: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,230: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_by_context>
airflow-worker-1     | [2024-12-13 14:42:17,230: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,230: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,230: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,231: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_by_yield>
airflow-worker-1     | [2024-12-13 14:42:17,231: DEBUG/ForkPoolWorker-15] Failed to find locale C
spark-worker-3-1     | 24/12/13 13:22:13 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:22:13 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:22:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 13:22:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 13:33:08 INFO Utils: Successfully started service 'sparkWorker' on port 39467.
spark-worker-1-1     | 24/12/13 13:33:08 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 13:33:08 INFO Worker: Starting Spark worker 172.18.0.8:39467 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 13:33:08 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 13:33:08 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 13:33:08 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:33:08 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 13:42:39 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=43403" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:43403" "--executor-id" "0" "--hostname" "172.18.0.9" "--cores" "10" "--app-id" "app-20241213134239-0000" "--worker-url" "spark://Worker@172.18.0.9:40025" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 13:42:42 INFO Worker: Asked to kill executor app-20241213134239-0000/0
spark-worker-2-1     | 24/12/13 13:42:42 INFO ExecutorRunner: Runner thread for executor app-20241213134239-0000/0 interrupted
spark-worker-2-1     | 24/12/13 13:42:42 INFO ExecutorRunner: Killing process!
spark-worker-2-1     | 24/12/13 13:42:42 INFO Worker: Executor app-20241213134239-0000/0 finished with state KILLED exitStatus 143
spark-worker-2-1     | 24/12/13 13:42:42 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-2-1     | 24/12/13 13:42:42 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213134239-0000, execId=0)
spark-worker-1-1     | 24/12/13 13:33:08 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:33:08 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 13:33:08 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 13:42:42 INFO Worker: Cleaning up local directories for application app-20241213134239-0000
spark-worker-2-1     | 24/12/13 13:42:42 INFO ExternalShuffleBlockResolver: Application app-20241213134239-0000 removed, cleanupLocalDirs = true
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:50:03.68 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | 24/12/13 13:22:14 INFO Utils: Successfully started service 'sparkWorker' on port 35133.
spark-worker-3-1     | 24/12/13 13:22:14 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 13:22:14 INFO Worker: Starting Spark worker 172.18.0.7:35133 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 13:22:14 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 13:22:14 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 13:22:14 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 13:22:14 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:50:03.68 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:50:03.68 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:50:03.69 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:50:03.69 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:50:03.71 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
airflow-worker-1     | [2024-12-13 14:42:17,231: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
spark-worker-1-1     | 24/12/13 13:33:08 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 13:33:08 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 13:33:08 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 56 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 13:22:14 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 13:22:14 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 13:22:14 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 13:22:14 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
airflow-worker-1     | [2024-12-13 14:42:17,231: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-master-1       | 24/12/13 13:41:42 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 13:41:42 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 13:41:42 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 13:41:42 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 13:41:42 INFO Master: Registering worker 172.18.0.9:40025 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:41:42 INFO Master: Registering worker 172.18.0.8:42081 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:41:42 INFO Master: Registering worker 172.18.0.7:45553 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:42:39 INFO Master: Registering app MyAwesomeSpark
spark-worker-3-1     | 24/12/13 13:22:14 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 13:22:15 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 75 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 13:22:15 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 13:23:01 INFO Worker: Asked to launch executor app-20241213132300-0000/1 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 13:23:01 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:23:01 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 13:23:01 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:23:01 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:23:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 13:23:01 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33895" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@e538db3bcc6c:33895" "--executor-id" "1" "--hostname" "172.18.0.7" "--cores" "10" "--app-id" "app-20241213132300-0000" "--worker-url" "spark://Worker@172.18.0.7:35133" "--resourceProfileId" "0"
airflow-worker-1     | [2024-12-13 14:42:17,231: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_from_classic_operator>
airflow-worker-1     | [2024-12-13 14:42:17,231: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_decorator.py
airflow-worker-1     | [2024-12-13 14:42:17,234: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_decorator>
airflow-worker-1     | [2024-12-13 14:42:17,234: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_decorator.py
spark-worker-3-1     | 24/12/13 13:23:04 INFO Worker: Asked to kill executor app-20241213132300-0000/1
spark-worker-3-1     | 24/12/13 13:23:04 INFO ExecutorRunner: Runner thread for executor app-20241213132300-0000/1 interrupted
spark-worker-3-1     | 24/12/13 13:23:04 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 13:23:04 INFO Worker: Executor app-20241213132300-0000/1 finished with state KILLED exitStatus 143
spark-worker-3-1     | 24/12/13 13:23:04 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-master-1       | 24/12/13 13:42:39 INFO Master: Registered app MyAwesomeSpark with ID app-20241213134239-0000
spark-master-1       | 24/12/13 13:42:39 INFO Master: Start scheduling for app app-20241213134239-0000 with rpId: 0
spark-master-1       | 24/12/13 13:42:39 INFO Master: Launching executor app-20241213134239-0000/0 on worker worker-20241213134142-172.18.0.9-40025
spark-master-1       | 24/12/13 13:42:39 INFO Master: Launching executor app-20241213134239-0000/1 on worker worker-20241213134142-172.18.0.7-45553
airflow-worker-1     | [2024-12-13 14:42:17,235: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_short_circuit_decorator>
airflow-worker-1     | [2024-12-13 14:42:17,236: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_controller_dag.py
airflow-worker-1     | [2024-12-13 14:42:17,238: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_trigger_controller_dag>
airflow-worker-1     | [2024-12-13 14:42:17,241: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only.py
airflow-worker-1     | [2024-12-13 14:42:17,251: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: latest_only>
airflow-worker-1     | [2024-12-13 14:42:17,251: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_time_delta_sensor_async.py
airflow-worker-1     | [2024-12-13 14:42:17,254: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_time_delta_sensor_async>
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:50:03.74 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:50:03.75 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | 24/12/13 13:23:04 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213132300-0000, execId=1)
spark-worker-3-1     | 24/12/13 13:23:04 INFO Worker: Cleaning up local directories for application app-20241213132300-0000
spark-worker-3-1     | 24/12/13 13:23:04 INFO ExternalShuffleBlockResolver: Application app-20241213132300-0000 removed, cleanupLocalDirs = true
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:33:05.02 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:33:05.02 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:33:05.03 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:33:05.03 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:33:05.03 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:33:05.05 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:33:05.07 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:33:05.07 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:33:05.07 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | 24/12/13 13:42:39 INFO Master: Launching executor app-20241213134239-0000/2 on worker worker-20241213134142-172.18.0.8-42081
spark-master-1       | 24/12/13 13:42:39 INFO Master: Start scheduling for app app-20241213134239-0000 with rpId: 0
spark-master-1       | 24/12/13 13:42:39 INFO Master: Start scheduling for app app-20241213134239-0000 with rpId: 0
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:50:03.75 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:50:03.78 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 13:50:05 INFO Worker: Started daemon with process name: 33@e92f47d0559d
spark-worker-2-1     | 24/12/13 13:50:05 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 13:50:05 INFO SignalUtils: Registering signal handler for HUP
spark-worker-2-1     | 24/12/13 13:50:05 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 13:50:06 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:50:06 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:50:06 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 13:50:06 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:50:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 13:42:39 INFO Master: Start scheduling for app app-20241213134239-0000 with rpId: 0
spark-master-1       | 24/12/13 13:42:42 INFO Master: Received unregister request from application app-20241213134239-0000
spark-master-1       | 24/12/13 13:42:42 INFO Master: Removing app app-20241213134239-0000
spark-master-1       | 24/12/13 13:42:42 WARN Master: Got status update for unknown executor app-20241213134239-0000/2
spark-master-1       | 24/12/13 13:42:42 WARN Master: Got status update for unknown executor app-20241213134239-0000/0
spark-master-1       | 24/12/13 13:42:42 WARN Master: Got status update for unknown executor app-20241213134239-0000/1
spark-master-1       | 24/12/13 13:42:42 INFO Master: 172.18.0.6:38806 got disassociated, removing it.
spark-master-1       | 24/12/13 13:42:42 INFO Master: 22550a098c1e:43403 got disassociated, removing it.
spark-master-1       | 24/12/13 13:49:43 INFO Master: 172.18.0.7:44840 got disassociated, removing it.
spark-master-1       | 24/12/13 13:49:43 INFO Master: 172.18.0.7:45553 got disassociated, removing it.
spark-worker-2-1     | 24/12/13 13:50:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 13:50:06 INFO Utils: Successfully started service 'sparkWorker' on port 45547.
airflow-worker-1     | [2024-12-13 14:42:17,254: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_templates.py
airflow-worker-1     | [2024-12-13 14:42:17,255: DEBUG/ForkPoolWorker-15] Failed to find locale C
spark-master-1       | 24/12/13 13:49:43 INFO Master: Removing worker worker-20241213134142-172.18.0.7-45553 on 172.18.0.7:45553
spark-master-1       | 24/12/13 13:49:43 INFO Master: Telling app of lost worker: worker-20241213134142-172.18.0.7-45553
spark-master-1       | 24/12/13 13:49:43 INFO Master: 172.18.0.9:56108 got disassociated, removing it.
spark-master-1       | 24/12/13 13:49:43 INFO Master: 172.18.0.9:40025 got disassociated, removing it.
spark-master-1       | 24/12/13 13:49:43 INFO Master: Removing worker worker-20241213134142-172.18.0.9-40025 on 172.18.0.9:40025
spark-master-1       | 24/12/13 13:49:43 INFO Master: Telling app of lost worker: worker-20241213134142-172.18.0.9-40025
spark-master-1       | 24/12/13 13:49:43 INFO Master: 172.18.0.8:52730 got disassociated, removing it.
spark-master-1       | 24/12/13 13:49:43 INFO Master: 172.18.0.8:42081 got disassociated, removing it.
spark-master-1       | 24/12/13 13:49:43 INFO Master: Removing worker worker-20241213134142-172.18.0.8-42081 on 172.18.0.8:42081
spark-master-1       | 24/12/13 13:49:43 INFO Master: Telling app of lost worker: worker-20241213134142-172.18.0.8-42081
spark-master-1       | [38;5;6mspark [38;5;5m13:50:03.43 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:50:03.44 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:50:03.44 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:50:03.44 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:50:03.44 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:50:03.47 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m13:50:03.48 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m13:50:03.49 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | [38;5;6mspark [38;5;5m13:50:03.49 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m13:50:03.50 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 13:50:05 INFO Master: Started daemon with process name: 36@f9573584fb53
airflow-worker-1     | [2024-12-13 14:42:17,255: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,255: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,256: DEBUG/ForkPoolWorker-15] Failed to find locale C
spark-worker-2-1     | 24/12/13 13:50:06 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 13:50:07 INFO Worker: Starting Spark worker 172.18.0.6:45547 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 13:50:07 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 13:50:07 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 13:50:07 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:50:07 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 13:50:07 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:50:07 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 13:50:07 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 13:50:07 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-master-1       | 24/12/13 13:50:05 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 13:50:05 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 13:50:05 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 13:50:06 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | 24/12/13 13:50:06 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 13:50:06 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 13:50:06 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 13:50:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 13:50:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 13:50:06 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 13:50:06 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 13:33:09 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 13:33:53 INFO Worker: Asked to launch executor app-20241213133353-0000/0 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 13:33:53 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 13:33:53 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:33:53 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:33:53 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:50:07 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 13:50:07 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 33 ms (0 ms spent in bootstraps)
spark-worker-2-1     | 24/12/13 13:50:07 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 13:50:48 INFO Worker: Asked to launch executor app-20241213135048-0000/1 for MyAwesomeSpark
spark-worker-2-1     | 24/12/13 13:50:48 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:50:48 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 13:50:06 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 13:50:06 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 13:50:07 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 13:50:07 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 13:50:07 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 13:50:07 INFO Master: Registering worker 172.18.0.6:45547 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:50:07 INFO Master: Registering worker 172.18.0.9:34887 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 13:33:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:33:53 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=41103" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@e538db3bcc6c:41103" "--executor-id" "0" "--hostname" "172.18.0.8" "--cores" "10" "--app-id" "app-20241213133353-0000" "--worker-url" "spark://Worker@172.18.0.8:39467" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 13:33:57 INFO Worker: Asked to kill executor app-20241213133353-0000/0
spark-worker-1-1     | 24/12/13 13:33:57 INFO ExecutorRunner: Runner thread for executor app-20241213133353-0000/0 interrupted
spark-worker-1-1     | 24/12/13 13:33:57 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 13:33:57 INFO Worker: Executor app-20241213133353-0000/0 finished with state KILLED exitStatus 0
spark-worker-1-1     | 24/12/13 13:33:57 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-1-1     | 24/12/13 13:33:57 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213133353-0000, execId=0)
spark-worker-1-1     | 24/12/13 13:33:57 INFO Worker: Cleaning up local directories for application app-20241213133353-0000
spark-worker-1-1     | 24/12/13 13:33:57 INFO ExternalShuffleBlockResolver: Application app-20241213133353-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:41:38.83 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:41:38.83 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:41:38.83 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:41:38.83 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | 24/12/13 13:50:48 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 13:50:48 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:50:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:33:05.08 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-master-1       | 24/12/13 13:50:07 INFO Master: Registering worker 172.18.0.7:35621 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:50:48 INFO Master: Registering app MyAwesomeSpark
spark-master-1       | 24/12/13 13:50:48 INFO Master: Registered app MyAwesomeSpark with ID app-20241213135048-0000
spark-master-1       | 24/12/13 13:50:48 INFO Master: Start scheduling for app app-20241213135048-0000 with rpId: 0
spark-master-1       | 24/12/13 13:50:48 INFO Master: Launching executor app-20241213135048-0000/0 on worker worker-20241213135006-172.18.0.7-35621
spark-master-1       | 24/12/13 13:50:48 INFO Master: Launching executor app-20241213135048-0000/1 on worker worker-20241213135006-172.18.0.6-45547
spark-master-1       | 24/12/13 13:50:48 INFO Master: Launching executor app-20241213135048-0000/2 on worker worker-20241213135006-172.18.0.9-34887
spark-master-1       | 24/12/13 13:50:49 INFO Master: Start scheduling for app app-20241213135048-0000 with rpId: 0
spark-master-1       | 24/12/13 13:50:49 INFO Master: Start scheduling for app app-20241213135048-0000 with rpId: 0
spark-master-1       | 24/12/13 13:50:49 INFO Master: Start scheduling for app app-20241213135048-0000 with rpId: 0
spark-master-1       | 24/12/13 13:50:52 INFO Master: Received unregister request from application app-20241213135048-0000
spark-master-1       | 24/12/13 13:50:52 INFO Master: Removing app app-20241213135048-0000
spark-master-1       | 24/12/13 13:50:52 WARN Master: Got status update for unknown executor app-20241213135048-0000/1
spark-master-1       | 24/12/13 13:50:52 INFO Master: 172.18.0.11:60842 got disassociated, removing it.
spark-master-1       | 24/12/13 13:50:52 INFO Master: 22550a098c1e:39451 got disassociated, removing it.
spark-worker-2-1     | 24/12/13 13:50:48 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=39451" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:39451" "--executor-id" "1" "--hostname" "172.18.0.6" "--cores" "10" "--app-id" "app-20241213135048-0000" "--worker-url" "spark://Worker@172.18.0.6:45547" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 13:50:52 INFO Worker: Asked to kill executor app-20241213135048-0000/1
spark-worker-2-1     | 24/12/13 13:50:52 INFO ExecutorRunner: Runner thread for executor app-20241213135048-0000/1 interrupted
spark-worker-2-1     | 24/12/13 13:50:52 INFO ExecutorRunner: Killing process!
spark-worker-2-1     | 24/12/13 13:50:52 INFO Worker: Executor app-20241213135048-0000/1 finished with state KILLED exitStatus 0
spark-worker-2-1     | 24/12/13 13:50:52 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-worker-2-1     | 24/12/13 13:50:52 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213135048-0000, execId=1)
spark-worker-3-1     | ========================================
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 13:33:07 INFO Worker: Started daemon with process name: 33@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 13:33:07 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 13:50:52 WARN Master: Got status update for unknown executor app-20241213135048-0000/0
spark-master-1       | 24/12/13 13:50:52 WARN Master: Got status update for unknown executor app-20241213135048-0000/2
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:41:38.83 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:41:38.85 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:41:38.87 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:41:38.87 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:41:38.87 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | 24/12/13 13:33:07 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 13:33:07 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 13:33:07 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:33:07 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 13:33:07 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:33:07 INFO SecurityManager: Changing modify acls groups to: 
airflow-worker-1     | [2024-12-13 14:42:17,256: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,256: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,257: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_templates>
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:41:38.88 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-3-1     | 24/12/13 13:33:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 13:41:41 INFO Worker: Started daemon with process name: 34@f484bf7bba3e
spark-worker-1-1     | 24/12/13 13:41:41 INFO SignalUtils: Registering signal handler for TERM
spark-worker-1-1     | 24/12/13 13:41:41 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 13:41:41 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 13:41:41 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 13:41:41 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:41:41 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:41:41 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:41:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:41:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 13:41:42 INFO Utils: Successfully started service 'sparkWorker' on port 42081.
spark-worker-1-1     | 24/12/13 13:41:42 INFO Worker: Worker decommissioning not enabled.
airflow-worker-1     | [2024-12-13 14:42:17,257: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api_virtualenv.py
airflow-worker-1     | [2024-12-13 14:42:17,259: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_api_virtualenv>
airflow-worker-1     | [2024-12-13 14:42:17,259: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensor_decorator.py
airflow-worker-1     | [2024-12-13 14:42:17,260: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sensor_decorator>
airflow-worker-1     | [2024-12-13 14:42:17,260: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_operator.py
airflow-worker-1     | [2024-12-13 14:42:17,261: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_short_circuit_operator>
airflow-worker-1     | [2024-12-13 14:42:17,261: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcom.py
airflow-worker-1     | [2024-12-13 14:42:17,266: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom>
airflow-worker-1     | [2024-12-13 14:42:17,267: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias.py
airflow-worker-1     | [2024-12-13 14:42:17,790: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_consumer>
airflow-worker-1     | [2024-12-13 14:42:17,790: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_consumer>
airflow-worker-1     | [2024-12-13 14:42:17,790: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_producer>
spark-master-1       | 24/12/13 13:54:14 INFO Master: 172.18.0.9:42816 got disassociated, removing it.
spark-master-1       | 24/12/13 13:54:14 INFO Master: 172.18.0.7:58276 got disassociated, removing it.
spark-master-1       | 24/12/13 13:54:14 INFO Master: 172.18.0.9:34887 got disassociated, removing it.
spark-master-1       | 24/12/13 13:54:14 INFO Master: Removing worker worker-20241213135006-172.18.0.9-34887 on 172.18.0.9:34887
spark-master-1       | 24/12/13 13:54:14 INFO Master: Telling app of lost worker: worker-20241213135006-172.18.0.9-34887
spark-master-1       | 24/12/13 13:54:14 INFO Master: 172.18.0.7:35621 got disassociated, removing it.
spark-worker-1-1     | 24/12/13 13:41:42 INFO Worker: Starting Spark worker 172.18.0.8:42081 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 13:41:42 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 13:41:42 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 13:41:42 INFO ResourceUtils: ==============================================================
airflow-worker-1     | [2024-12-13 14:42:17,790: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_producer>
airflow-worker-1     | [2024-12-13 14:42:17,790: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown_taskflow.py
airflow-worker-1     | [2024-12-13 14:42:17,792: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_setup_teardown_taskflow>
airflow-worker-1     | [2024-12-13 14:42:17,792: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api.py
airflow-worker-1     | [2024-12-13 14:42:17,794: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_api>
airflow-worker-1     | [2024-12-13 14:42:17,794: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_labels.py
airflow-worker-1     | [2024-12-13 14:42:17,794: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,794: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,795: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,795: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,795: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,795: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,796: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_labels>
spark-worker-3-1     | 24/12/13 13:33:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 13:33:08 INFO Utils: Successfully started service 'sparkWorker' on port 41621.
spark-master-1       | 24/12/13 13:54:14 INFO Master: Removing worker worker-20241213135006-172.18.0.7-35621 on 172.18.0.7:35621
spark-master-1       | 24/12/13 13:54:14 INFO Master: Telling app of lost worker: worker-20241213135006-172.18.0.7-35621
airflow-worker-1     | [2024-12-13 14:42:17,796: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:42:17,807: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:42:17,807: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_display_name.py
airflow-worker-1     | [2024-12-13 14:42:17,811: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_display_name>
airflow-worker-1     | [2024-12-13 14:42:17,811: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
airflow-worker-1     | [2024-12-13 14:42:17,831: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:17,831: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_consumer_with_no_taskflow>
spark-worker-3-1     | 24/12/13 13:33:08 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 13:33:08 INFO Worker: Starting Spark worker 172.18.0.7:41621 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 13:33:08 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 13:33:08 INFO Worker: Spark home: /opt/bitnami/spark
spark-master-1       | 24/12/13 13:54:14 INFO Master: 172.18.0.6:59686 got disassociated, removing it.
spark-master-1       | 24/12/13 13:54:14 INFO Master: 172.18.0.6:45547 got disassociated, removing it.
spark-worker-2-1     | 24/12/13 13:50:52 INFO Worker: Cleaning up local directories for application app-20241213135048-0000
spark-worker-2-1     | 24/12/13 13:50:52 INFO ExternalShuffleBlockResolver: Application app-20241213135048-0000 removed, cleanupLocalDirs = true
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:54:40.38 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:54:40.39 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:54:40.39 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:54:40.39 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | 24/12/13 13:33:08 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:54:40.39 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:54:40.40 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:54:40.41 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:54:40.41 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:54:40.42 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:54:40.43 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-1-1     | 24/12/13 13:41:42 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 13:41:42 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:41:42 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 13:41:42 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-1-1     | 24/12/13 13:41:42 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 13:41:42 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 13:41:42 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 56 ms (0 ms spent in bootstraps)
spark-worker-1-1     | 24/12/13 13:41:42 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 13:42:39 INFO Worker: Asked to launch executor app-20241213134239-0000/2 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 13:42:39 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 13:42:39 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:42:39 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:42:39 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:42:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:42:39 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=43403" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:43403" "--executor-id" "2" "--hostname" "172.18.0.8" "--cores" "10" "--app-id" "app-20241213134239-0000" "--worker-url" "spark://Worker@172.18.0.8:42081" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 13:42:42 INFO Worker: Asked to kill executor app-20241213134239-0000/2
spark-worker-1-1     | 24/12/13 13:42:42 INFO ExecutorRunner: Runner thread for executor app-20241213134239-0000/2 interrupted
spark-worker-1-1     | 24/12/13 13:42:42 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 13:42:42 INFO Worker: Executor app-20241213134239-0000/2 finished with state KILLED exitStatus 143
spark-worker-1-1     | 24/12/13 13:42:42 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
spark-worker-1-1     | 24/12/13 13:42:42 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213134239-0000, execId=2)
spark-worker-1-1     | 24/12/13 13:42:42 INFO ExternalShuffleBlockResolver: Application app-20241213134239-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | 24/12/13 13:42:42 INFO Worker: Cleaning up local directories for application app-20241213134239-0000
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:50:03.82 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:50:03.83 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 13:54:42 INFO Worker: Started daemon with process name: 34@e92f47d0559d
spark-worker-2-1     | 24/12/13 13:54:42 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 13:54:42 INFO SignalUtils: Registering signal handler for HUP
spark-worker-2-1     | 24/12/13 13:54:42 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 13:54:14 INFO Master: Removing worker worker-20241213135006-172.18.0.6-45547 on 172.18.0.6:45547
spark-master-1       | 24/12/13 13:54:14 INFO Master: Telling app of lost worker: worker-20241213135006-172.18.0.6-45547
spark-master-1       | [38;5;6mspark [38;5;5m13:54:39.97 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:54:39.97 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:54:39.97 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:54:39.97 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:54:39.98 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:54:40.00 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:50:03.83 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:50:03.83 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:50:03.83 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:50:03.86 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:50:03.88 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | 24/12/13 13:54:43 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:54:43 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:54:43 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 13:54:43 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:54:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 13:54:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 13:54:43 INFO Utils: Successfully started service 'sparkWorker' on port 33903.
spark-worker-2-1     | 24/12/13 13:54:43 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 13:54:44 INFO Worker: Starting Spark worker 172.18.0.6:33903 with 10 cores, 2.0 GiB RAM
spark-master-1       | [38;5;6mspark [38;5;5m13:54:40.02 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m13:54:40.03 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m13:54:40.04 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | [38;5;6mspark [38;5;5m13:54:40.06 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-worker-3-1     | 24/12/13 13:33:08 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 13:33:08 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 13:33:08 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
airflow-worker-1     | [2024-12-13 14:42:17,831: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:17,831: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:17,831: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group_decorator.py
airflow-worker-1     | [2024-12-13 14:42:17,835: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_task_group_decorator>
airflow-worker-1     | [2024-12-13 14:42:17,836: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_target_dag.py
airflow-worker-1     | [2024-12-13 14:42:17,837: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_trigger_target_dag>
airflow-worker-1     | [2024-12-13 14:42:17,837: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_external_task_marker_dag.py
airflow-worker-1     | [2024-12-13 14:42:17,839: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_external_task_marker_child>
airflow-worker-1     | [2024-12-13 14:42:17,839: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_external_task_marker_parent>
airflow-worker-1     | [2024-12-13 14:42:17,839: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_skip_dag.py
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 13:54:42 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-master-1       | 24/12/13 13:54:42 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 13:54:42 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 13:54:42 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 13:54:43 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:50:03.90 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:50:03.90 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:50:03.91 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | 24/12/13 13:33:08 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 13:33:08 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-3-1     | 24/12/13 13:33:08 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 13:33:08 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 46 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 13:33:09 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 13:33:53 INFO Worker: Asked to launch executor app-20241213133353-0000/2 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 13:33:53 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:33:53 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 13:33:53 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:33:53 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:33:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 13:33:53 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=41103" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@e538db3bcc6c:41103" "--executor-id" "2" "--hostname" "172.18.0.7" "--cores" "10" "--app-id" "app-20241213133353-0000" "--worker-url" "spark://Worker@172.18.0.7:41621" "--resourceProfileId" "0"
spark-worker-3-1     | 24/12/13 13:33:57 INFO Worker: Asked to kill executor app-20241213133353-0000/2
spark-worker-3-1     | 24/12/13 13:33:57 INFO ExecutorRunner: Runner thread for executor app-20241213133353-0000/2 interrupted
spark-worker-3-1     | 24/12/13 13:33:57 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 13:33:57 INFO Worker: Executor app-20241213133353-0000/2 finished with state KILLED exitStatus 0
airflow-worker-1     | [2024-12-13 14:42:17,842: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_skip_dag>
airflow-worker-1     | [2024-12-13 14:42:17,842: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_workday_timetable.py
airflow-worker-1     | [2024-12-13 14:42:17,843: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_workday_timetable>
airflow-worker-1     | [2024-12-13 14:42:17,843: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_dag.py
airflow-worker-1     | [2024-12-13 14:42:17,844: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_dag>
airflow-worker-1     | [2024-12-13 14:42:17,844: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_operator.py
airflow-worker-1     | [2024-12-13 14:42:17,846: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_python_operator>
spark-worker-3-1     | 24/12/13 13:33:57 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
spark-worker-3-1     | 24/12/13 13:33:57 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213133353-0000, execId=2)
spark-worker-3-1     | 24/12/13 13:33:57 INFO ExternalShuffleBlockResolver: Application app-20241213133353-0000 removed, cleanupLocalDirs = true
spark-worker-3-1     | 24/12/13 13:33:57 INFO Worker: Cleaning up local directories for application app-20241213133353-0000
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:41:38.80 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:41:38.80 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:41:38.80 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:41:38.80 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:41:38.80 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:41:38.81 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:41:38.82 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | 24/12/13 13:54:44 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 13:54:44 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 13:54:44 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:54:44 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 13:54:44 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:54:44 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 13:54:44 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 13:54:44 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 13:54:44 INFO Worker: Connecting to master spark-master:7077...
spark-master-1       | 24/12/13 13:54:43 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 13:54:43 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 13:54:43 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 13:54:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 13:54:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 13:54:43 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 13:54:43 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 13:54:44 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 38 ms (0 ms spent in bootstraps)
spark-worker-2-1     | 24/12/13 13:54:44 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | ========================================
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 13:50:05 INFO Worker: Started daemon with process name: 34@f484bf7bba3e
spark-worker-1-1     | 24/12/13 13:50:05 INFO SignalUtils: Registering signal handler for TERM
spark-worker-1-1     | 24/12/13 13:50:05 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:41:38.83 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:41:38.83 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:41:38.84 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
spark-worker-2-1     | 24/12/13 13:55:33 INFO Worker: Asked to launch executor app-20241213135533-0000/0 for MyAwesomeSpark
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 13:41:41 INFO Worker: Started daemon with process name: 33@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 13:41:41 INFO SignalUtils: Registering signal handler for TERM
spark-worker-3-1     | 24/12/13 13:41:41 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 13:54:44 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 13:54:44 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 13:54:44 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 13:54:44 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 13:54:44 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 13:54:44 INFO Master: Registering worker 172.18.0.7:33809 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:54:44 INFO Master: Registering worker 172.18.0.8:42559 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:54:44 INFO Master: Registering worker 172.18.0.6:33903 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:55:33 INFO Master: Registering app MyAwesomeSpark
spark-master-1       | 24/12/13 13:55:33 INFO Master: Registered app MyAwesomeSpark with ID app-20241213135533-0000
spark-master-1       | 24/12/13 13:55:33 INFO Master: Start scheduling for app app-20241213135533-0000 with rpId: 0
spark-master-1       | 24/12/13 13:55:33 INFO Master: Launching executor app-20241213135533-0000/0 on worker worker-20241213135444-172.18.0.6-33903
spark-master-1       | 24/12/13 13:55:33 INFO Master: Launching executor app-20241213135533-0000/1 on worker worker-20241213135444-172.18.0.8-42559
spark-master-1       | 24/12/13 13:55:33 INFO Master: Launching executor app-20241213135533-0000/2 on worker worker-20241213135444-172.18.0.7-33809
spark-master-1       | 24/12/13 13:55:33 INFO Master: Start scheduling for app app-20241213135533-0000 with rpId: 0
spark-master-1       | 24/12/13 13:55:33 INFO Master: Start scheduling for app app-20241213135533-0000 with rpId: 0
spark-master-1       | 24/12/13 13:55:33 INFO Master: Start scheduling for app app-20241213135533-0000 with rpId: 0
spark-master-1       | 24/12/13 13:55:36 INFO Master: Received unregister request from application app-20241213135533-0000
spark-master-1       | 24/12/13 13:55:36 INFO Master: Removing app app-20241213135533-0000
spark-master-1       | 24/12/13 13:55:36 WARN Master: Got status update for unknown executor app-20241213135533-0000/0
spark-master-1       | 24/12/13 13:55:36 WARN Master: Got status update for unknown executor app-20241213135533-0000/1
spark-master-1       | 24/12/13 13:55:36 WARN Master: Got status update for unknown executor app-20241213135533-0000/2
spark-master-1       | 24/12/13 13:55:36 INFO Master: 172.18.0.9:36828 got disassociated, removing it.
spark-master-1       | 24/12/13 13:55:36 INFO Master: 22550a098c1e:46315 got disassociated, removing it.
spark-master-1       | 24/12/13 13:57:12 INFO Master: 172.18.0.7:57452 got disassociated, removing it.
spark-master-1       | 24/12/13 13:57:12 INFO Master: 172.18.0.7:33809 got disassociated, removing it.
spark-master-1       | 24/12/13 13:57:12 INFO Master: Removing worker worker-20241213135444-172.18.0.7-33809 on 172.18.0.7:33809
spark-master-1       | 24/12/13 13:57:12 INFO Master: Telling app of lost worker: worker-20241213135444-172.18.0.7-33809
spark-master-1       | 24/12/13 13:57:12 INFO Master: 172.18.0.6:55724 got disassociated, removing it.
spark-master-1       | 24/12/13 13:57:12 INFO Master: 172.18.0.6:33903 got disassociated, removing it.
spark-master-1       | 24/12/13 13:57:12 INFO Master: Removing worker worker-20241213135444-172.18.0.6-33903 on 172.18.0.6:33903
spark-master-1       | 24/12/13 13:57:12 INFO Master: Telling app of lost worker: worker-20241213135444-172.18.0.6-33903
spark-master-1       | 24/12/13 13:57:12 INFO Master: 172.18.0.8:42312 got disassociated, removing it.
spark-master-1       | 24/12/13 13:57:12 INFO Master: 172.18.0.8:42559 got disassociated, removing it.
spark-master-1       | 24/12/13 13:57:12 INFO Master: Removing worker worker-20241213135444-172.18.0.8-42559 on 172.18.0.8:42559
spark-master-1       | 24/12/13 13:57:12 INFO Master: Telling app of lost worker: worker-20241213135444-172.18.0.8-42559
spark-master-1       | [38;5;6mspark [38;5;5m13:57:29.94 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:57:29.94 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:57:29.94 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:57:29.94 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m13:57:29.95 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m13:57:29.99 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m13:57:30.01 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m13:57:30.01 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m13:57:30.02 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | [38;5;6mspark [38;5;5m13:57:30.02 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 13:57:32 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-master-1       | 24/12/13 13:57:32 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 13:57:32 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 13:57:32 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 13:57:32 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | 24/12/13 13:57:32 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 13:57:32 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 13:57:32 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:50:05 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 13:50:06 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 13:50:06 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 13:57:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 13:57:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 13:57:32 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-worker-1-1     | 24/12/13 13:50:06 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:50:06 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:50:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:50:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 13:57:32 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 13:57:32 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 13:57:32 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 13:57:33 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-worker-1-1     | 24/12/13 13:50:06 INFO Utils: Successfully started service 'sparkWorker' on port 34887.
spark-worker-1-1     | 24/12/13 13:50:06 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 13:50:07 INFO Worker: Starting Spark worker 172.18.0.9:34887 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 13:50:07 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 13:50:07 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 13:50:07 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:50:07 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 13:55:33 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:55:33 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:55:33 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 13:55:33 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:55:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 13:57:33 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 13:57:33 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 13:57:33 INFO Master: Registering worker 172.18.0.8:44617 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 13:55:33 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=46315" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:46315" "--executor-id" "0" "--hostname" "172.18.0.6" "--cores" "10" "--app-id" "app-20241213135533-0000" "--worker-url" "spark://Worker@172.18.0.6:33903" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 13:55:36 INFO Worker: Asked to kill executor app-20241213135533-0000/0
spark-worker-2-1     | 24/12/13 13:55:36 INFO ExecutorRunner: Runner thread for executor app-20241213135533-0000/0 interrupted
spark-worker-2-1     | 24/12/13 13:55:36 INFO ExecutorRunner: Killing process!
spark-worker-2-1     | 24/12/13 13:55:36 INFO Worker: Executor app-20241213135533-0000/0 finished with state KILLED exitStatus 143
spark-worker-2-1     | 24/12/13 13:55:36 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-2-1     | 24/12/13 13:55:36 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213135533-0000, execId=0)
spark-worker-2-1     | 24/12/13 13:55:36 INFO ExternalShuffleBlockResolver: Application app-20241213135533-0000 removed, cleanupLocalDirs = true
spark-worker-2-1     | 24/12/13 13:55:36 INFO Worker: Cleaning up local directories for application app-20241213135533-0000
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:57:30.27 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:57:30.27 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:57:30.27 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:57:30.28 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:57:30.28 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:57:30.29 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:57:30.31 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | 24/12/13 13:50:07 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:50:07 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 13:50:07 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 13:41:41 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 13:41:41 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:41:41 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 13:41:41 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:41:41 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:41:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 13:41:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 13:41:42 INFO Utils: Successfully started service 'sparkWorker' on port 45553.
spark-worker-3-1     | 24/12/13 13:41:42 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 13:41:42 INFO Worker: Starting Spark worker 172.18.0.7:45553 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 13:41:42 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 13:41:42 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 13:41:42 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:50:07 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 13:50:07 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 13:50:07 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 36 ms (0 ms spent in bootstraps)
spark-worker-1-1     | 24/12/13 13:50:07 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
airflow-worker-1     | [2024-12-13 14:42:17,846: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group.py
airflow-worker-1     | [2024-12-13 14:42:17,848: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_task_group>
airflow-worker-1     | [2024-12-13 14:42:17,848: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_datetime_operator.py
airflow-worker-1     | [2024-12-13 14:42:17,848: DEBUG/ForkPoolWorker-15] Failed to find locale C
spark-master-1       | 24/12/13 13:57:33 INFO Master: Registering worker 172.18.0.7:42061 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:57:33 INFO Master: Registering worker 172.18.0.6:36503 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 13:58:20 INFO Master: Registering app MyAwesomeSpark
spark-master-1       | 24/12/13 13:58:20 INFO Master: Registered app MyAwesomeSpark with ID app-20241213135820-0000
spark-master-1       | 24/12/13 13:58:20 INFO Master: Start scheduling for app app-20241213135820-0000 with rpId: 0
spark-master-1       | 24/12/13 13:58:20 INFO Master: Launching executor app-20241213135820-0000/0 on worker worker-20241213135732-172.18.0.7-42061
spark-master-1       | 24/12/13 13:58:21 INFO Master: Launching executor app-20241213135820-0000/1 on worker worker-20241213135732-172.18.0.6-36503
spark-worker-3-1     | 24/12/13 13:41:42 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 13:41:42 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:57:30.31 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:57:30.31 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m13:57:30.33 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | 24/12/13 13:41:42 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 13:41:42 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 13:41:42 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-1-1     | 24/12/13 13:50:48 INFO Worker: Asked to launch executor app-20241213135048-0000/2 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 13:50:48 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 13:50:48 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:50:48 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:50:48 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:50:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:50:48 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=39451" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:39451" "--executor-id" "2" "--hostname" "172.18.0.9" "--cores" "10" "--app-id" "app-20241213135048-0000" "--worker-url" "spark://Worker@172.18.0.9:34887" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 13:50:52 INFO Worker: Asked to kill executor app-20241213135048-0000/2
spark-master-1       | 24/12/13 13:58:21 INFO Master: Launching executor app-20241213135820-0000/2 on worker worker-20241213135732-172.18.0.8-44617
spark-master-1       | 24/12/13 13:58:21 INFO Master: Start scheduling for app app-20241213135820-0000 with rpId: 0
spark-master-1       | 24/12/13 13:58:21 INFO Master: Start scheduling for app app-20241213135820-0000 with rpId: 0
spark-master-1       | 24/12/13 13:58:21 INFO Master: Start scheduling for app app-20241213135820-0000 with rpId: 0
spark-master-1       | 24/12/13 13:58:35 INFO Master: Received unregister request from application app-20241213135820-0000
spark-master-1       | 24/12/13 13:58:35 INFO Master: Removing app app-20241213135820-0000
airflow-worker-1     | [2024-12-13 14:42:17,848: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,848: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,849: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,849: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,849: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,849: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,849: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,850: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,850: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,850: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 13:57:32 INFO Worker: Started daemon with process name: 34@e92f47d0559d
spark-worker-2-1     | 24/12/13 13:57:32 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 13:57:32 INFO SignalUtils: Registering signal handler for HUP
spark-worker-2-1     | 24/12/13 13:57:32 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 13:58:35 INFO Master: 172.18.0.11:41124 got disassociated, removing it.
spark-master-1       | 24/12/13 13:58:35 INFO Master: 22550a098c1e:45293 got disassociated, removing it.
spark-worker-3-1     | 24/12/13 13:41:42 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 13:41:42 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 34 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 13:41:42 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 13:42:39 INFO Worker: Asked to launch executor app-20241213134239-0000/1 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 13:42:39 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:42:39 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 13:42:39 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:42:39 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:42:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 13:42:39 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=43403" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:43403" "--executor-id" "1" "--hostname" "172.18.0.7" "--cores" "10" "--app-id" "app-20241213134239-0000" "--worker-url" "spark://Worker@172.18.0.7:45553" "--resourceProfileId" "0"
airflow-worker-1     | [2024-12-13 14:42:17,850: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,850: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator_2>
airflow-worker-1     | [2024-12-13 14:42:17,850: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,850: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,850: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,851: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator>
airflow-worker-1     | [2024-12-13 14:42:17,851: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,851: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,851: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,851: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator_3>
airflow-worker-1     | [2024-12-13 14:42:17,851: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dag_decorator.py
spark-master-1       | 24/12/13 13:58:35 WARN Master: Got status update for unknown executor app-20241213135820-0000/2
spark-master-1       | 24/12/13 13:58:35 WARN Master: Got status update for unknown executor app-20241213135820-0000/0
spark-master-1       | 24/12/13 13:58:35 WARN Master: Got status update for unknown executor app-20241213135820-0000/1
spark-master-1       | 24/12/13 13:59:14 INFO Master: 172.18.0.7:57586 got disassociated, removing it.
spark-master-1       | 24/12/13 13:59:14 INFO Master: 172.18.0.7:42061 got disassociated, removing it.
spark-master-1       | 24/12/13 13:59:14 INFO Master: Removing worker worker-20241213135732-172.18.0.7-42061 on 172.18.0.7:42061
spark-master-1       | 24/12/13 13:59:14 INFO Master: Telling app of lost worker: worker-20241213135732-172.18.0.7-42061
spark-worker-2-1     | 24/12/13 13:57:32 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:57:32 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:57:32 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 13:57:32 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:57:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
airflow-worker-1     | [2024-12-13 14:42:17,923: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dag_decorator>
spark-worker-2-1     | 24/12/13 13:57:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 13:57:32 INFO Utils: Successfully started service 'sparkWorker' on port 42061.
spark-worker-2-1     | 24/12/13 13:57:32 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 13:57:33 INFO Worker: Starting Spark worker 172.18.0.7:42061 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 13:57:33 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 13:42:42 INFO Worker: Asked to kill executor app-20241213134239-0000/1
spark-worker-3-1     | 24/12/13 13:42:42 INFO ExecutorRunner: Runner thread for executor app-20241213134239-0000/1 interrupted
spark-worker-3-1     | 24/12/13 13:42:42 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 13:42:42 INFO Worker: Executor app-20241213134239-0000/1 finished with state KILLED exitStatus 143
spark-worker-3-1     | 24/12/13 13:42:42 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-master-1       | 24/12/13 13:59:14 INFO Master: 172.18.0.8:42472 got disassociated, removing it.
spark-master-1       | 24/12/13 13:59:14 INFO Master: 172.18.0.8:44617 got disassociated, removing it.
spark-master-1       | 24/12/13 13:59:14 INFO Master: Removing worker worker-20241213135732-172.18.0.8-44617 on 172.18.0.8:44617
spark-master-1       | 24/12/13 13:59:14 INFO Master: Telling app of lost worker: worker-20241213135732-172.18.0.8-44617
spark-master-1       | 24/12/13 13:59:14 INFO Master: 172.18.0.6:47888 got disassociated, removing it.
airflow-worker-1     | [2024-12-13 14:42:17,924: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_day_of_week_operator.py
spark-worker-1-1     | 24/12/13 13:50:52 INFO ExecutorRunner: Runner thread for executor app-20241213135048-0000/2 interrupted
spark-worker-1-1     | 24/12/13 13:50:52 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 13:42:42 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213134239-0000, execId=1)
spark-worker-3-1     | 24/12/13 13:42:42 INFO Worker: Cleaning up local directories for application app-20241213134239-0000
spark-worker-3-1     | 24/12/13 13:42:42 INFO ExternalShuffleBlockResolver: Application app-20241213134239-0000 removed, cleanupLocalDirs = true
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:50:03.70 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:50:03.71 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:50:03.71 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:50:03.71 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:50:03.71 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:50:03.73 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:50:03.75 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:50:03.75 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | 24/12/13 13:59:14 INFO Master: 172.18.0.6:36503 got disassociated, removing it.
spark-master-1       | 24/12/13 13:59:14 INFO Master: Removing worker worker-20241213135732-172.18.0.6-36503 on 172.18.0.6:36503
spark-master-1       | 24/12/13 13:59:14 INFO Master: Telling app of lost worker: worker-20241213135732-172.18.0.6-36503
spark-master-1       | [38;5;6mspark [38;5;5m14:00:09.80 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m14:00:09.80 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:50:03.75 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:50:03.78 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
spark-master-1       | [38;5;6mspark [38;5;5m14:00:09.80 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:00:09.80 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:00:09.80 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m14:00:09.83 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m14:00:09.85 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m14:00:09.86 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | 24/12/13 13:50:52 INFO Worker: Executor app-20241213135048-0000/2 finished with state KILLED exitStatus 0
spark-worker-1-1     | 24/12/13 13:50:52 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
spark-worker-1-1     | 24/12/13 13:50:52 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213135048-0000, execId=2)
spark-worker-1-1     | 24/12/13 13:50:52 INFO Worker: Cleaning up local directories for application app-20241213135048-0000
spark-worker-1-1     | 24/12/13 13:50:52 INFO ExternalShuffleBlockResolver: Application app-20241213135048-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:54:40.45 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:54:40.45 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:54:40.45 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:54:40.45 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:54:40.45 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:54:40.46 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:54:40.52 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:54:40.53 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:54:40.54 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:54:40.56 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-2-1     | 24/12/13 13:57:33 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 13:57:33 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:57:33 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 13:57:33 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 13:57:33 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 13:57:33 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-master-1       | [38;5;6mspark [38;5;5m14:00:09.87 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m14:00:09.88 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
airflow-worker-1     | [2024-12-13 14:42:17,924: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,924: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,925: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,926: DEBUG/ForkPoolWorker-15] Failed to find locale C
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 14:00:12 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-master-1       | 24/12/13 14:00:12 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 14:00:12 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 14:00:12 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 14:00:12 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:57:33 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 13:57:33 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 13:57:33 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 34 ms (0 ms spent in bootstraps)
spark-worker-2-1     | 24/12/13 13:57:33 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
airflow-worker-1     | [2024-12-13 14:42:17,926: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,926: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-master-1       | 24/12/13 14:00:12 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 14:00:12 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 14:00:12 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 14:00:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 14:00:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
airflow-worker-1     | [2024-12-13 14:42:17,926: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_weekday_branch_operator>
airflow-worker-1     | [2024-12-13 14:42:17,927: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_inlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:42:17,927: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,927: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,927: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,928: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,928: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,928: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,929: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,929: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
spark-worker-2-1     | 24/12/13 13:58:21 INFO Worker: Asked to launch executor app-20241213135820-0000/0 for MyAwesomeSpark
spark-worker-2-1     | 24/12/13 13:58:21 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 13:58:21 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:58:21 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 13:50:05 INFO Worker: Started daemon with process name: 33@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 13:50:05 INFO SignalUtils: Registering signal handler for TERM
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 13:54:42 INFO Worker: Started daemon with process name: 34@f484bf7bba3e
spark-worker-1-1     | 24/12/13 13:54:42 INFO SignalUtils: Registering signal handler for TERM
airflow-worker-1     | [2024-12-13 14:42:17,929: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,929: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: read_dataset_event_from_classic>
spark-worker-2-1     | 24/12/13 13:58:21 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 13:58:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:54:42 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 13:54:42 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 13:54:43 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 13:54:43 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 13:58:21 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45293" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:45293" "--executor-id" "0" "--hostname" "172.18.0.7" "--cores" "10" "--app-id" "app-20241213135820-0000" "--worker-url" "spark://Worker@172.18.0.7:42061" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 13:58:35 INFO Worker: Asked to kill executor app-20241213135820-0000/0
spark-worker-2-1     | 24/12/13 13:58:35 INFO ExecutorRunner: Runner thread for executor app-20241213135820-0000/0 interrupted
spark-worker-2-1     | 24/12/13 13:58:35 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 13:54:43 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:54:43 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:54:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:54:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 13:54:43 INFO Utils: Successfully started service 'sparkWorker' on port 42559.
spark-worker-2-1     | 24/12/13 13:58:35 INFO Worker: Executor app-20241213135820-0000/0 finished with state KILLED exitStatus 143
spark-worker-2-1     | 24/12/13 13:58:35 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-2-1     | 24/12/13 13:58:35 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213135820-0000, execId=0)
spark-master-1       | 24/12/13 14:00:13 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 14:00:13 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 13:50:05 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 13:50:05 INFO SignalUtils: Registering signal handler for INT
airflow-worker-1     | [2024-12-13 14:42:17,929: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,929: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,929: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,930: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: read_dataset_event>
airflow-worker-1     | [2024-12-13 14:42:17,930: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping.py
spark-worker-1-1     | 24/12/13 13:54:43 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 13:54:44 INFO Worker: Starting Spark worker 172.18.0.8:42559 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 13:54:44 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 13:54:44 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 13:54:44 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:54:44 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-master-1       | 24/12/13 14:00:13 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 14:00:13 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 14:00:13 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 14:00:14 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
airflow-worker-1     | [2024-12-13 14:42:17,933: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dynamic_task_mapping>
airflow-worker-1     | [2024-12-13 14:42:17,933: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_subdag_operator.py
airflow-worker-1     | [2024-12-13 14:42:17,934: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,934: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,934: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-master-1       | 24/12/13 14:00:14 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 14:00:14 INFO Master: Registering worker 172.18.0.6:35139 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 14:00:14 INFO Master: Registering worker 172.18.0.8:36401 with 10 cores, 2.0 GiB RAM
airflow-worker-1     | [2024-12-13 14:42:17,955: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,955: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,955: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:17,963: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator.section-1>
airflow-worker-1     | [2024-12-13 14:42:17,963: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator.section-2>
airflow-worker-1     | [2024-12-13 14:42:17,963: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator>
airflow-worker-1     | [2024-12-13 14:42:17,964: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial.py
airflow-worker-1     | [2024-12-13 14:42:17,965: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial>
airflow-worker-1     | [2024-12-13 14:42:17,965: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_complex.py
airflow-worker-1     | [2024-12-13 14:42:17,968: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_complex>
airflow-worker-1     | [2024-12-13 14:42:17,968: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_python_dop_operator_3.py
airflow-worker-1     | [2024-12-13 14:42:17,969: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:17,969: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:17,969: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-worker-3-1     | 24/12/13 13:50:06 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:50:06 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 13:50:06 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:50:06 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:50:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 13:50:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 13:58:35 INFO ExternalShuffleBlockResolver: Application app-20241213135820-0000 removed, cleanupLocalDirs = true
spark-worker-2-1     | 24/12/13 13:58:35 INFO Worker: Cleaning up local directories for application app-20241213135820-0000
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:00:10.04 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:00:10.04 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:00:10.05 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:00:10.05 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:00:10.05 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:00:10.07 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | 24/12/13 13:54:44 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:54:44 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 13:54:44 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-master-1       | 24/12/13 14:00:14 INFO Master: Registering worker 172.18.0.7:46879 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 14:00:53 INFO Master: Registering app MyAwesomeSpark
spark-master-1       | 24/12/13 14:00:53 INFO Master: Registered app MyAwesomeSpark with ID app-20241213140053-0000
spark-master-1       | 24/12/13 14:00:53 INFO Master: Start scheduling for app app-20241213140053-0000 with rpId: 0
spark-master-1       | 24/12/13 14:00:53 INFO Master: Launching executor app-20241213140053-0000/0 on worker worker-20241213140013-172.18.0.8-36401
spark-master-1       | 24/12/13 14:00:53 INFO Master: Launching executor app-20241213140053-0000/1 on worker worker-20241213140013-172.18.0.6-35139
spark-worker-3-1     | 24/12/13 13:50:06 INFO Utils: Successfully started service 'sparkWorker' on port 35621.
spark-worker-3-1     | 24/12/13 13:50:06 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 13:50:07 INFO Worker: Starting Spark worker 172.18.0.7:35621 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 13:50:07 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 13:50:07 INFO Worker: Spark home: /opt/bitnami/spark
spark-master-1       | 24/12/13 14:00:53 INFO Master: Launching executor app-20241213140053-0000/2 on worker worker-20241213140013-172.18.0.7-46879
spark-master-1       | 24/12/13 14:00:53 INFO Master: Start scheduling for app app-20241213140053-0000 with rpId: 0
spark-master-1       | 24/12/13 14:00:53 INFO Master: Start scheduling for app app-20241213140053-0000 with rpId: 0
spark-master-1       | 24/12/13 14:00:53 INFO Master: Start scheduling for app app-20241213140053-0000 with rpId: 0
spark-master-1       | 24/12/13 14:00:57 INFO Master: Received unregister request from application app-20241213140053-0000
spark-master-1       | 24/12/13 14:00:57 INFO Master: Removing app app-20241213140053-0000
spark-master-1       | 24/12/13 14:00:57 INFO Master: 172.18.0.12:37074 got disassociated, removing it.
spark-master-1       | 24/12/13 14:00:57 INFO Master: 22550a098c1e:45213 got disassociated, removing it.
spark-master-1       | 24/12/13 14:00:57 WARN Master: Got status update for unknown executor app-20241213140053-0000/2
spark-master-1       | 24/12/13 14:00:57 WARN Master: Got status update for unknown executor app-20241213140053-0000/1
spark-worker-1-1     | 24/12/13 13:54:44 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 13:54:44 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 13:54:44 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 46 ms (0 ms spent in bootstraps)
spark-worker-1-1     | 24/12/13 13:54:44 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
airflow-worker-1     | [2024-12-13 14:42:17,970: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_dop_operator_v3>
airflow-worker-1     | [2024-12-13 14:42:17,970: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/subdags/subdag.py
airflow-worker-1     | [2024-12-13 14:42:17,970: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/event_listener.py
airflow-worker-1     | [2024-12-13 14:42:17,971: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/listener_plugin.py
airflow-worker-1     | [2024-12-13 14:42:17,971: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/decreasing_priority_weight_strategy.py
airflow-worker-1     | [2024-12-13 14:42:17,971: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/workday.py
spark-worker-1-1     | 24/12/13 13:55:33 INFO Worker: Asked to launch executor app-20241213135533-0000/1 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 13:55:33 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 13:55:33 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:55:33 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:55:33 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:55:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 14:00:57 WARN Master: Got status update for unknown executor app-20241213140053-0000/0
spark-master-1       | 24/12/13 14:01:16 INFO Master: 172.18.0.7:59252 got disassociated, removing it.
spark-master-1       | 24/12/13 14:01:16 INFO Master: 172.18.0.7:46879 got disassociated, removing it.
spark-master-1       | 24/12/13 14:01:16 INFO Master: Removing worker worker-20241213140013-172.18.0.7-46879 on 172.18.0.7:46879
spark-worker-1-1     | 24/12/13 13:55:33 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=46315" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:46315" "--executor-id" "1" "--hostname" "172.18.0.8" "--cores" "10" "--app-id" "app-20241213135533-0000" "--worker-url" "spark://Worker@172.18.0.8:42559" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 13:55:36 INFO Worker: Asked to kill executor app-20241213135533-0000/1
spark-worker-1-1     | 24/12/13 13:55:36 INFO ExecutorRunner: Runner thread for executor app-20241213135533-0000/1 interrupted
spark-worker-1-1     | 24/12/13 13:55:36 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 13:55:36 INFO Worker: Executor app-20241213135533-0000/1 finished with state KILLED exitStatus 143
spark-worker-1-1     | 24/12/13 13:55:36 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-worker-1-1     | 24/12/13 13:55:36 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213135533-0000, execId=1)
spark-worker-1-1     | 24/12/13 13:55:36 INFO Worker: Cleaning up local directories for application app-20241213135533-0000
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:00:10.08 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:00:10.08 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:00:10.09 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
airflow-worker-1     | [2024-12-13 14:42:18,020: DEBUG/ForkPoolWorker-15] Plugins are already loaded. Skipping.
airflow-worker-1     | [2024-12-13 14:42:18,020: DEBUG/ForkPoolWorker-15] Integrate DAG plugins
airflow-worker-1     | [2024-12-13 14:42:18,024: DEBUG/ForkPoolWorker-15] previous_execution_date was called
airflow-worker-1     | [2024-12-13 14:42:18,037: INFO/ForkPoolWorker-15] Running <TaskInstance: pipeline.split manual__2024-12-13T14:42:07.082582+00:00 [queued]> on host 65b6e979e5f1
spark-worker-1-1     | 24/12/13 13:55:36 INFO ExternalShuffleBlockResolver: Application app-20241213135533-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:57:30.30 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:57:30.30 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:57:30.30 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:57:30.31 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:57:30.31 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:57:30.33 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:57:30.36 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:57:30.36 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:57:30.36 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m13:57:30.37 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:00:10.10 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 14:00:12 INFO Worker: Started daemon with process name: 34@e92f47d0559d
spark-worker-2-1     | 24/12/13 14:00:12 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 14:00:12 INFO SignalUtils: Registering signal handler for HUP
spark-worker-2-1     | 24/12/13 14:00:12 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 14:00:12 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | 24/12/13 14:01:16 INFO Master: Telling app of lost worker: worker-20241213140013-172.18.0.7-46879
spark-master-1       | 24/12/13 14:01:16 INFO Master: 172.18.0.6:52924 got disassociated, removing it.
spark-master-1       | 24/12/13 14:01:16 INFO Master: 172.18.0.6:35139 got disassociated, removing it.
spark-master-1       | 24/12/13 14:01:16 INFO Master: Removing worker worker-20241213140013-172.18.0.6-35139 on 172.18.0.6:35139
spark-master-1       | 24/12/13 14:01:16 INFO Master: Telling app of lost worker: worker-20241213140013-172.18.0.6-35139
spark-master-1       | 24/12/13 14:01:16 INFO Master: 172.18.0.8:34108 got disassociated, removing it.
spark-master-1       | 24/12/13 14:01:16 INFO Master: 172.18.0.8:36401 got disassociated, removing it.
spark-master-1       | 24/12/13 14:01:16 INFO Master: Removing worker worker-20241213140013-172.18.0.8-36401 on 172.18.0.8:36401
spark-master-1       | 24/12/13 14:01:16 INFO Master: Telling app of lost worker: worker-20241213140013-172.18.0.8-36401
spark-master-1       | [38;5;6mspark [38;5;5m14:02:39.97 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m14:02:39.98 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:02:39.98 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:02:39.98 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:02:39.98 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m14:02:40.02 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m14:02:40.04 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m14:02:40.05 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | [38;5;6mspark [38;5;5m14:02:40.05 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m14:02:40.06 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
airflow-worker-1     | [2024-12-13 14:42:18,037: DEBUG/ForkPoolWorker-15] Disposing DB connection pool (PID 79)
airflow-worker-1     | [2024-12-13 14:42:18,037: DEBUG/ForkPoolWorker-15] Setting up DB connection pool (PID 79)
airflow-worker-1     | [2024-12-13 14:42:18,037: DEBUG/ForkPoolWorker-15] settings.prepare_engine_args(): Using NullPool
airflow-worker-1     | [2024-12-13 14:42:19,636: INFO/MainProcess] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[4bfa70d0-8f9d-47ad-ad62-f601a99fd476] received
spark-worker-3-1     | 24/12/13 13:50:07 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 13:50:07 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 13:50:07 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 13:50:07 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 13:50:07 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 14:00:12 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 14:00:12 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 14:00:12 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 14:00:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 14:00:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 14:00:13 INFO Utils: Successfully started service 'sparkWorker' on port 46879.
spark-worker-2-1     | 24/12/13 14:00:13 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 14:00:13 INFO Worker: Starting Spark worker 172.18.0.7:46879 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 13:50:07 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-3-1     | 24/12/13 13:50:07 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 13:50:07 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 34 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 13:50:07 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 13:50:48 INFO Worker: Asked to launch executor app-20241213135048-0000/0 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 13:50:48 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:50:48 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 13:50:48 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:50:48 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-2-1     | 24/12/13 14:00:13 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 14:00:13 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 14:00:13 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 14:00:13 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 14:00:13 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 14:00:13 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 14:00:14 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 14:00:14 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 14:00:14 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 14:00:14 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.3:7077 after 27 ms (0 ms spent in bootstraps)
spark-worker-2-1     | 24/12/13 14:00:14 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 14:00:53 INFO Worker: Asked to launch executor app-20241213140053-0000/2 for MyAwesomeSpark
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 14:02:42 INFO Master: Started daemon with process name: 35@f9573584fb53
spark-master-1       | 24/12/13 14:02:42 INFO SignalUtils: Registering signal handler for TERM
airflow-worker-1     | [2024-12-13 14:42:19,735: DEBUG/ForkPoolWorker-15] Calling callbacks: []
airflow-worker-1     | [2024-12-13 14:42:19,768: INFO/ForkPoolWorker-16] [4bfa70d0-8f9d-47ad-ad62-f601a99fd476] Executing command in Celery: ['airflow', 'tasks', 'run', 'pipeline', 'upload_to_minio', 'manual__2024-12-13T14:42:07.082582+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-worker-1     | [2024-12-13 14:42:19,776: INFO/ForkPoolWorker-15] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[6789a300-d913-4a6d-aee0-508369a38049] succeeded in 3.5427096679995884s: None
spark-worker-3-1     | 24/12/13 13:50:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 13:50:48 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=39451" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:39451" "--executor-id" "0" "--hostname" "172.18.0.7" "--cores" "10" "--app-id" "app-20241213135048-0000" "--worker-url" "spark://Worker@172.18.0.7:35621" "--resourceProfileId" "0"
spark-worker-3-1     | 24/12/13 13:50:52 INFO Worker: Asked to kill executor app-20241213135048-0000/0
spark-worker-3-1     | 24/12/13 13:50:52 INFO ExecutorRunner: Runner thread for executor app-20241213135048-0000/0 interrupted
spark-worker-3-1     | 24/12/13 13:50:52 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 13:50:52 INFO Worker: Executor app-20241213135048-0000/0 finished with state KILLED exitStatus 0
spark-worker-3-1     | 24/12/13 13:50:52 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-3-1     | 24/12/13 13:50:52 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213135048-0000, execId=0)
spark-worker-2-1     | 24/12/13 14:00:53 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 14:00:53 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 14:00:53 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 14:00:53 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 14:00:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 14:00:53 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45213" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:45213" "--executor-id" "2" "--hostname" "172.18.0.7" "--cores" "10" "--app-id" "app-20241213140053-0000" "--worker-url" "spark://Worker@172.18.0.7:46879" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 14:00:57 INFO Worker: Asked to kill executor app-20241213140053-0000/2
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 13:57:32 INFO Worker: Started daemon with process name: 34@f484bf7bba3e
spark-worker-1-1     | 24/12/13 13:57:32 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 14:02:42 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 14:02:42 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 14:02:42 INFO SecurityManager: Changing view acls to: spark
airflow-worker-1     | [2024-12-13 14:42:20,213: INFO/ForkPoolWorker-16] Filling up the DagBag from /opt/airflow/dags/pipeline.py
airflow-worker-1     | [2024-12-13 14:42:20,526: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: pipeline>
airflow-worker-1     | [2024-12-13 14:42:20,526: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcomargs.py
airflow-worker-1     | [2024-12-13 14:42:20,533: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_xcom_args_with_operators>
airflow-worker-1     | [2024-12-13 14:42:20,533: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_xcom_args>
spark-worker-3-1     | 24/12/13 13:50:52 INFO ExternalShuffleBlockResolver: Application app-20241213135048-0000 removed, cleanupLocalDirs = true
spark-worker-3-1     | 24/12/13 13:50:52 INFO Worker: Cleaning up local directories for application app-20241213135048-0000
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:54:40.45 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | 24/12/13 14:02:42 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 14:00:57 INFO ExecutorRunner: Runner thread for executor app-20241213140053-0000/2 interrupted
spark-worker-2-1     | 24/12/13 14:00:57 INFO ExecutorRunner: Killing process!
spark-worker-2-1     | 24/12/13 14:00:57 INFO Worker: Executor app-20241213140053-0000/2 finished with state KILLED exitStatus 143
spark-worker-2-1     | 24/12/13 14:00:57 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
airflow-worker-1     | [2024-12-13 14:42:20,533: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensors.py
airflow-worker-1     | [2024-12-13 14:42:20,536: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_sensors>
airflow-worker-1     | [2024-12-13 14:42:20,536: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
airflow-worker-1     | [2024-12-13 14:42:20,538: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: latest_only_with_trigger>
airflow-worker-1     | [2024-12-13 14:42:20,538: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:42:20,540: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_local_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:42:20,540: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_nested_branch_dag.py
airflow-worker-1     | [2024-12-13 14:42:20,540: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,541: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,541: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,542: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,542: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,542: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,543: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_nested_branch_dag>
airflow-worker-1     | [2024-12-13 14:42:20,543: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:54:40.45 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:54:40.45 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:54:40.45 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:54:40.46 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:54:40.50 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:54:40.53 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:54:40.53 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:54:40.54 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | 
spark-worker-1-1     | 24/12/13 13:57:32 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 13:57:32 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 13:57:32 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:54:40.55 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
spark-worker-1-1     | 24/12/13 13:57:32 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:57:32 INFO SecurityManager: Changing view acls groups to: 
airflow-worker-1     | [2024-12-13 14:42:20,544: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_dynamic_task_mapping_with_no_taskflow_operators>
airflow-worker-1     | [2024-12-13 14:42:20,544: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_operator.py
spark-worker-2-1     | 24/12/13 14:00:57 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213140053-0000, execId=2)
spark-worker-2-1     | 24/12/13 14:00:57 INFO ExternalShuffleBlockResolver: Application app-20241213140053-0000 removed, cleanupLocalDirs = true
spark-worker-2-1     | 24/12/13 14:00:57 INFO Worker: Cleaning up local directories for application app-20241213140053-0000
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:02:40.20 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:02:40.20 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:02:40.22 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 13:54:42 INFO Worker: Started daemon with process name: 33@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 13:54:42 INFO SignalUtils: Registering signal handler for TERM
spark-worker-3-1     | 24/12/13 13:54:42 INFO SignalUtils: Registering signal handler for HUP
airflow-worker-1     | [2024-12-13 14:42:20,544: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,544: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,545: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:02:40.22 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:02:40.22 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:02:40.23 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:02:40.26 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:02:40.26 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:02:40.26 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:02:40.27 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 14:02:42 INFO Worker: Started daemon with process name: 34@e92f47d0559d
spark-worker-2-1     | 24/12/13 14:02:42 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 14:02:42 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 13:54:42 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 13:54:43 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:54:43 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 13:54:43 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:54:43 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:54:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 13:54:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 13:54:43 INFO Utils: Successfully started service 'sparkWorker' on port 33809.
spark-worker-1-1     | 24/12/13 13:57:32 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 13:57:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:57:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 13:57:32 INFO Utils: Successfully started service 'sparkWorker' on port 44617.
spark-worker-1-1     | 24/12/13 13:57:32 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 13:57:33 INFO Worker: Starting Spark worker 172.18.0.8:44617 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 13:57:33 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 13:57:33 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 13:57:33 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:57:33 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 14:02:42 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 14:02:42 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 14:02:42 INFO SecurityManager: Changing modify acls to: spark
airflow-worker-1     | [2024-12-13 14:42:20,546: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:42:20,546: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:42:20,546: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator.py
airflow-worker-1     | [2024-12-13 14:42:20,547: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,547: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
spark-master-1       | 24/12/13 14:02:42 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 14:02:42 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 14:02:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 14:02:42 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 14:02:42 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 14:02:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 14:02:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 14:02:43 INFO Utils: Successfully started service 'sparkWorker' on port 36365.
spark-worker-2-1     | 24/12/13 14:02:43 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 14:02:43 INFO Worker: Starting Spark worker 172.18.0.7:36365 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 14:02:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 14:02:43 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 14:02:43 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 14:02:43 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 14:02:43 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 14:02:43 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 14:02:43 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-worker-2-1     | 24/12/13 14:02:43 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 14:02:43 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 14:02:43 INFO ResourceUtils: ==============================================================
spark-master-1       | 24/12/13 14:02:43 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 14:02:44 INFO Master: Registering worker 172.18.0.9:37199 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 14:02:44 INFO Master: Registering worker 172.18.0.7:36365 with 10 cores, 2.0 GiB RAM
airflow-worker-1     | [2024-12-13 14:42:20,547: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,550: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,550: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,550: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,551: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_operator>
airflow-worker-1     | [2024-12-13 14:42:20,551: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_passing_params_via_test_command.py
spark-master-1       | 24/12/13 14:02:44 INFO Master: Registering worker 172.18.0.8:32919 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 14:04:27 INFO Master: 172.18.0.8:36820 got disassociated, removing it.
airflow-worker-1     | [2024-12-13 14:42:20,551: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,551: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,551: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,552: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_passing_params_via_test_command>
spark-worker-2-1     | 24/12/13 14:02:43 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 13:54:43 INFO Worker: Worker decommissioning not enabled.
airflow-worker-1     | [2024-12-13 14:42:20,553: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_objectstorage.py
airflow-worker-1     | [2024-12-13 14:42:20,554: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: tutorial_objectstorage>
airflow-worker-1     | [2024-12-13 14:42:20,554: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_decorator.py
airflow-worker-1     | [2024-12-13 14:42:20,557: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_python_decorator>
airflow-worker-1     | [2024-12-13 14:42:20,557: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_trigger_ui.py
spark-worker-3-1     | 24/12/13 13:54:44 INFO Worker: Starting Spark worker 172.18.0.7:33809 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 13:54:44 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 13:54:44 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 13:54:44 INFO ResourceUtils: ==============================================================
airflow-worker-1     | [2024-12-13 14:42:20,559: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_params_trigger_ui>
airflow-worker-1     | [2024-12-13 14:42:20,559: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_ui_tutorial.py
airflow-worker-1     | [2024-12-13 14:42:20,561: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_params_ui_tutorial>
airflow-worker-1     | [2024-12-13 14:42:20,561: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown.py
airflow-worker-1     | [2024-12-13 14:42:20,563: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_setup_teardown>
airflow-worker-1     | [2024-12-13 14:42:20,563: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sla_dag.py
airflow-worker-1     | [2024-12-13 14:42:20,563: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,563: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
spark-worker-1-1     | 24/12/13 13:57:33 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 13:57:33 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 13:57:33 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 13:54:44 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 13:54:44 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 13:54:44 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 13:54:44 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 13:54:44 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-3-1     | 24/12/13 13:54:44 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 13:54:44 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 58 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 13:54:44 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 13:55:33 INFO Worker: Asked to launch executor app-20241213135533-0000/2 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 13:55:33 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:55:33 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 13:55:33 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:55:33 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:55:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 13:55:33 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=46315" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:46315" "--executor-id" "2" "--hostname" "172.18.0.7" "--cores" "10" "--app-id" "app-20241213135533-0000" "--worker-url" "spark://Worker@172.18.0.7:33809" "--resourceProfileId" "0"
spark-master-1       | 24/12/13 14:04:27 INFO Master: 172.18.0.9:50150 got disassociated, removing it.
spark-master-1       | 24/12/13 14:04:27 INFO Master: 172.18.0.8:32919 got disassociated, removing it.
spark-master-1       | 24/12/13 14:04:27 INFO Master: Removing worker worker-20241213140243-172.18.0.8-32919 on 172.18.0.8:32919
spark-master-1       | 24/12/13 14:04:27 INFO Master: Telling app of lost worker: worker-20241213140243-172.18.0.8-32919
airflow-worker-1     | [2024-12-13 14:42:20,563: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,564: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_sla_dag>
spark-worker-2-1     | 24/12/13 14:02:43 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 14:02:43 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 14:02:43 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 14:02:43 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 14:02:43 INFO Worker: Connecting to master spark-master:7077...
airflow-worker-1     | [2024-12-13 14:42:20,564: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator_decorator.py
airflow-worker-1     | [2024-12-13 14:42:20,565: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,565: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,565: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-master-1       | 24/12/13 14:04:27 INFO Master: 172.18.0.9:37199 got disassociated, removing it.
spark-master-1       | 24/12/13 14:04:27 INFO Master: Removing worker worker-20241213140243-172.18.0.9-37199 on 172.18.0.9:37199
spark-master-1       | 24/12/13 14:04:27 INFO Master: Telling app of lost worker: worker-20241213140243-172.18.0.9-37199
spark-master-1       | 24/12/13 14:04:27 INFO Master: 172.18.0.7:33490 got disassociated, removing it.
spark-master-1       | 24/12/13 14:04:27 INFO Master: 172.18.0.7:36365 got disassociated, removing it.
spark-master-1       | 24/12/13 14:04:27 INFO Master: Removing worker worker-20241213140243-172.18.0.7-36365 on 172.18.0.7:36365
spark-worker-1-1     | 24/12/13 13:57:33 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 13:57:33 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 13:57:33 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 48 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 13:55:36 INFO Worker: Asked to kill executor app-20241213135533-0000/2
spark-worker-3-1     | 24/12/13 13:55:36 INFO ExecutorRunner: Runner thread for executor app-20241213135533-0000/2 interrupted
spark-worker-3-1     | 24/12/13 13:55:36 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 13:55:36 INFO Worker: Executor app-20241213135533-0000/2 finished with state KILLED exitStatus 143
spark-worker-3-1     | 24/12/13 13:55:36 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
spark-worker-3-1     | 24/12/13 13:55:36 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213135533-0000, execId=2)
spark-master-1       | 24/12/13 14:04:27 INFO Master: Telling app of lost worker: worker-20241213140243-172.18.0.7-36365
spark-master-1       | [38;5;6mspark [38;5;5m14:05:10.11 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m14:05:10.11 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:05:10.11 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | 24/12/13 14:02:43 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 63 ms (0 ms spent in bootstraps)
spark-worker-2-1     | 24/12/13 14:02:44 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 13:57:33 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 13:58:21 INFO Worker: Asked to launch executor app-20241213135820-0000/2 for MyAwesomeSpark
spark-master-1       | [38;5;6mspark [38;5;5m14:05:10.11 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:05:10.11 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:05:10.48 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:05:10.48 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:05:10.49 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:05:10.49 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:05:10.12 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m14:05:10.16 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m14:05:10.16 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m14:05:10.17 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | [38;5;6mspark [38;5;5m14:05:10.18 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
airflow-worker-1     | [2024-12-13 14:42:20,569: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,569: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,569: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,570: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_python_operator_decorator>
spark-worker-3-1     | 24/12/13 13:55:36 INFO ExternalShuffleBlockResolver: Application app-20241213135533-0000 removed, cleanupLocalDirs = true
spark-worker-3-1     | 24/12/13 13:55:36 INFO Worker: Cleaning up local directories for application app-20241213135533-0000
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:57:30.20 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:57:30.20 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:57:30.20 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:57:30.21 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
airflow-worker-1     | [2024-12-13 14:42:20,570: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_datasets.py
airflow-worker-1     | [2024-12-13 14:42:20,571: DEBUG/ForkPoolWorker-16] Initializing Providers Manager[dataset_uris]
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:57:30.21 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:57:30.22 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:57:30.25 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
airflow-worker-1     | [2024-12-13 14:42:20,771: DEBUG/ForkPoolWorker-16] Initialization of Providers Manager[dataset_uris] took 0.20 seconds
airflow-worker-1     | [2024-12-13 14:42:20,772: DEBUG/ForkPoolWorker-16] Failed to find locale C
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:57:30.26 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:57:30.26 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m13:57:30.27 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:05:10.49 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:05:10.50 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:05:10.55 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:05:10.55 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:05:10.56 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:05:10.57 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | 24/12/13 13:58:21 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 13:58:21 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 13:58:21 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 13:58:21 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 14:05:12 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-master-1       | 24/12/13 14:05:12 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 14:05:12 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 13:58:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 13:58:21 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45293" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:45293" "--executor-id" "2" "--hostname" "172.18.0.8" "--cores" "10" "--app-id" "app-20241213135820-0000" "--worker-url" "spark://Worker@172.18.0.8:44617" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 13:58:35 INFO Worker: Asked to kill executor app-20241213135820-0000/2
spark-worker-1-1     | 24/12/13 13:58:35 INFO ExecutorRunner: Runner thread for executor app-20241213135820-0000/2 interrupted
spark-worker-1-1     | 24/12/13 13:58:35 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 13:58:35 INFO Worker: Executor app-20241213135820-0000/2 finished with state KILLED exitStatus 143
spark-worker-1-1     | 24/12/13 13:58:35 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
spark-worker-1-1     | 24/12/13 13:58:35 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213135820-0000, execId=2)
spark-worker-1-1     | 24/12/13 13:58:35 INFO Worker: Cleaning up local directories for application app-20241213135820-0000
airflow-worker-1     | [2024-12-13 14:42:20,772: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,772: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,774: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,774: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,774: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-worker-3-1     | ========================================
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 13:57:32 INFO Worker: Started daemon with process name: 33@ed56f9ca93ac
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 14:05:12 INFO Worker: Started daemon with process name: 34@e92f47d0559d
spark-worker-2-1     | 24/12/13 14:05:12 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 14:05:12 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 14:05:12 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 14:05:12 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | 24/12/13 14:05:12 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 14:05:12 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 14:05:12 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 14:05:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 14:05:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 14:05:12 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 14:05:12 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 14:05:12 INFO Master: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 13:58:35 INFO ExternalShuffleBlockResolver: Application app-20241213135820-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:00:10.03 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | 24/12/13 13:57:32 INFO SignalUtils: Registering signal handler for TERM
spark-worker-3-1     | 24/12/13 13:57:32 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 13:57:32 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 13:57:32 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:57:32 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 13:57:32 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:57:32 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:57:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 13:57:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 14:05:13 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 14:05:13 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 14:05:13 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 14:05:13 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 14:05:13 INFO Master: Registering worker 172.18.0.8:42747 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 14:05:13 INFO Master: Registering worker 172.18.0.9:44077 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 14:05:13 INFO Master: Registering worker 172.18.0.6:35791 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:00:10.03 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:00:10.03 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | 24/12/13 14:05:12 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 14:05:12 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 14:05:12 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 14:05:12 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 14:05:12 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 14:05:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 14:05:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 14:05:13 INFO Utils: Successfully started service 'sparkWorker' on port 44077.
spark-worker-2-1     | 24/12/13 14:05:13 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 14:05:13 INFO Worker: Starting Spark worker 172.18.0.9:44077 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 14:05:13 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 14:05:13 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:00:10.03 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:00:10.03 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | 24/12/13 13:57:32 INFO Utils: Successfully started service 'sparkWorker' on port 36503.
spark-worker-3-1     | 24/12/13 13:57:32 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 13:57:33 INFO Worker: Starting Spark worker 172.18.0.6:36503 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 13:57:33 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 13:57:33 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 13:57:33 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 13:57:33 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 13:57:33 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 13:57:33 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 13:57:33 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 13:57:33 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-3-1     | 24/12/13 13:57:33 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 13:57:33 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 42 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 13:57:33 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
airflow-worker-1     | [2024-12-13 14:42:20,774: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_consumes_1>
airflow-worker-1     | [2024-12-13 14:42:20,775: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_produces_2>
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:00:10.05 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:00:10.06 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:00:10.07 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:00:10.07 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:00:10.08 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | 24/12/13 14:05:13 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 14:05:13 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 14:05:13 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 13:58:21 INFO Worker: Asked to launch executor app-20241213135820-0000/1 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 13:58:21 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 13:58:21 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 14:05:59 INFO Master: Registering app MyAwesomeSpark
spark-master-1       | 24/12/13 14:05:59 INFO Master: Registered app MyAwesomeSpark with ID app-20241213140559-0000
spark-master-1       | 24/12/13 14:05:59 INFO Master: Start scheduling for app app-20241213140559-0000 with rpId: 0
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 14:00:12 INFO Worker: Started daemon with process name: 34@f484bf7bba3e
spark-worker-1-1     | 24/12/13 14:00:12 INFO SignalUtils: Registering signal handler for TERM
airflow-worker-1     | [2024-12-13 14:42:20,775: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_consumes_unknown_never_scheduled>
airflow-worker-1     | [2024-12-13 14:42:20,775: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: consume_1_or_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:42:20,775: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_consumes_1_never_scheduled>
airflow-worker-1     | [2024-12-13 14:42:20,775: DEBUG/ForkPoolWorker-16] Failed to find locale C
spark-master-1       | 24/12/13 14:05:59 INFO Master: Launching executor app-20241213140559-0000/0 on worker worker-20241213140513-172.18.0.9-44077
spark-master-1       | 24/12/13 14:05:59 INFO Master: Launching executor app-20241213140559-0000/1 on worker worker-20241213140513-172.18.0.6-35791
spark-worker-2-1     | 24/12/13 14:05:13 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 14:05:13 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 14:05:13 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 14:05:13 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 14:05:13 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 24 ms (0 ms spent in bootstraps)
spark-master-1       | 24/12/13 14:05:59 INFO Master: Launching executor app-20241213140559-0000/2 on worker worker-20241213140513-172.18.0.8-42747
spark-master-1       | 24/12/13 14:05:59 INFO Master: Start scheduling for app app-20241213140559-0000 with rpId: 0
spark-master-1       | 24/12/13 14:05:59 INFO Master: Start scheduling for app app-20241213140559-0000 with rpId: 0
spark-master-1       | 24/12/13 14:05:59 INFO Master: Start scheduling for app app-20241213140559-0000 with rpId: 0
spark-master-1       | 24/12/13 14:06:02 INFO Master: Received unregister request from application app-20241213140559-0000
spark-master-1       | 24/12/13 14:06:02 INFO Master: Removing app app-20241213140559-0000
spark-worker-1-1     | 24/12/13 14:00:12 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 14:00:12 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 14:00:12 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 14:00:12 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 14:00:12 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 14:00:12 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 14:00:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 14:00:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 14:00:13 INFO Utils: Successfully started service 'sparkWorker' on port 35139.
spark-worker-1-1     | 24/12/13 14:00:13 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 14:00:13 INFO Worker: Starting Spark worker 172.18.0.6:35139 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 14:00:13 INFO Worker: Running Spark version 3.5.0
spark-master-1       | 24/12/13 14:06:03 WARN Master: Got status update for unknown executor app-20241213140559-0000/1
spark-master-1       | 24/12/13 14:06:03 WARN Master: Got status update for unknown executor app-20241213140559-0000/2
spark-master-1       | 24/12/13 14:06:03 WARN Master: Got status update for unknown executor app-20241213140559-0000/0
airflow-worker-1     | [2024-12-13 14:42:20,775: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,775: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,775: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_produces_1>
airflow-worker-1     | [2024-12-13 14:42:20,775: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: consume_1_and_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:42:20,775: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: consume_1_or_both_2_and_3_with_dataset_expressions>
spark-worker-1-1     | 24/12/13 14:00:13 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 14:00:13 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 14:00:13 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 14:05:13 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 14:05:59 INFO Worker: Asked to launch executor app-20241213140559-0000/0 for MyAwesomeSpark
spark-worker-2-1     | 24/12/13 14:05:59 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 14:05:59 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 14:05:59 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 14:05:59 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 14:05:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 14:05:59 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=39289" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:39289" "--executor-id" "0" "--hostname" "172.18.0.9" "--cores" "10" "--app-id" "app-20241213140559-0000" "--worker-url" "spark://Worker@172.18.0.9:44077" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 14:06:02 INFO Worker: Asked to kill executor app-20241213140559-0000/0
spark-worker-2-1     | 24/12/13 14:06:02 INFO ExecutorRunner: Runner thread for executor app-20241213140559-0000/0 interrupted
spark-worker-2-1     | 24/12/13 14:06:02 INFO ExecutorRunner: Killing process!
spark-worker-2-1     | 24/12/13 14:06:03 INFO Worker: Executor app-20241213140559-0000/0 finished with state KILLED exitStatus 143
spark-worker-2-1     | 24/12/13 14:06:03 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-2-1     | 24/12/13 14:06:03 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213140559-0000, execId=0)
spark-worker-2-1     | 24/12/13 14:06:03 INFO ExternalShuffleBlockResolver: Application app-20241213140559-0000 removed, cleanupLocalDirs = true
spark-master-1       | 24/12/13 14:06:03 INFO Master: 172.18.0.12:39432 got disassociated, removing it.
spark-master-1       | 24/12/13 14:06:03 INFO Master: 22550a098c1e:39289 got disassociated, removing it.
spark-master-1       | 24/12/13 14:10:53 INFO Master: 172.18.0.6:40558 got disassociated, removing it.
spark-master-1       | 24/12/13 14:10:53 INFO Master: 172.18.0.6:35791 got disassociated, removing it.
spark-master-1       | 24/12/13 14:10:53 INFO Master: Removing worker worker-20241213140513-172.18.0.6-35791 on 172.18.0.6:35791
spark-master-1       | 24/12/13 14:10:53 INFO Master: Telling app of lost worker: worker-20241213140513-172.18.0.6-35791
spark-master-1       | 24/12/13 14:10:53 INFO Master: 172.18.0.8:58054 got disassociated, removing it.
spark-master-1       | 24/12/13 14:10:53 INFO Master: 172.18.0.8:42747 got disassociated, removing it.
spark-master-1       | 24/12/13 14:10:53 INFO Master: Removing worker worker-20241213140513-172.18.0.8-42747 on 172.18.0.8:42747
spark-master-1       | 24/12/13 14:10:53 INFO Master: Telling app of lost worker: worker-20241213140513-172.18.0.8-42747
spark-master-1       | 24/12/13 14:10:53 INFO Master: 172.18.0.9:48370 got disassociated, removing it.
spark-master-1       | 24/12/13 14:10:53 INFO Master: 172.18.0.9:44077 got disassociated, removing it.
spark-worker-3-1     | 24/12/13 13:58:21 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 13:58:21 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 13:58:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 13:58:21 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45293" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:45293" "--executor-id" "1" "--hostname" "172.18.0.6" "--cores" "10" "--app-id" "app-20241213135820-0000" "--worker-url" "spark://Worker@172.18.0.6:36503" "--resourceProfileId" "0"
spark-worker-3-1     | 24/12/13 13:58:35 INFO Worker: Asked to kill executor app-20241213135820-0000/1
spark-worker-3-1     | 24/12/13 13:58:35 INFO ExecutorRunner: Runner thread for executor app-20241213135820-0000/1 interrupted
spark-worker-3-1     | 24/12/13 13:58:35 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 13:58:35 INFO Worker: Executor app-20241213135820-0000/1 finished with state KILLED exitStatus 143
spark-worker-3-1     | 24/12/13 13:58:35 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-worker-3-1     | 24/12/13 13:58:35 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213135820-0000, execId=1)
spark-worker-3-1     | 24/12/13 13:58:35 INFO ExternalShuffleBlockResolver: Application app-20241213135820-0000 removed, cleanupLocalDirs = true
spark-worker-3-1     | 24/12/13 13:58:35 INFO Worker: Cleaning up local directories for application app-20241213135820-0000
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:00:10.10 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:00:10.10 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:00:10.10 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:00:10.10 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:00:10.11 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:00:10.12 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:00:10.15 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:00:10.15 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:00:10.16 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:00:10.20 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 14:00:12 INFO Worker: Started daemon with process name: 34@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 14:00:12 INFO SignalUtils: Registering signal handler for TERM
spark-worker-3-1     | 24/12/13 14:00:12 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 14:00:12 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 14:00:12 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 14:00:12 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 14:00:12 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 14:00:12 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 14:06:03 INFO Worker: Cleaning up local directories for application app-20241213140559-0000
spark-master-1       | 24/12/13 14:10:53 INFO Master: Removing worker worker-20241213140513-172.18.0.9-44077 on 172.18.0.9:44077
spark-master-1       | 24/12/13 14:10:53 INFO Master: Telling app of lost worker: worker-20241213140513-172.18.0.9-44077
spark-master-1       | [38;5;6mspark [38;5;5m14:40:53.67 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | 24/12/13 14:00:13 INFO ResourceUtils: ==============================================================
airflow-worker-1     | [2024-12-13 14:42:20,775: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: conditional_dataset_and_time_based_timetable>
airflow-worker-1     | [2024-12-13 14:42:20,775: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_consumes_1_and_2>
airflow-worker-1     | [2024-12-13 14:42:20,776: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_outlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:42:20,777: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,777: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,777: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,778: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,778: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,778: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:40:53.92 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:40:53.92 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:40:53.93 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:40:53.93 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
airflow-worker-1     | [2024-12-13 14:42:20,778: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,778: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,778: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,779: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,779: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,779: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,779: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_with_extra_by_context>
airflow-worker-1     | [2024-12-13 14:42:20,779: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,779: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,780: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-worker-3-1     | 24/12/13 14:00:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 14:00:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 14:00:13 INFO Utils: Successfully started service 'sparkWorker' on port 36401.
spark-worker-3-1     | 24/12/13 14:00:13 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 14:00:14 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 14:00:14 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-1-1     | 24/12/13 14:00:14 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 14:00:14 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 14:00:14 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.3:7077 after 50 ms (0 ms spent in bootstraps)
spark-worker-1-1     | 24/12/13 14:00:14 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 14:00:53 INFO Worker: Asked to launch executor app-20241213140053-0000/1 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 14:00:53 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:40:53.93 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:40:53.94 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:40:53.95 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | 24/12/13 14:00:53 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 14:00:53 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 14:00:53 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | [38;5;6mspark [38;5;5m14:40:53.67 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:40:53.67 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:40:53.68 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:40:53.68 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m14:40:53.70 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-master-1       | [38;5;6mspark [38;5;5m14:40:53.72 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m14:40:53.72 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | [38;5;6mspark [38;5;5m14:40:53.73 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m14:40:53.75 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 14:00:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 14:00:53 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45213" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:45213" "--executor-id" "1" "--hostname" "172.18.0.6" "--cores" "10" "--app-id" "app-20241213140053-0000" "--worker-url" "spark://Worker@172.18.0.6:35139" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 14:00:57 INFO Worker: Asked to kill executor app-20241213140053-0000/1
spark-worker-1-1     | 24/12/13 14:00:57 INFO ExecutorRunner: Runner thread for executor app-20241213140053-0000/1 interrupted
spark-master-1       | 24/12/13 14:40:56 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-master-1       | 24/12/13 14:40:56 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 14:40:56 INFO SignalUtils: Registering signal handler for HUP
spark-master-1       | 24/12/13 14:40:56 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 14:40:56 INFO SecurityManager: Changing view acls to: spark
airflow-worker-1     | [2024-12-13 14:42:20,780: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_with_extra_by_yield>
airflow-worker-1     | [2024-12-13 14:42:20,780: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,780: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,780: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-master-1       | 24/12/13 14:40:56 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 14:40:56 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 14:40:56 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 14:40:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 14:40:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 14:00:13 INFO Worker: Starting Spark worker 172.18.0.8:36401 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 14:00:13 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 14:00:13 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 14:00:13 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 14:00:57 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 14:00:57 INFO Worker: Executor app-20241213140053-0000/1 finished with state KILLED exitStatus 143
spark-worker-1-1     | 24/12/13 14:00:57 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-worker-1-1     | 24/12/13 14:00:57 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213140053-0000, execId=1)
spark-master-1       | 24/12/13 14:40:56 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 14:40:56 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 14:40:56 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 14:40:56 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:40:53.96 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | 24/12/13 14:00:13 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 14:00:13 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 14:00:13 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 14:00:14 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 14:00:14 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:40:53.97 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:40:53.98 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
airflow-worker-1     | [2024-12-13 14:42:20,780: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_with_extra_from_classic_operator>
airflow-worker-1     | [2024-12-13 14:42:20,780: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_decorator.py
airflow-worker-1     | [2024-12-13 14:42:20,782: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_bash_decorator>
airflow-worker-1     | [2024-12-13 14:42:20,783: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_decorator.py
airflow-worker-1     | [2024-12-13 14:42:20,784: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_short_circuit_decorator>
spark-worker-1-1     | 24/12/13 14:00:57 INFO Worker: Cleaning up local directories for application app-20241213140053-0000
spark-worker-1-1     | 24/12/13 14:00:57 INFO ExternalShuffleBlockResolver: Application app-20241213140053-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:02:40.29 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:02:40.29 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:02:40.30 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:02:40.30 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 14:40:56 INFO Worker: Started daemon with process name: 34@e92f47d0559d
spark-worker-2-1     | 24/12/13 14:40:56 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 14:40:56 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 14:00:14 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 14:00:14 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.3:7077 after 45 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 14:00:14 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 14:00:53 INFO Worker: Asked to launch executor app-20241213140053-0000/0 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 14:00:53 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 14:40:56 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 14:40:56 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:02:40.30 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:02:40.31 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:02:40.33 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:02:40.35 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:02:40.35 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:02:40.36 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 14:02:42 INFO Worker: Started daemon with process name: 34@f484bf7bba3e
spark-worker-1-1     | 24/12/13 14:02:42 INFO SignalUtils: Registering signal handler for TERM
spark-worker-1-1     | 24/12/13 14:02:42 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 14:02:42 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 14:02:42 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 14:40:56 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 14:40:56 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 14:40:56 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 14:40:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 14:40:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 14:40:56 INFO Utils: Successfully started service 'sparkWorker' on port 34275.
spark-worker-2-1     | 24/12/13 14:40:56 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 14:40:56 INFO Worker: Starting Spark worker 172.18.0.8:34275 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 14:40:56 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 14:40:56 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 14:40:57 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 14:40:57 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 14:40:57 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 14:40:57 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 14:02:42 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 14:02:42 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 14:02:42 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 14:02:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 14:02:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 14:02:43 INFO Utils: Successfully started service 'sparkWorker' on port 37199.
spark-worker-1-1     | 24/12/13 14:02:43 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 14:02:43 INFO Worker: Starting Spark worker 172.18.0.9:37199 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 14:02:43 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 14:02:43 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 14:02:43 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 14:40:57 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 14:40:57 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-1-1     | 24/12/13 14:02:43 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 14:40:57 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 14:40:57 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 72 ms (0 ms spent in bootstraps)
spark-worker-2-1     | 24/12/13 14:40:57 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 14:42:27 INFO Worker: Asked to launch executor app-20241213144226-0000/2 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 14:02:43 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 14:02:43 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 14:02:43 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-1-1     | 24/12/13 14:02:43 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 14:02:43 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 14:02:43 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 157 ms (0 ms spent in bootstraps)
spark-worker-1-1     | 24/12/13 14:02:44 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 14:00:53 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 14:00:53 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 14:00:53 INFO SecurityManager: Changing modify acls groups to: 
spark-master-1       | 24/12/13 14:40:57 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 14:40:57 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 14:40:57 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 14:40:57 INFO Master: Registering worker 172.18.0.7:32921 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 14:40:57 INFO Master: Registering worker 172.18.0.6:44861 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 14:40:57 INFO Master: Registering worker 172.18.0.8:34275 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 14:42:26 INFO Master: Registering app MyAwesomeSpark
spark-master-1       | 24/12/13 14:42:26 INFO Master: Registered app MyAwesomeSpark with ID app-20241213144226-0000
spark-master-1       | 24/12/13 14:42:26 INFO Master: Start scheduling for app app-20241213144226-0000 with rpId: 0
spark-master-1       | 24/12/13 14:42:26 INFO Master: Launching executor app-20241213144226-0000/0 on worker worker-20241213144056-172.18.0.7-32921
spark-master-1       | 24/12/13 14:42:26 INFO Master: Launching executor app-20241213144226-0000/1 on worker worker-20241213144056-172.18.0.6-44861
spark-master-1       | 24/12/13 14:42:26 INFO Master: Launching executor app-20241213144226-0000/2 on worker worker-20241213144056-172.18.0.8-34275
spark-master-1       | 24/12/13 14:42:27 INFO Master: Start scheduling for app app-20241213144226-0000 with rpId: 0
spark-master-1       | 24/12/13 14:42:27 INFO Master: Start scheduling for app app-20241213144226-0000 with rpId: 0
spark-master-1       | 24/12/13 14:42:27 INFO Master: Start scheduling for app app-20241213144226-0000 with rpId: 0
spark-master-1       | 24/12/13 14:42:30 INFO Master: Received unregister request from application app-20241213144226-0000
spark-master-1       | 24/12/13 14:42:30 INFO Master: Removing app app-20241213144226-0000
spark-master-1       | 24/12/13 14:42:30 WARN Master: Got status update for unknown executor app-20241213144226-0000/0
spark-worker-3-1     | 24/12/13 14:00:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:05:10.49 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:05:10.49 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:05:10.49 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
airflow-worker-1     | [2024-12-13 14:42:20,784: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_controller_dag.py
airflow-worker-1     | [2024-12-13 14:42:20,786: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_trigger_controller_dag>
airflow-worker-1     | [2024-12-13 14:42:20,786: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only.py
airflow-worker-1     | [2024-12-13 14:42:20,787: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: latest_only>
airflow-worker-1     | [2024-12-13 14:42:20,787: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_time_delta_sensor_async.py
airflow-worker-1     | [2024-12-13 14:42:20,788: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_time_delta_sensor_async>
airflow-worker-1     | [2024-12-13 14:42:20,788: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_templates.py
airflow-worker-1     | [2024-12-13 14:42:20,788: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,788: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
spark-worker-3-1     | 24/12/13 14:00:53 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45213" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:45213" "--executor-id" "0" "--hostname" "172.18.0.8" "--cores" "10" "--app-id" "app-20241213140053-0000" "--worker-url" "spark://Worker@172.18.0.8:36401" "--resourceProfileId" "0"
spark-worker-3-1     | 24/12/13 14:00:57 INFO Worker: Asked to kill executor app-20241213140053-0000/0
spark-worker-3-1     | 24/12/13 14:00:57 INFO ExecutorRunner: Runner thread for executor app-20241213140053-0000/0 interrupted
spark-worker-3-1     | 24/12/13 14:00:57 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 14:00:57 INFO Worker: Executor app-20241213140053-0000/0 finished with state KILLED exitStatus 143
spark-master-1       | 24/12/13 14:42:30 WARN Master: Got status update for unknown executor app-20241213144226-0000/1
spark-master-1       | 24/12/13 14:42:30 WARN Master: Got status update for unknown executor app-20241213144226-0000/2
spark-master-1       | 24/12/13 14:42:30 INFO Master: 172.18.0.11:48120 got disassociated, removing it.
spark-master-1       | 24/12/13 14:42:30 INFO Master: 65b6e979e5f1:42543 got disassociated, removing it.
spark-worker-3-1     | 24/12/13 14:00:57 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:05:10.50 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:05:10.50 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:05:10.51 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
airflow-worker-1     | [2024-12-13 14:42:20,788: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,789: DEBUG/ForkPoolWorker-16] Failed to find locale C
spark-master-1       | 24/12/13 14:47:08 INFO Master: 172.18.0.6:52916 got disassociated, removing it.
spark-master-1       | 24/12/13 14:47:08 INFO Master: 172.18.0.6:44861 got disassociated, removing it.
spark-master-1       | 24/12/13 14:47:08 INFO Master: Removing worker worker-20241213144056-172.18.0.6-44861 on 172.18.0.6:44861
spark-master-1       | 24/12/13 14:47:08 INFO Master: Telling app of lost worker: worker-20241213144056-172.18.0.6-44861
spark-worker-2-1     | 24/12/13 14:42:27 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 14:42:27 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 14:42:27 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 14:42:27 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 14:42:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 14:42:27 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=42543" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@65b6e979e5f1:42543" "--executor-id" "2" "--hostname" "172.18.0.8" "--cores" "10" "--app-id" "app-20241213144226-0000" "--worker-url" "spark://Worker@172.18.0.8:34275" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 14:42:30 INFO Worker: Asked to kill executor app-20241213144226-0000/2
spark-worker-2-1     | 24/12/13 14:42:30 INFO ExecutorRunner: Runner thread for executor app-20241213144226-0000/2 interrupted
spark-worker-2-1     | 24/12/13 14:42:30 INFO ExecutorRunner: Killing process!
spark-worker-2-1     | 24/12/13 14:42:30 INFO Worker: Executor app-20241213144226-0000/2 finished with state KILLED exitStatus 143
spark-worker-2-1     | 24/12/13 14:42:30 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
spark-worker-2-1     | 24/12/13 14:42:30 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213144226-0000, execId=2)
spark-worker-2-1     | 24/12/13 14:42:30 INFO ExternalShuffleBlockResolver: Application app-20241213144226-0000 removed, cleanupLocalDirs = true
spark-worker-2-1     | 24/12/13 14:42:30 INFO Worker: Cleaning up local directories for application app-20241213144226-0000
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:56:59.63 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:56:59.63 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:56:59.63 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:56:59.63 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
airflow-worker-1     | [2024-12-13 14:42:20,789: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,790: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,790: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: tutorial_taskflow_templates>
airflow-worker-1     | [2024-12-13 14:42:20,790: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api_virtualenv.py
airflow-worker-1     | [2024-12-13 14:42:20,792: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: tutorial_taskflow_api_virtualenv>
airflow-worker-1     | [2024-12-13 14:42:20,792: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensor_decorator.py
airflow-worker-1     | [2024-12-13 14:42:20,793: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_sensor_decorator>
airflow-worker-1     | [2024-12-13 14:42:20,793: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_operator.py
airflow-worker-1     | [2024-12-13 14:42:20,794: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_short_circuit_operator>
spark-worker-3-1     | 24/12/13 14:00:57 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213140053-0000, execId=0)
spark-worker-3-1     | 24/12/13 14:00:57 INFO Worker: Cleaning up local directories for application app-20241213140053-0000
spark-worker-3-1     | 24/12/13 14:00:57 INFO ExternalShuffleBlockResolver: Application app-20241213140053-0000 removed, cleanupLocalDirs = true
airflow-worker-1     | [2024-12-13 14:42:20,794: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcom.py
airflow-worker-1     | [2024-12-13 14:42:20,795: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_xcom>
airflow-worker-1     | [2024-12-13 14:42:20,795: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias.py
airflow-worker-1     | [2024-12-13 14:42:20,939: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_alias_example_alias_producer>
airflow-worker-1     | [2024-12-13 14:42:20,939: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_s3_bucket_producer>
airflow-worker-1     | [2024-12-13 14:42:20,939: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_s3_bucket_consumer>
airflow-worker-1     | [2024-12-13 14:42:20,939: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_alias_example_alias_consumer>
airflow-worker-1     | [2024-12-13 14:42:20,939: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown_taskflow.py
airflow-worker-1     | [2024-12-13 14:42:20,941: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_setup_teardown_taskflow>
airflow-worker-1     | [2024-12-13 14:42:20,941: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api.py
airflow-worker-1     | [2024-12-13 14:42:20,942: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: tutorial_taskflow_api>
airflow-worker-1     | [2024-12-13 14:42:20,942: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_labels.py
airflow-worker-1     | [2024-12-13 14:42:20,942: DEBUG/ForkPoolWorker-16] Failed to find locale C
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:56:59.63 [0m[38;5;2mINFO [0m ==> 
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:56:59.64 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:56:59.66 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-2-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:56:59.66 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-2-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:02:40.26 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:02:40.26 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:02:40.26 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:02:40.26 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:02:40.27 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:02:40.27 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:02:40.29 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:02:40.29 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:02:40.29 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:02:40.31 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 14:02:42 INFO Worker: Started daemon with process name: 33@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 14:02:42 INFO SignalUtils: Registering signal handler for TERM
spark-worker-3-1     | 24/12/13 14:02:42 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 14:02:42 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 14:02:42 INFO SecurityManager: Changing view acls to: spark
spark-master-1       | 24/12/13 14:47:08 INFO Master: 172.18.0.7:34444 got disassociated, removing it.
spark-master-1       | 24/12/13 14:47:08 INFO Master: 172.18.0.7:32921 got disassociated, removing it.
spark-master-1       | 24/12/13 14:47:08 INFO Master: Removing worker worker-20241213144056-172.18.0.7-32921 on 172.18.0.7:32921
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:05:10.57 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:05:10.57 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:05:10.57 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:05:10.59 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:56:59.66 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-2-1     | [38;5;6mspark [38;5;5m14:56:59.67 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-2-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-e92f47d0559d.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 14:05:12 INFO Worker: Started daemon with process name: 34@f484bf7bba3e
spark-worker-2-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-2-1     | ========================================
spark-worker-2-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-2-1     | 24/12/13 14:57:01 INFO Worker: Started daemon with process name: 33@e92f47d0559d
spark-worker-2-1     | 24/12/13 14:57:01 INFO SignalUtils: Registering signal handler for TERM
spark-worker-2-1     | 24/12/13 14:57:01 INFO SignalUtils: Registering signal handler for HUP
spark-worker-2-1     | 24/12/13 14:57:01 INFO SignalUtils: Registering signal handler for INT
spark-worker-2-1     | 24/12/13 14:57:01 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 14:57:01 INFO SecurityManager: Changing modify acls to: spark
spark-worker-2-1     | 24/12/13 14:57:01 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 14:57:01 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 14:57:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-2-1     | 24/12/13 14:57:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-2-1     | 24/12/13 14:57:02 INFO Utils: Successfully started service 'sparkWorker' on port 45523.
spark-worker-2-1     | 24/12/13 14:57:02 INFO Worker: Worker decommissioning not enabled.
spark-worker-2-1     | 24/12/13 14:57:02 INFO Worker: Starting Spark worker 172.18.0.7:45523 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 14:57:02 INFO Worker: Running Spark version 3.5.0
spark-worker-2-1     | 24/12/13 14:57:02 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 14:57:02 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 14:57:02 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-2-1     | 24/12/13 14:57:02 INFO ResourceUtils: ==============================================================
spark-worker-2-1     | 24/12/13 14:57:02 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-2-1     | 24/12/13 14:57:02 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-2-1     | 24/12/13 14:57:02 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://e92f47d0559d:8081
spark-worker-2-1     | 24/12/13 14:57:02 INFO Worker: Connecting to master spark-master:7077...
spark-worker-2-1     | 24/12/13 14:57:03 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 76 ms (0 ms spent in bootstraps)
spark-master-1       | 24/12/13 14:47:08 INFO Master: Telling app of lost worker: worker-20241213144056-172.18.0.7-32921
spark-master-1       | 24/12/13 14:47:08 INFO Master: 172.18.0.8:48896 got disassociated, removing it.
spark-master-1       | 24/12/13 14:47:08 INFO Master: 172.18.0.8:34275 got disassociated, removing it.
spark-worker-3-1     | 24/12/13 14:02:42 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 14:02:42 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 14:05:12 INFO SignalUtils: Registering signal handler for TERM
spark-worker-1-1     | 24/12/13 14:05:12 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 14:05:12 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 14:05:12 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 14:02:42 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 14:02:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 14:02:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 14:02:43 INFO Utils: Successfully started service 'sparkWorker' on port 32919.
spark-worker-3-1     | 24/12/13 14:02:43 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 14:02:43 INFO Worker: Starting Spark worker 172.18.0.8:32919 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 14:02:43 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 14:02:43 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-2-1     | 24/12/13 14:57:03 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-2-1     | 24/12/13 14:58:31 INFO Worker: Asked to launch executor app-20241213145831-0000/2 for MyAwesomeSpark
spark-worker-2-1     | 24/12/13 14:58:32 INFO SecurityManager: Changing view acls to: spark
spark-worker-2-1     | 24/12/13 14:58:32 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 14:02:43 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 14:02:43 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 14:02:43 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 14:02:43 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 14:02:43 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 14:02:43 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-3-1     | 24/12/13 14:02:43 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 14:02:43 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 37 ms (0 ms spent in bootstraps)
airflow-worker-1     | [2024-12-13 14:42:20,942: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,943: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,943: DEBUG/ForkPoolWorker-16] Failed to find locale C
spark-worker-3-1     | 24/12/13 14:02:44 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:05:10.37 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:05:10.37 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-2-1     | 24/12/13 14:58:32 INFO SecurityManager: Changing view acls groups to: 
spark-worker-2-1     | 24/12/13 14:58:32 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-2-1     | 24/12/13 14:58:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 14:47:08 INFO Master: Removing worker worker-20241213144056-172.18.0.8-34275 on 172.18.0.8:34275
spark-master-1       | 24/12/13 14:47:08 INFO Master: Telling app of lost worker: worker-20241213144056-172.18.0.8-34275
spark-master-1       | [38;5;6mspark [38;5;5m14:56:59.37 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m14:56:59.38 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:56:59.38 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:56:59.38 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-master-1       | [38;5;6mspark [38;5;5m14:56:59.38 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m14:56:59.40 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-2-1     | 24/12/13 14:58:32 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=35597" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@65b6e979e5f1:35597" "--executor-id" "2" "--hostname" "172.18.0.7" "--cores" "10" "--app-id" "app-20241213145831-0000" "--worker-url" "spark://Worker@172.18.0.7:45523" "--resourceProfileId" "0"
spark-worker-2-1     | 24/12/13 14:58:35 INFO Worker: Asked to kill executor app-20241213145831-0000/2
spark-worker-2-1     | 24/12/13 14:58:35 INFO ExecutorRunner: Runner thread for executor app-20241213145831-0000/2 interrupted
airflow-worker-1     | [2024-12-13 14:42:20,943: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,943: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,944: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_labels>
spark-worker-1-1     | 24/12/13 14:05:12 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 14:05:12 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 14:05:12 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 14:05:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 14:05:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 14:05:13 INFO Utils: Successfully started service 'sparkWorker' on port 42747.
spark-worker-1-1     | 24/12/13 14:05:13 INFO Worker: Worker decommissioning not enabled.
airflow-worker-1     | [2024-12-13 14:42:20,944: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:42:20,947: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:42:20,947: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_display_name.py
airflow-worker-1     | [2024-12-13 14:42:20,947: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_display_name>
airflow-worker-1     | [2024-12-13 14:42:20,947: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
airflow-worker-1     | [2024-12-13 14:42:20,953: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_alias_example_alias_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:20,953: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_alias_example_alias_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:20,953: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_s3_bucket_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:20,953: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_s3_bucket_producer_with_no_taskflow>
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:05:10.38 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:05:10.38 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:05:10.38 [0m[38;5;2mINFO [0m ==> 
spark-master-1       | [38;5;6mspark [38;5;5m14:56:59.42 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-master-1       | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-master-1       | [38;5;6mspark [38;5;5m14:56:59.45 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-master-1       | [38;5;6mspark [38;5;5m14:56:59.46 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-master-1       | 
spark-master-1       | [38;5;6mspark [38;5;5m14:56:59.47 [0m[38;5;2mINFO [0m ==> ** Starting Spark in master mode **
spark-master-1       | starting org.apache.spark.deploy.master.Master, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.master.Master-1-f9573584fb53.out
spark-master-1       | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host f9573584fb53 --port 7077 --webui-port 9080
spark-master-1       | ========================================
spark-master-1       | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-master-1       | 24/12/13 14:57:01 INFO Master: Started daemon with process name: 36@f9573584fb53
spark-master-1       | 24/12/13 14:57:01 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 14:57:01 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:05:10.41 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:05:10.43 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-2-1     | 24/12/13 14:58:35 INFO ExecutorRunner: Killing process!
spark-worker-2-1     | 24/12/13 14:58:35 INFO Worker: Executor app-20241213145831-0000/2 finished with state KILLED exitStatus 0
spark-worker-2-1     | 24/12/13 14:58:35 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
spark-worker-1-1     | 24/12/13 14:05:13 INFO Worker: Starting Spark worker 172.18.0.8:42747 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 14:05:13 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 14:05:13 INFO Worker: Spark home: /opt/bitnami/spark
airflow-worker-1     | [2024-12-13 14:42:20,953: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group_decorator.py
airflow-worker-1     | [2024-12-13 14:42:20,956: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_task_group_decorator>
airflow-worker-1     | [2024-12-13 14:42:20,956: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_target_dag.py
spark-worker-1-1     | 24/12/13 14:05:13 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 14:05:13 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 14:05:13 INFO ResourceUtils: ==============================================================
airflow-worker-1     | [2024-12-13 14:42:20,957: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_trigger_target_dag>
airflow-worker-1     | [2024-12-13 14:42:20,957: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_external_task_marker_dag.py
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:05:10.44 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:05:10.45 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:05:10.47 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 14:05:12 INFO Worker: Started daemon with process name: 34@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 14:05:12 INFO SignalUtils: Registering signal handler for TERM
spark-worker-3-1     | 24/12/13 14:05:12 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 14:05:12 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 14:05:12 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 14:05:12 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 14:05:12 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 14:05:13 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 14:05:13 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-1-1     | 24/12/13 14:05:13 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 14:05:13 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 14:05:13 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 28 ms (0 ms spent in bootstraps)
airflow-worker-1     | [2024-12-13 14:42:20,959: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_external_task_marker_child>
airflow-worker-1     | [2024-12-13 14:42:20,959: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_external_task_marker_parent>
airflow-worker-1     | [2024-12-13 14:42:20,959: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_skip_dag.py
airflow-worker-1     | [2024-12-13 14:42:20,960: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_skip_dag>
airflow-worker-1     | [2024-12-13 14:42:20,960: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_workday_timetable.py
spark-master-1       | 24/12/13 14:57:01 INFO SignalUtils: Registering signal handler for INT
spark-master-1       | 24/12/13 14:57:01 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 14:05:13 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 14:05:59 INFO Worker: Asked to launch executor app-20241213140559-0000/2 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 14:05:59 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 14:05:59 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 14:05:59 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 14:05:59 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 14:05:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 14:05:59 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=39289" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:39289" "--executor-id" "2" "--hostname" "172.18.0.8" "--cores" "10" "--app-id" "app-20241213140559-0000" "--worker-url" "spark://Worker@172.18.0.8:42747" "--resourceProfileId" "0"
spark-worker-3-1     | 24/12/13 14:05:12 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 14:05:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 14:05:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 14:05:13 INFO Utils: Successfully started service 'sparkWorker' on port 35791.
spark-worker-3-1     | 24/12/13 14:05:13 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 14:05:13 INFO Worker: Starting Spark worker 172.18.0.6:35791 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 14:05:13 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 14:05:13 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 14:05:13 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 14:05:13 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 14:06:02 INFO Worker: Asked to kill executor app-20241213140559-0000/2
airflow-worker-1     | [2024-12-13 14:42:20,961: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_workday_timetable>
airflow-worker-1     | [2024-12-13 14:42:20,961: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_dag.py
airflow-worker-1     | [2024-12-13 14:42:20,962: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: tutorial_dag>
airflow-worker-1     | [2024-12-13 14:42:20,962: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_operator.py
airflow-worker-1     | [2024-12-13 14:42:20,963: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_python_operator>
airflow-worker-1     | [2024-12-13 14:42:20,963: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group.py
airflow-worker-1     | [2024-12-13 14:42:20,964: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_task_group>
airflow-worker-1     | [2024-12-13 14:42:20,964: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_datetime_operator.py
spark-master-1       | 24/12/13 14:57:01 INFO SecurityManager: Changing modify acls to: spark
spark-master-1       | 24/12/13 14:57:01 INFO SecurityManager: Changing view acls groups to: 
spark-master-1       | 24/12/13 14:57:01 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 14:05:13 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 14:05:13 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 14:06:02 INFO ExecutorRunner: Runner thread for executor app-20241213140559-0000/2 interrupted
spark-worker-1-1     | 24/12/13 14:06:02 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 14:06:03 INFO Worker: Executor app-20241213140559-0000/2 finished with state KILLED exitStatus 143
spark-worker-1-1     | 24/12/13 14:06:03 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
airflow-worker-1     | [2024-12-13 14:42:20,965: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,965: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,965: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,966: DEBUG/ForkPoolWorker-16] Failed to find locale C
spark-master-1       | 24/12/13 14:57:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-master-1       | 24/12/13 14:57:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-master-1       | 24/12/13 14:57:02 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
spark-master-1       | 24/12/13 14:57:02 INFO Master: Starting Spark master at spark://f9573584fb53:7077
spark-master-1       | 24/12/13 14:57:02 INFO Master: Running Spark version 3.5.0
spark-master-1       | 24/12/13 14:57:02 INFO JettyUtils: Start Jetty 0.0.0.0:9080 for MasterUI
spark-master-1       | 24/12/13 14:57:02 INFO Utils: Successfully started service 'MasterUI' on port 9080.
spark-master-1       | 24/12/13 14:57:02 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://f9573584fb53:9080
spark-master-1       | 24/12/13 14:57:02 INFO Master: I have been elected leader! New state: ALIVE
spark-master-1       | 24/12/13 14:57:03 INFO Master: Registering worker 172.18.0.9:46363 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 14:57:03 INFO Master: Registering worker 172.18.0.6:43171 with 10 cores, 2.0 GiB RAM
spark-master-1       | 24/12/13 14:57:03 INFO Master: Registering worker 172.18.0.7:45523 with 10 cores, 2.0 GiB RAM
spark-worker-2-1     | 24/12/13 14:58:35 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213145831-0000, execId=2)
spark-worker-2-1     | 24/12/13 14:58:35 INFO ExternalShuffleBlockResolver: Application app-20241213145831-0000 removed, cleanupLocalDirs = true
spark-worker-2-1     | 24/12/13 14:58:35 INFO Worker: Cleaning up local directories for application app-20241213145831-0000
spark-worker-3-1     | 24/12/13 14:05:13 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 14:05:13 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-3-1     | 24/12/13 14:05:13 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 14:05:13 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 41 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 14:05:13 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 14:05:59 INFO Worker: Asked to launch executor app-20241213140559-0000/1 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 14:05:59 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 14:05:59 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 14:06:03 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213140559-0000, execId=2)
spark-worker-1-1     | 24/12/13 14:06:03 INFO ExternalShuffleBlockResolver: Application app-20241213140559-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | 24/12/13 14:06:03 INFO Worker: Cleaning up local directories for application app-20241213140559-0000
spark-worker-3-1     | 24/12/13 14:05:59 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 14:05:59 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 14:05:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 14:05:59 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=39289" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@22550a098c1e:39289" "--executor-id" "1" "--hostname" "172.18.0.6" "--cores" "10" "--app-id" "app-20241213140559-0000" "--worker-url" "spark://Worker@172.18.0.6:35791" "--resourceProfileId" "0"
spark-worker-3-1     | 24/12/13 14:06:02 INFO Worker: Asked to kill executor app-20241213140559-0000/1
spark-worker-3-1     | 24/12/13 14:06:02 INFO ExecutorRunner: Runner thread for executor app-20241213140559-0000/1 interrupted
spark-worker-3-1     | 24/12/13 14:06:02 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 14:06:03 INFO Worker: Executor app-20241213140559-0000/1 finished with state KILLED exitStatus 143
spark-worker-3-1     | 24/12/13 14:06:03 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:40:53.99 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:40:53.99 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:40:53.99 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:40:53.99 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:40:53.99 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:40:54.09 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:40:54.10 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:40:54.11 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:40:54.11 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:40:54.12 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 14:40:56 INFO Worker: Started daemon with process name: 33@f484bf7bba3e
spark-worker-1-1     | 24/12/13 14:40:56 INFO SignalUtils: Registering signal handler for TERM
spark-worker-1-1     | 24/12/13 14:40:56 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 14:40:56 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 14:40:56 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 14:40:56 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 14:40:56 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 14:40:56 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 14:40:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
airflow-worker-1     | [2024-12-13 14:42:20,966: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,966: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,966: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,966: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,966: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,967: DEBUG/ForkPoolWorker-16] Failed to find locale C
spark-worker-1-1     | 24/12/13 14:40:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 14:40:56 INFO Utils: Successfully started service 'sparkWorker' on port 44861.
spark-worker-1-1     | 24/12/13 14:40:56 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 14:40:56 INFO Worker: Starting Spark worker 172.18.0.6:44861 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 14:40:56 INFO Worker: Running Spark version 3.5.0
airflow-worker-1     | [2024-12-13 14:42:20,967: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,967: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,967: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_datetime_operator_2>
airflow-worker-1     | [2024-12-13 14:42:20,967: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,967: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,967: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:20,968: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_datetime_operator_3>
airflow-worker-1     | [2024-12-13 14:42:20,968: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:20,968: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:20,968: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-worker-1-1     | 24/12/13 14:40:56 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 14:40:57 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 14:40:57 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 14:40:57 INFO ResourceUtils: ==============================================================
spark-worker-1-1     | 24/12/13 14:40:57 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 14:40:57 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-1-1     | 24/12/13 14:40:57 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 14:40:57 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 14:40:57 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 83 ms (0 ms spent in bootstraps)
spark-master-1       | 24/12/13 14:58:31 INFO Master: Registering app MyAwesomeSpark
spark-master-1       | 24/12/13 14:58:31 INFO Master: Registered app MyAwesomeSpark with ID app-20241213145831-0000
airflow-worker-1     | [2024-12-13 14:42:20,968: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_datetime_operator>
airflow-worker-1     | [2024-12-13 14:42:20,968: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dag_decorator.py
airflow-worker-1     | [2024-12-13 14:42:21,006: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_dag_decorator>
airflow-worker-1     | [2024-12-13 14:42:21,006: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_day_of_week_operator.py
airflow-worker-1     | [2024-12-13 14:42:21,007: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:21,007: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
spark-worker-1-1     | 24/12/13 14:40:57 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 14:42:27 INFO Worker: Asked to launch executor app-20241213144226-0000/1 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 14:42:27 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 14:42:27 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 14:42:27 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 14:42:27 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 14:06:03 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213140559-0000, execId=1)
spark-worker-3-1     | 24/12/13 14:06:03 INFO ExternalShuffleBlockResolver: Application app-20241213140559-0000 removed, cleanupLocalDirs = true
spark-worker-3-1     | 24/12/13 14:06:03 INFO Worker: Cleaning up local directories for application app-20241213140559-0000
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:40:53.99 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:40:53.99 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | 24/12/13 14:42:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 14:42:27 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=42543" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@65b6e979e5f1:42543" "--executor-id" "1" "--hostname" "172.18.0.6" "--cores" "10" "--app-id" "app-20241213144226-0000" "--worker-url" "spark://Worker@172.18.0.6:44861" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 14:42:30 INFO Worker: Asked to kill executor app-20241213144226-0000/1
spark-worker-1-1     | 24/12/13 14:42:30 INFO ExecutorRunner: Runner thread for executor app-20241213144226-0000/1 interrupted
spark-worker-1-1     | 24/12/13 14:42:30 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 14:42:30 INFO Worker: Executor app-20241213144226-0000/1 finished with state KILLED exitStatus 143
spark-worker-1-1     | 24/12/13 14:42:30 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-worker-1-1     | 24/12/13 14:42:30 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213144226-0000, execId=1)
spark-worker-1-1     | 24/12/13 14:42:30 INFO Worker: Cleaning up local directories for application app-20241213144226-0000
airflow-worker-1     | [2024-12-13 14:42:21,007: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:21,007: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:21,007: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:21,008: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-worker-1-1     | 24/12/13 14:42:30 INFO ExternalShuffleBlockResolver: Application app-20241213144226-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:56:59.66 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:56:59.67 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:56:59.67 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:56:59.67 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:56:59.67 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:56:59.69 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
airflow-worker-1     | [2024-12-13 14:42:21,008: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_weekday_branch_operator>
airflow-worker-1     | [2024-12-13 14:42:21,008: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_inlet_event_extra.py
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:56:59.72 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-1-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:56:59.72 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-1-1     | 
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:56:59.72 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
airflow-worker-1     | [2024-12-13 14:42:21,008: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:21,008: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:21,009: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-master-1       | 24/12/13 14:58:31 INFO Master: Start scheduling for app app-20241213145831-0000 with rpId: 0
spark-master-1       | 24/12/13 14:58:31 INFO Master: Launching executor app-20241213145831-0000/0 on worker worker-20241213145702-172.18.0.9-46363
spark-master-1       | 24/12/13 14:58:31 INFO Master: Launching executor app-20241213145831-0000/1 on worker worker-20241213145702-172.18.0.6-43171
spark-master-1       | 24/12/13 14:58:31 INFO Master: Launching executor app-20241213145831-0000/2 on worker worker-20241213145702-172.18.0.7-45523
spark-master-1       | 24/12/13 14:58:32 INFO Master: Start scheduling for app app-20241213145831-0000 with rpId: 0
spark-master-1       | 24/12/13 14:58:32 INFO Master: Start scheduling for app app-20241213145831-0000 with rpId: 0
spark-master-1       | 24/12/13 14:58:32 INFO Master: Start scheduling for app app-20241213145831-0000 with rpId: 0
spark-master-1       | 24/12/13 14:58:35 INFO Master: Received unregister request from application app-20241213145831-0000
spark-master-1       | 24/12/13 14:58:35 INFO Master: Removing app app-20241213145831-0000
spark-worker-1-1     | [38;5;6mspark [38;5;5m14:56:59.73 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-1-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-f484bf7bba3e.out
spark-worker-1-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-1-1     | ========================================
spark-worker-1-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-1-1     | 24/12/13 14:57:01 INFO Worker: Started daemon with process name: 34@f484bf7bba3e
spark-worker-1-1     | 24/12/13 14:57:01 INFO SignalUtils: Registering signal handler for TERM
spark-master-1       | 24/12/13 14:58:35 WARN Master: Got status update for unknown executor app-20241213145831-0000/1
spark-master-1       | 24/12/13 14:58:35 WARN Master: Got status update for unknown executor app-20241213145831-0000/2
spark-master-1       | 24/12/13 14:58:35 INFO Master: 172.18.0.11:55100 got disassociated, removing it.
spark-master-1       | 24/12/13 14:58:35 INFO Master: 65b6e979e5f1:35597 got disassociated, removing it.
spark-worker-1-1     | 24/12/13 14:57:01 INFO SignalUtils: Registering signal handler for HUP
spark-worker-1-1     | 24/12/13 14:57:01 INFO SignalUtils: Registering signal handler for INT
spark-worker-1-1     | 24/12/13 14:57:01 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 14:57:01 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 14:57:01 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 14:57:01 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 14:57:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 14:57:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-1-1     | 24/12/13 14:57:02 INFO Utils: Successfully started service 'sparkWorker' on port 43171.
spark-worker-1-1     | 24/12/13 14:57:02 INFO Worker: Worker decommissioning not enabled.
spark-worker-1-1     | 24/12/13 14:57:02 INFO Worker: Starting Spark worker 172.18.0.6:43171 with 10 cores, 2.0 GiB RAM
spark-worker-1-1     | 24/12/13 14:57:02 INFO Worker: Running Spark version 3.5.0
spark-worker-1-1     | 24/12/13 14:57:02 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-1-1     | 24/12/13 14:57:02 INFO ResourceUtils: ==============================================================
spark-master-1       | 24/12/13 14:58:35 WARN Master: Got status update for unknown executor app-20241213145831-0000/0
spark-master-1       | 24/12/13 15:02:12 INFO Master: 172.18.0.7:45154 got disassociated, removing it.
spark-master-1       | 24/12/13 15:02:12 INFO Master: 172.18.0.7:45523 got disassociated, removing it.
spark-master-1       | 24/12/13 15:02:12 INFO Master: Removing worker worker-20241213145702-172.18.0.7-45523 on 172.18.0.7:45523
spark-master-1       | 24/12/13 15:02:12 INFO Master: Telling app of lost worker: worker-20241213145702-172.18.0.7-45523
spark-master-1       | 24/12/13 15:02:12 INFO Master: 172.18.0.9:48548 got disassociated, removing it.
spark-master-1       | 24/12/13 15:02:12 INFO Master: 172.18.0.9:46363 got disassociated, removing it.
spark-master-1       | 24/12/13 15:02:12 INFO Master: Removing worker worker-20241213145702-172.18.0.9-46363 on 172.18.0.9:46363
spark-master-1       | 24/12/13 15:02:12 INFO Master: Telling app of lost worker: worker-20241213145702-172.18.0.9-46363
spark-master-1       | 24/12/13 15:02:12 INFO Master: 172.18.0.6:51840 got disassociated, removing it.
spark-master-1       | 24/12/13 15:02:12 INFO Master: 172.18.0.6:43171 got disassociated, removing it.
spark-master-1       | 24/12/13 15:02:12 INFO Master: Removing worker worker-20241213145702-172.18.0.6-43171 on 172.18.0.6:43171
spark-worker-1-1     | 24/12/13 14:57:02 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-1-1     | 24/12/13 14:57:02 INFO ResourceUtils: ==============================================================
airflow-worker-1     | [2024-12-13 14:42:21,009: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:21,009: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:21,009: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:21,010: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:21,010: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:40:53.99 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:40:53.99 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:40:53.99 [0m[38;5;2mINFO [0m ==> 
spark-worker-1-1     | 24/12/13 14:57:02 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-1-1     | 24/12/13 14:57:02 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-1-1     | 24/12/13 14:57:02 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://f484bf7bba3e:8081
spark-worker-1-1     | 24/12/13 14:57:02 INFO Worker: Connecting to master spark-master:7077...
spark-worker-1-1     | 24/12/13 14:57:02 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 72 ms (0 ms spent in bootstraps)
spark-worker-1-1     | 24/12/13 14:57:03 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-1-1     | 24/12/13 14:58:31 INFO Worker: Asked to launch executor app-20241213145831-0000/1 for MyAwesomeSpark
spark-worker-1-1     | 24/12/13 14:58:32 INFO SecurityManager: Changing view acls to: spark
spark-worker-1-1     | 24/12/13 14:58:32 INFO SecurityManager: Changing modify acls to: spark
spark-worker-1-1     | 24/12/13 14:58:32 INFO SecurityManager: Changing view acls groups to: 
spark-worker-1-1     | 24/12/13 14:58:32 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-1-1     | 24/12/13 14:58:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-1-1     | 24/12/13 14:58:32 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=35597" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@65b6e979e5f1:35597" "--executor-id" "1" "--hostname" "172.18.0.6" "--cores" "10" "--app-id" "app-20241213145831-0000" "--worker-url" "spark://Worker@172.18.0.6:43171" "--resourceProfileId" "0"
spark-worker-1-1     | 24/12/13 14:58:35 INFO Worker: Asked to kill executor app-20241213145831-0000/1
spark-worker-1-1     | 24/12/13 14:58:35 INFO ExecutorRunner: Runner thread for executor app-20241213145831-0000/1 interrupted
spark-worker-1-1     | 24/12/13 14:58:35 INFO ExecutorRunner: Killing process!
spark-worker-1-1     | 24/12/13 14:58:35 INFO Worker: Executor app-20241213145831-0000/1 finished with state KILLED exitStatus 0
spark-worker-1-1     | 24/12/13 14:58:35 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
spark-worker-1-1     | 24/12/13 14:58:35 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213145831-0000, execId=1)
spark-worker-1-1     | 24/12/13 14:58:35 INFO ExternalShuffleBlockResolver: Application app-20241213145831-0000 removed, cleanupLocalDirs = true
spark-worker-1-1     | 24/12/13 14:58:35 INFO Worker: Cleaning up local directories for application app-20241213145831-0000
airflow-worker-1     | [2024-12-13 14:42:21,010: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:21,010: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: read_dataset_event>
airflow-worker-1     | [2024-12-13 14:42:21,010: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:21,010: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:21,010: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:21,010: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: read_dataset_event_from_classic>
airflow-worker-1     | [2024-12-13 14:42:21,010: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping.py
airflow-worker-1     | [2024-12-13 14:42:21,012: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_dynamic_task_mapping>
airflow-worker-1     | [2024-12-13 14:42:21,012: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_subdag_operator.py
airflow-worker-1     | [2024-12-13 14:42:21,013: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:21,013: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:21,013: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:21,019: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:21,020: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:21,020: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:21,024: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_subdag_operator.section-1>
airflow-worker-1     | [2024-12-13 14:42:21,024: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_subdag_operator.section-2>
airflow-worker-1     | [2024-12-13 14:42:21,024: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_subdag_operator>
airflow-worker-1     | [2024-12-13 14:42:21,024: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial.py
airflow-worker-1     | [2024-12-13 14:42:21,025: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: tutorial>
airflow-worker-1     | [2024-12-13 14:42:21,026: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_complex.py
airflow-worker-1     | [2024-12-13 14:42:21,028: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_complex>
airflow-worker-1     | [2024-12-13 14:42:21,028: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_python_dop_operator_3.py
airflow-worker-1     | [2024-12-13 14:42:21,029: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:21,029: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:21,029: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-master-1       | 24/12/13 15:02:12 INFO Master: Telling app of lost worker: worker-20241213145702-172.18.0.6-43171
airflow-worker-1     | [2024-12-13 14:42:21,030: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_dop_operator_v3>
airflow-worker-1     | [2024-12-13 14:42:21,030: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/subdags/subdag.py
airflow-worker-1     | [2024-12-13 14:42:21,030: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/event_listener.py
airflow-worker-1     | [2024-12-13 14:42:21,030: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/listener_plugin.py
airflow-worker-1     | [2024-12-13 14:42:21,031: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/decreasing_priority_weight_strategy.py
airflow-worker-1     | [2024-12-13 14:42:21,031: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/workday.py
airflow-worker-1     | [2024-12-13 14:42:21,057: DEBUG/ForkPoolWorker-16] Plugins are already loaded. Skipping.
airflow-worker-1     | [2024-12-13 14:42:21,057: DEBUG/ForkPoolWorker-16] Integrate DAG plugins
airflow-worker-1     | [2024-12-13 14:42:21,059: DEBUG/ForkPoolWorker-16] previous_execution_date was called
airflow-worker-1     | [2024-12-13 14:42:21,068: INFO/ForkPoolWorker-16] Running <TaskInstance: pipeline.upload_to_minio manual__2024-12-13T14:42:07.082582+00:00 [queued]> on host 65b6e979e5f1
airflow-worker-1     | [2024-12-13 14:42:21,068: DEBUG/ForkPoolWorker-16] Disposing DB connection pool (PID 82)
airflow-worker-1     | [2024-12-13 14:42:21,068: DEBUG/ForkPoolWorker-16] Setting up DB connection pool (PID 82)
airflow-worker-1     | [2024-12-13 14:42:21,068: DEBUG/ForkPoolWorker-16] settings.prepare_engine_args(): Using NullPool
airflow-worker-1     | [2024-12-13 14:42:21,429: DEBUG/ForkPoolWorker-16] Calling callbacks: []
airflow-worker-1     | [2024-12-13 14:42:21,445: INFO/ForkPoolWorker-16] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[4bfa70d0-8f9d-47ad-ad62-f601a99fd476] succeeded in 1.8055745849997038s: None
airflow-worker-1     | [2024-12-13 14:42:22,374: INFO/MainProcess] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[6b4a109a-dc6a-4f8e-a6dd-bd03e7bafc3b] received
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:40:54.09 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:40:54.12 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:40:54.12 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:40:54.12 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:40:54.14 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
airflow-worker-1     | [2024-12-13 14:42:22,382: INFO/ForkPoolWorker-15] [6b4a109a-dc6a-4f8e-a6dd-bd03e7bafc3b] Executing command in Celery: ['airflow', 'tasks', 'run', 'pipeline', 'train_and_predict', 'manual__2024-12-13T14:42:07.082582+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-worker-1     | [2024-12-13 14:42:22,648: INFO/ForkPoolWorker-15] Filling up the DagBag from /opt/airflow/dags/pipeline.py
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 14:40:56 INFO Worker: Started daemon with process name: 34@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 14:40:56 INFO SignalUtils: Registering signal handler for TERM
airflow-worker-1     | [2024-12-13 14:42:22,980: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: pipeline>
airflow-worker-1     | [2024-12-13 14:42:22,980: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcomargs.py
airflow-worker-1     | [2024-12-13 14:42:22,988: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom_args>
airflow-worker-1     | [2024-12-13 14:42:22,989: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom_args_with_operators>
spark-worker-3-1     | 24/12/13 14:40:56 INFO SignalUtils: Registering signal handler for HUP
spark-worker-3-1     | 24/12/13 14:40:56 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 14:40:56 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 14:40:56 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 14:40:56 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 14:40:56 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 14:40:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 14:40:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
spark-worker-3-1     | 24/12/13 14:40:56 INFO Utils: Successfully started service 'sparkWorker' on port 32921.
spark-worker-3-1     | 24/12/13 14:40:56 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 14:40:56 INFO Worker: Starting Spark worker 172.18.0.7:32921 with 10 cores, 2.0 GiB RAM
spark-worker-3-1     | 24/12/13 14:40:56 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 14:40:56 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 14:40:57 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 14:40:57 INFO ResourceUtils: No custom resources configured for spark.worker.
spark-worker-3-1     | 24/12/13 14:40:57 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 14:40:57 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 14:40:57 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
spark-worker-3-1     | 24/12/13 14:40:57 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-3-1     | 24/12/13 14:40:57 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 14:40:57 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 63 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 14:40:57 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 14:42:27 INFO Worker: Asked to launch executor app-20241213144226-0000/0 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 14:42:27 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 14:42:27 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 14:42:27 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 14:42:27 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 14:42:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 14:42:27 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=42543" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@65b6e979e5f1:42543" "--executor-id" "0" "--hostname" "172.18.0.7" "--cores" "10" "--app-id" "app-20241213144226-0000" "--worker-url" "spark://Worker@172.18.0.7:32921" "--resourceProfileId" "0"
spark-worker-3-1     | 24/12/13 14:42:30 INFO Worker: Asked to kill executor app-20241213144226-0000/0
spark-worker-3-1     | 24/12/13 14:42:30 INFO ExecutorRunner: Runner thread for executor app-20241213144226-0000/0 interrupted
spark-worker-3-1     | 24/12/13 14:42:30 INFO ExecutorRunner: Killing process!
spark-worker-3-1     | 24/12/13 14:42:30 INFO Worker: Executor app-20241213144226-0000/0 finished with state KILLED exitStatus 143
spark-worker-3-1     | 24/12/13 14:42:30 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-3-1     | 24/12/13 14:42:30 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213144226-0000, execId=0)
spark-worker-3-1     | 24/12/13 14:42:30 INFO ExternalShuffleBlockResolver: Application app-20241213144226-0000 removed, cleanupLocalDirs = true
spark-worker-3-1     | 24/12/13 14:42:30 INFO Worker: Cleaning up local directories for application app-20241213144226-0000
airflow-worker-1     | [2024-12-13 14:42:22,989: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensors.py
airflow-worker-1     | [2024-12-13 14:42:22,992: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sensors>
airflow-worker-1     | [2024-12-13 14:42:22,992: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
airflow-worker-1     | [2024-12-13 14:42:22,993: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: latest_only_with_trigger>
airflow-worker-1     | [2024-12-13 14:42:22,993: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:42:22,994: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_local_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:42:22,994: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_nested_branch_dag.py
airflow-worker-1     | [2024-12-13 14:42:22,995: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:22,995: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:22,995: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:22,996: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:22,996: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:56:59.73 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:56:59.73 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:56:59.74 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:56:59.74 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:56:59.74 [0m[38;5;2mINFO [0m ==> 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:56:59.76 [0m[38;5;2mINFO [0m ==> ** Starting Spark setup **
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:56:59.78 [0m[38;5;2mINFO [0m ==> Detected mounted configuration file...
spark-worker-3-1     | find: '/docker-entrypoint-initdb.d/': No such file or directory
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:56:59.78 [0m[38;5;2mINFO [0m ==> No custom scripts in /docker-entrypoint-initdb.d
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:56:59.78 [0m[38;5;2mINFO [0m ==> ** Spark setup finished! **
spark-worker-3-1     | 
spark-worker-3-1     | [38;5;6mspark [38;5;5m14:56:59.79 [0m[38;5;2mINFO [0m ==> ** Starting Spark in worker mode **
spark-worker-3-1     | starting org.apache.spark.deploy.worker.Worker, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.deploy.worker.Worker-1-ed56f9ca93ac.out
spark-worker-3-1     | Spark Command: /opt/bitnami/java/bin/java -cp /opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://spark-master:7077
spark-worker-3-1     | ========================================
spark-worker-3-1     | Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
spark-worker-3-1     | 24/12/13 14:57:01 INFO Worker: Started daemon with process name: 34@ed56f9ca93ac
spark-worker-3-1     | 24/12/13 14:57:01 INFO SignalUtils: Registering signal handler for TERM
spark-worker-3-1     | 24/12/13 14:57:01 INFO SignalUtils: Registering signal handler for HUP
airflow-worker-1     | [2024-12-13 14:42:22,996: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:22,997: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_nested_branch_dag>
airflow-worker-1     | [2024-12-13 14:42:22,997: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py
airflow-worker-1     | [2024-12-13 14:42:22,998: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dynamic_task_mapping_with_no_taskflow_operators>
airflow-worker-1     | [2024-12-13 14:42:22,998: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_operator.py
airflow-worker-1     | [2024-12-13 14:42:22,998: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:22,998: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
spark-worker-3-1     | 24/12/13 14:57:01 INFO SignalUtils: Registering signal handler for INT
spark-worker-3-1     | 24/12/13 14:57:01 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 14:57:01 INFO SecurityManager: Changing modify acls to: spark
airflow-worker-1     | [2024-12-13 14:42:22,999: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,000: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:42:23,000: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:42:23,000: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator.py
airflow-worker-1     | [2024-12-13 14:42:23,001: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,001: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,001: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,003: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,003: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,003: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
spark-worker-3-1     | 24/12/13 14:57:01 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 14:57:01 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 14:57:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 14:57:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
airflow-worker-1     | [2024-12-13 14:42:23,004: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_operator>
airflow-worker-1     | [2024-12-13 14:42:23,004: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_passing_params_via_test_command.py
airflow-worker-1     | [2024-12-13 14:42:23,005: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,005: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,005: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,006: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_passing_params_via_test_command>
airflow-worker-1     | [2024-12-13 14:42:23,006: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_objectstorage.py
airflow-worker-1     | [2024-12-13 14:42:23,007: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_objectstorage>
spark-worker-3-1     | 24/12/13 14:57:02 INFO Utils: Successfully started service 'sparkWorker' on port 46363.
spark-worker-3-1     | 24/12/13 14:57:02 INFO Worker: Worker decommissioning not enabled.
spark-worker-3-1     | 24/12/13 14:57:02 INFO Worker: Starting Spark worker 172.18.0.9:46363 with 10 cores, 2.0 GiB RAM
airflow-worker-1     | [2024-12-13 14:42:23,007: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_decorator.py
airflow-worker-1     | [2024-12-13 14:42:23,009: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_python_decorator>
airflow-worker-1     | [2024-12-13 14:42:23,009: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_trigger_ui.py
airflow-worker-1     | [2024-12-13 14:42:23,011: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_params_trigger_ui>
spark-worker-3-1     | 24/12/13 14:57:02 INFO Worker: Running Spark version 3.5.0
spark-worker-3-1     | 24/12/13 14:57:02 INFO Worker: Spark home: /opt/bitnami/spark
spark-worker-3-1     | 24/12/13 14:57:02 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 14:57:02 INFO ResourceUtils: No custom resources configured for spark.worker.
airflow-worker-1     | [2024-12-13 14:42:23,011: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_ui_tutorial.py
airflow-worker-1     | [2024-12-13 14:42:23,012: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_params_ui_tutorial>
airflow-worker-1     | [2024-12-13 14:42:23,012: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown.py
spark-worker-3-1     | 24/12/13 14:57:02 INFO ResourceUtils: ==============================================================
spark-worker-3-1     | 24/12/13 14:57:02 INFO JettyUtils: Start Jetty 0.0.0.0:8081 for WorkerUI
spark-worker-3-1     | 24/12/13 14:57:02 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
airflow-worker-1     | [2024-12-13 14:42:23,013: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_setup_teardown>
airflow-worker-1     | [2024-12-13 14:42:23,013: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sla_dag.py
airflow-worker-1     | [2024-12-13 14:42:23,014: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,014: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,014: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,015: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sla_dag>
airflow-worker-1     | [2024-12-13 14:42:23,015: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator_decorator.py
airflow-worker-1     | [2024-12-13 14:42:23,015: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,015: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,015: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,019: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,019: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,019: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,019: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_python_operator_decorator>
airflow-worker-1     | [2024-12-13 14:42:23,019: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_datasets.py
airflow-worker-1     | [2024-12-13 14:42:23,020: DEBUG/ForkPoolWorker-15] Initializing Providers Manager[dataset_uris]
airflow-worker-1     | [2024-12-13 14:42:23,201: DEBUG/ForkPoolWorker-15] Initialization of Providers Manager[dataset_uris] took 0.18 seconds
airflow-worker-1     | [2024-12-13 14:42:23,201: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,201: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,202: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,204: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,204: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
spark-worker-3-1     | 24/12/13 14:57:02 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://ed56f9ca93ac:8081
spark-worker-3-1     | 24/12/13 14:57:02 INFO Worker: Connecting to master spark-master:7077...
spark-worker-3-1     | 24/12/13 14:57:02 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 53 ms (0 ms spent in bootstraps)
spark-worker-3-1     | 24/12/13 14:57:03 INFO Worker: Successfully registered with master spark://f9573584fb53:7077
spark-worker-3-1     | 24/12/13 14:58:31 INFO Worker: Asked to launch executor app-20241213145831-0000/0 for MyAwesomeSpark
spark-worker-3-1     | 24/12/13 14:58:32 INFO SecurityManager: Changing view acls to: spark
spark-worker-3-1     | 24/12/13 14:58:32 INFO SecurityManager: Changing modify acls to: spark
spark-worker-3-1     | 24/12/13 14:58:32 INFO SecurityManager: Changing view acls groups to: 
spark-worker-3-1     | 24/12/13 14:58:32 INFO SecurityManager: Changing modify acls groups to: 
spark-worker-3-1     | 24/12/13 14:58:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
spark-worker-3-1     | 24/12/13 14:58:32 INFO ExecutorRunner: Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=35597" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@65b6e979e5f1:35597" "--executor-id" "0" "--hostname" "172.18.0.9" "--cores" "10" "--app-id" "app-20241213145831-0000" "--worker-url" "spark://Worker@172.18.0.9:46363" "--resourceProfileId" "0"
spark-worker-3-1     | 24/12/13 14:58:35 INFO Worker: Asked to kill executor app-20241213145831-0000/0
spark-worker-3-1     | 24/12/13 14:58:35 INFO ExecutorRunner: Runner thread for executor app-20241213145831-0000/0 interrupted
spark-worker-3-1     | 24/12/13 14:58:35 INFO ExecutorRunner: Killing process!
airflow-worker-1     | [2024-12-13 14:42:23,204: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,205: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1>
airflow-worker-1     | [2024-12-13 14:42:23,205: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,205: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,205: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,205: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_produces_1>
spark-worker-3-1     | 24/12/13 14:58:35 INFO Worker: Executor app-20241213145831-0000/0 finished with state KILLED exitStatus 143
spark-worker-3-1     | 24/12/13 14:58:35 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
spark-worker-3-1     | 24/12/13 14:58:35 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241213145831-0000, execId=0)
airflow-worker-1     | [2024-12-13 14:42:23,205: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1_and_2>
airflow-worker-1     | [2024-12-13 14:42:23,205: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_and_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:42:23,205: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_produces_2>
airflow-worker-1     | [2024-12-13 14:42:23,205: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_unknown_never_scheduled>
airflow-worker-1     | [2024-12-13 14:42:23,205: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1_never_scheduled>
spark-worker-3-1     | 24/12/13 14:58:35 INFO Worker: Cleaning up local directories for application app-20241213145831-0000
spark-worker-3-1     | 24/12/13 14:58:35 INFO ExternalShuffleBlockResolver: Application app-20241213145831-0000 removed, cleanupLocalDirs = true
airflow-worker-1     | [2024-12-13 14:42:23,205: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: conditional_dataset_and_time_based_timetable>
airflow-worker-1     | [2024-12-13 14:42:23,205: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_or_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:42:23,205: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_or_both_2_and_3_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:42:23,206: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_outlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:42:23,207: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,207: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,207: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,207: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,207: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,207: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,208: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,208: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,208: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,209: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,209: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,209: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,209: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_by_yield>
airflow-worker-1     | [2024-12-13 14:42:23,209: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,209: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,209: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,210: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_from_classic_operator>
airflow-worker-1     | [2024-12-13 14:42:23,210: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,210: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,210: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,210: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_by_context>
airflow-worker-1     | [2024-12-13 14:42:23,210: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_decorator.py
airflow-worker-1     | [2024-12-13 14:42:23,212: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_decorator>
airflow-worker-1     | [2024-12-13 14:42:23,212: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_decorator.py
airflow-worker-1     | [2024-12-13 14:42:23,214: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_short_circuit_decorator>
airflow-worker-1     | [2024-12-13 14:42:23,214: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_controller_dag.py
airflow-worker-1     | [2024-12-13 14:42:23,216: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_trigger_controller_dag>
airflow-worker-1     | [2024-12-13 14:42:23,216: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only.py
airflow-worker-1     | [2024-12-13 14:42:23,217: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: latest_only>
airflow-worker-1     | [2024-12-13 14:42:23,218: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_time_delta_sensor_async.py
airflow-worker-1     | [2024-12-13 14:42:23,218: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_time_delta_sensor_async>
airflow-worker-1     | [2024-12-13 14:42:23,218: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_templates.py
airflow-worker-1     | [2024-12-13 14:42:23,219: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,219: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,219: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,220: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,220: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,220: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,221: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_templates>
airflow-worker-1     | [2024-12-13 14:42:23,221: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api_virtualenv.py
airflow-worker-1     | [2024-12-13 14:42:23,222: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_api_virtualenv>
airflow-worker-1     | [2024-12-13 14:42:23,222: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensor_decorator.py
airflow-worker-1     | [2024-12-13 14:42:23,223: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sensor_decorator>
airflow-worker-1     | [2024-12-13 14:42:23,223: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_operator.py
airflow-worker-1     | [2024-12-13 14:42:23,224: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_short_circuit_operator>
airflow-worker-1     | [2024-12-13 14:42:23,224: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcom.py
airflow-worker-1     | [2024-12-13 14:42:23,226: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom>
airflow-worker-1     | [2024-12-13 14:42:23,226: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias.py
airflow-worker-1     | [2024-12-13 14:42:23,368: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_consumer>
airflow-worker-1     | [2024-12-13 14:42:23,368: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_producer>
airflow-worker-1     | [2024-12-13 14:42:23,368: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_consumer>
airflow-worker-1     | [2024-12-13 14:42:23,368: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_producer>
airflow-worker-1     | [2024-12-13 14:42:23,368: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown_taskflow.py
airflow-worker-1     | [2024-12-13 14:42:23,370: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_setup_teardown_taskflow>
airflow-worker-1     | [2024-12-13 14:42:23,370: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api.py
airflow-worker-1     | [2024-12-13 14:42:23,371: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_api>
airflow-worker-1     | [2024-12-13 14:42:23,371: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_labels.py
airflow-worker-1     | [2024-12-13 14:42:23,372: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,372: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,372: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,372: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,372: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,372: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,373: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_labels>
airflow-worker-1     | [2024-12-13 14:42:23,373: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:42:23,376: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:42:23,376: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_display_name.py
airflow-worker-1     | [2024-12-13 14:42:23,376: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_display_name>
airflow-worker-1     | [2024-12-13 14:42:23,376: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
airflow-worker-1     | [2024-12-13 14:42:23,382: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:23,382: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:23,382: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:23,382: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:42:23,382: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group_decorator.py
airflow-worker-1     | [2024-12-13 14:42:23,386: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_task_group_decorator>
airflow-worker-1     | [2024-12-13 14:42:23,386: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_target_dag.py
airflow-worker-1     | [2024-12-13 14:42:23,387: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_trigger_target_dag>
airflow-worker-1     | [2024-12-13 14:42:23,387: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_external_task_marker_dag.py
airflow-worker-1     | [2024-12-13 14:42:23,389: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_external_task_marker_parent>
airflow-worker-1     | [2024-12-13 14:42:23,389: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_external_task_marker_child>
airflow-worker-1     | [2024-12-13 14:42:23,389: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_skip_dag.py
airflow-worker-1     | [2024-12-13 14:42:23,391: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_skip_dag>
airflow-worker-1     | [2024-12-13 14:42:23,391: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_workday_timetable.py
airflow-worker-1     | [2024-12-13 14:42:23,392: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_workday_timetable>
airflow-worker-1     | [2024-12-13 14:42:23,392: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_dag.py
airflow-worker-1     | [2024-12-13 14:42:23,393: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_dag>
airflow-worker-1     | [2024-12-13 14:42:23,393: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_operator.py
airflow-worker-1     | [2024-12-13 14:42:23,394: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_python_operator>
airflow-worker-1     | [2024-12-13 14:42:23,394: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group.py
airflow-worker-1     | [2024-12-13 14:42:23,395: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_task_group>
airflow-worker-1     | [2024-12-13 14:42:23,395: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_datetime_operator.py
airflow-worker-1     | [2024-12-13 14:42:23,396: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,396: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,396: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,396: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,396: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,396: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,397: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,397: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,397: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,397: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,397: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,398: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,398: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator_2>
airflow-worker-1     | [2024-12-13 14:42:23,398: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,398: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,398: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,398: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator_3>
airflow-worker-1     | [2024-12-13 14:42:23,398: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,398: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,398: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,398: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator>
airflow-worker-1     | [2024-12-13 14:42:23,399: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dag_decorator.py
airflow-worker-1     | [2024-12-13 14:42:23,435: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dag_decorator>
airflow-worker-1     | [2024-12-13 14:42:23,435: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_day_of_week_operator.py
airflow-worker-1     | [2024-12-13 14:42:23,436: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,436: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,436: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,437: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,437: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,437: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,437: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_weekday_branch_operator>
airflow-worker-1     | [2024-12-13 14:42:23,437: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_inlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:42:23,437: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,437: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,438: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,438: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,438: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,438: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,438: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,438: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,438: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,439: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: read_dataset_event>
airflow-worker-1     | [2024-12-13 14:42:23,439: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,439: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,439: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,439: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: read_dataset_event_from_classic>
airflow-worker-1     | [2024-12-13 14:42:23,439: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping.py
airflow-worker-1     | [2024-12-13 14:42:23,441: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dynamic_task_mapping>
airflow-worker-1     | [2024-12-13 14:42:23,441: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_subdag_operator.py
airflow-worker-1     | [2024-12-13 14:42:23,442: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,442: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,442: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,447: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,447: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,447: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,451: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator.section-1>
airflow-worker-1     | [2024-12-13 14:42:23,451: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator.section-2>
airflow-worker-1     | [2024-12-13 14:42:23,452: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator>
airflow-worker-1     | [2024-12-13 14:42:23,452: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial.py
airflow-worker-1     | [2024-12-13 14:42:23,453: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial>
airflow-worker-1     | [2024-12-13 14:42:23,453: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_complex.py
airflow-worker-1     | [2024-12-13 14:42:23,456: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_complex>
airflow-worker-1     | [2024-12-13 14:42:23,456: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_python_dop_operator_3.py
airflow-worker-1     | [2024-12-13 14:42:23,456: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:42:23,456: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:42:23,456: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:42:23,457: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_dop_operator_v3>
airflow-worker-1     | [2024-12-13 14:42:23,458: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/subdags/subdag.py
airflow-worker-1     | [2024-12-13 14:42:23,458: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/event_listener.py
airflow-worker-1     | [2024-12-13 14:42:23,458: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/listener_plugin.py
airflow-worker-1     | [2024-12-13 14:42:23,459: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/decreasing_priority_weight_strategy.py
airflow-worker-1     | [2024-12-13 14:42:23,459: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/workday.py
airflow-worker-1     | [2024-12-13 14:42:23,476: DEBUG/ForkPoolWorker-15] Plugins are already loaded. Skipping.
airflow-worker-1     | [2024-12-13 14:42:23,476: DEBUG/ForkPoolWorker-15] Integrate DAG plugins
airflow-worker-1     | [2024-12-13 14:42:23,478: DEBUG/ForkPoolWorker-15] previous_execution_date was called
airflow-worker-1     | [2024-12-13 14:42:23,483: INFO/ForkPoolWorker-15] Running <TaskInstance: pipeline.train_and_predict manual__2024-12-13T14:42:07.082582+00:00 [queued]> on host 65b6e979e5f1
airflow-worker-1     | [2024-12-13 14:42:23,483: DEBUG/ForkPoolWorker-15] Disposing DB connection pool (PID 85)
airflow-worker-1     | [2024-12-13 14:42:23,483: DEBUG/ForkPoolWorker-15] Setting up DB connection pool (PID 85)
airflow-worker-1     | [2024-12-13 14:42:23,483: DEBUG/ForkPoolWorker-15] settings.prepare_engine_args(): Using NullPool
airflow-worker-1     | Setting default log level to "WARN".
airflow-worker-1     | To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
airflow-worker-1     | 24/12/13 14:42:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
airflow-worker-1     | 24/12/13 14:42:30 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://movielens/train.csv.
airflow-worker-1     | java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
airflow-worker-1     | 	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
airflow-worker-1     | 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
airflow-worker-1     | 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
airflow-worker-1     | 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
airflow-worker-1     | 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
airflow-worker-1     | 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
airflow-worker-1     | 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
airflow-worker-1     | 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
airflow-worker-1     | 	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
airflow-worker-1     | 	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
airflow-worker-1     | 	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
airflow-worker-1     | 	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
airflow-worker-1     | 	at scala.Option.getOrElse(Option.scala:189)
airflow-worker-1     | 	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
airflow-worker-1     | 	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
airflow-worker-1     | 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
airflow-worker-1     | 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
airflow-worker-1     | 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
airflow-worker-1     | 	at java.lang.reflect.Method.invoke(Method.java:498)
airflow-worker-1     | 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
airflow-worker-1     | 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
airflow-worker-1     | 	at py4j.Gateway.invoke(Gateway.java:282)
airflow-worker-1     | 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
airflow-worker-1     | 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
airflow-worker-1     | 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
airflow-worker-1     | 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
airflow-worker-1     | 	at java.lang.Thread.run(Thread.java:750)
airflow-worker-1     | Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
airflow-worker-1     | 	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
airflow-worker-1     | 	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
airflow-worker-1     | 	... 26 more
airflow-worker-1     | 24/12/13 14:42:30 WARN TransportChannelHandler: Exception in connection from /172.18.0.7:50848
airflow-worker-1     | java.io.IOException: Connection reset by peer
airflow-worker-1     | 	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
airflow-worker-1     | 	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
airflow-worker-1     | 	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
airflow-worker-1     | 	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
airflow-worker-1     | 	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
airflow-worker-1     | 	at io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:254)
airflow-worker-1     | 	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)
airflow-worker-1     | 	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357)
airflow-worker-1     | 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)
airflow-worker-1     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
airflow-worker-1     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
airflow-worker-1     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
airflow-worker-1     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
airflow-worker-1     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
airflow-worker-1     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
airflow-worker-1     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
airflow-worker-1     | 	at java.lang.Thread.run(Thread.java:750)
airflow-worker-1     | 24/12/13 14:42:30 WARN TransportChannelHandler: Exception in connection from /172.18.0.6:55890
airflow-worker-1     | java.io.IOException: Connection reset by peer
airflow-worker-1     | 	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
airflow-worker-1     | 	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
airflow-worker-1     | 	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
airflow-worker-1     | 	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
airflow-worker-1     | 	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
airflow-worker-1     | 	at io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:254)
airflow-worker-1     | 	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)
airflow-worker-1     | 	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357)
airflow-worker-1     | 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)
airflow-worker-1     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
airflow-worker-1     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
airflow-worker-1     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
airflow-worker-1     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
airflow-worker-1     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
airflow-worker-1     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
airflow-worker-1     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
airflow-worker-1     | 	at java.lang.Thread.run(Thread.java:750)
airflow-worker-1     | 24/12/13 14:42:30 WARN TransportChannelHandler: Exception in connection from /172.18.0.8:48538
airflow-worker-1     | java.io.IOException: Connection reset by peer
airflow-worker-1     | 	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
airflow-worker-1     | 	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
airflow-worker-1     | 	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
airflow-worker-1     | 	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
airflow-worker-1     | 	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
airflow-worker-1     | 	at io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:254)
airflow-worker-1     | 	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)
airflow-worker-1     | 	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357)
airflow-worker-1     | 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)
airflow-worker-1     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
airflow-worker-1     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
airflow-worker-1     | 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
airflow-worker-1     | 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
airflow-worker-1     | 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
airflow-worker-1     | 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
airflow-worker-1     | 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
airflow-worker-1     | 	at java.lang.Thread.run(Thread.java:750)
airflow-worker-1     | [2024-12-13 14:42:30,957: DEBUG/ForkPoolWorker-15] Calling callbacks: []
airflow-worker-1     | [2024-12-13 14:42:30,974: INFO/ForkPoolWorker-15] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[6b4a109a-dc6a-4f8e-a6dd-bd03e7bafc3b] succeeded in 8.597778462999486s: None
airflow-worker-1     | 
airflow-worker-1     | worker: Warm shutdown (MainProcess)
airflow-worker-1     | [2024-12-13 14:47:00 +0000] [33] [INFO] Handling signal: term
airflow-worker-1     | [2024-12-13 14:47:00 +0000] [35] [INFO] Worker exiting (pid: 35)
airflow-worker-1     | [2024-12-13 14:47:00 +0000] [34] [INFO] Worker exiting (pid: 34)
airflow-worker-1     | [2024-12-13 14:47:00 +0000] [33] [INFO] Shutting down: Master
airflow-worker-1     | 
airflow-worker-1     | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-worker-1     | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-worker-1     | BACKEND=redis
airflow-worker-1     | DB_HOST=redis
airflow-worker-1     | DB_PORT=6379
airflow-worker-1     | 
airflow-worker-1     | /home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
airflow-worker-1     | [2024-12-13 14:57:21 +0000] [32] [INFO] Starting gunicorn 23.0.0
airflow-worker-1     | [2024-12-13 14:57:21 +0000] [32] [INFO] Listening at: http://[::]:8793 (32)
airflow-worker-1     | [2024-12-13 14:57:21 +0000] [32] [INFO] Using worker: sync
airflow-worker-1     | [2024-12-13 14:57:21 +0000] [33] [INFO] Booting worker with pid: 33
airflow-worker-1     | [2024-12-13 14:57:21 +0000] [35] [INFO] Booting worker with pid: 35
airflow-worker-1     |  
airflow-worker-1     |  -------------- celery@65b6e979e5f1 v5.4.0 (opalescent)
airflow-worker-1     | --- ***** ----- 
airflow-worker-1     | -- ******* ---- Linux-6.10.14-linuxkit-aarch64-with-glibc2.36 2024-12-13 14:57:21
airflow-worker-1     | - *** --- * --- 
airflow-worker-1     | - ** ---------- [config]
airflow-worker-1     | - ** ---------- .> app:         airflow.providers.celery.executors.celery_executor:0xffffa25692b0
airflow-worker-1     | - ** ---------- .> transport:   redis://redis:6379/0
airflow-worker-1     | - ** ---------- .> results:     postgresql://airflow:**@postgres/airflow
airflow-worker-1     | - *** --- * --- .> concurrency: 16 (prefork)
airflow-worker-1     | -- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)
airflow-worker-1     | --- ***** ----- 
airflow-worker-1     |  -------------- [queues]
airflow-worker-1     |                 .> default          exchange=default(direct) key=default
airflow-worker-1     |                 
airflow-worker-1     | 
airflow-worker-1     | [tasks]
airflow-worker-1     |   . airflow.providers.celery.executors.celery_executor_utils.execute_command
airflow-worker-1     | 
airflow-worker-1     | [2024-12-13 14:57:24,752: WARNING/MainProcess] /home/airflow/.local/lib/python3.12/site-packages/celery/worker/consumer/consumer.py:508: CPendingDeprecationWarning: The broker_connection_retry configuration setting will no longer determine
airflow-worker-1     | whether broker connection retries are made during startup in Celery 6.0 and above.
airflow-worker-1     | If you wish to retain the existing behavior for retrying connections on startup,
airflow-worker-1     | you should set broker_connection_retry_on_startup to True.
airflow-worker-1     |   warnings.warn(
airflow-worker-1     | 
airflow-worker-1     | [2024-12-13 14:57:24,760: INFO/MainProcess] Connected to redis://redis:6379/0
airflow-worker-1     | [2024-12-13 14:57:24,762: WARNING/MainProcess] /home/airflow/.local/lib/python3.12/site-packages/celery/worker/consumer/consumer.py:508: CPendingDeprecationWarning: The broker_connection_retry configuration setting will no longer determine
airflow-worker-1     | whether broker connection retries are made during startup in Celery 6.0 and above.
airflow-worker-1     | If you wish to retain the existing behavior for retrying connections on startup,
airflow-worker-1     | you should set broker_connection_retry_on_startup to True.
airflow-worker-1     |   warnings.warn(
airflow-worker-1     | 
airflow-worker-1     | [2024-12-13 14:57:24,769: INFO/MainProcess] mingle: searching for neighbors
airflow-worker-1     | [2024-12-13 14:57:25,782: INFO/MainProcess] mingle: all alone
airflow-worker-1     | [2024-12-13 14:57:25,789: INFO/MainProcess] celery@65b6e979e5f1 ready.
airflow-worker-1     | [2024-12-13 14:58:12,750: INFO/MainProcess] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[9b98a5c2-6d78-4f62-bdd3-7cc49598db0e] received
airflow-worker-1     | [2024-12-13 14:58:12,820: INFO/ForkPoolWorker-15] [9b98a5c2-6d78-4f62-bdd3-7cc49598db0e] Executing command in Celery: ['airflow', 'tasks', 'run', 'pipeline', 'download_and_unwrap', 'manual__2024-12-13T14:58:11.753084+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-worker-1     | [2024-12-13 14:58:13,272: INFO/ForkPoolWorker-15] Filling up the DagBag from /opt/airflow/dags/pipeline.py
airflow-worker-1     | [2024-12-13 14:58:13,609: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: pipeline>
airflow-worker-1     | [2024-12-13 14:58:13,609: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcomargs.py
airflow-worker-1     | [2024-12-13 14:58:13,617: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom_args>
airflow-worker-1     | [2024-12-13 14:58:13,618: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom_args_with_operators>
airflow-worker-1     | [2024-12-13 14:58:13,618: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensors.py
airflow-worker-1     | [2024-12-13 14:58:13,621: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sensors>
airflow-worker-1     | [2024-12-13 14:58:13,621: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
airflow-worker-1     | [2024-12-13 14:58:13,623: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: latest_only_with_trigger>
airflow-worker-1     | [2024-12-13 14:58:13,623: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:58:13,624: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_local_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:58:13,624: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_nested_branch_dag.py
airflow-worker-1     | [2024-12-13 14:58:13,624: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,624: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,624: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,626: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,626: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,626: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,626: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_nested_branch_dag>
airflow-worker-1     | [2024-12-13 14:58:13,626: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py
airflow-worker-1     | [2024-12-13 14:58:13,627: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dynamic_task_mapping_with_no_taskflow_operators>
airflow-worker-1     | [2024-12-13 14:58:13,627: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_operator.py
airflow-worker-1     | [2024-12-13 14:58:13,628: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,628: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,628: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,629: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:58:13,629: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:58:13,629: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator.py
airflow-worker-1     | [2024-12-13 14:58:13,630: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,630: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,630: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,632: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,632: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,632: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,633: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_operator>
airflow-worker-1     | [2024-12-13 14:58:13,633: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_passing_params_via_test_command.py
airflow-worker-1     | [2024-12-13 14:58:13,633: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,633: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,634: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,635: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_passing_params_via_test_command>
airflow-worker-1     | [2024-12-13 14:58:13,635: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_objectstorage.py
airflow-worker-1     | [2024-12-13 14:58:13,636: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_objectstorage>
airflow-worker-1     | [2024-12-13 14:58:13,636: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_decorator.py
airflow-worker-1     | [2024-12-13 14:58:13,637: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_python_decorator>
airflow-worker-1     | [2024-12-13 14:58:13,637: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_trigger_ui.py
airflow-worker-1     | [2024-12-13 14:58:13,639: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_params_trigger_ui>
airflow-worker-1     | [2024-12-13 14:58:13,639: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_ui_tutorial.py
airflow-worker-1     | [2024-12-13 14:58:13,641: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_params_ui_tutorial>
airflow-worker-1     | [2024-12-13 14:58:13,641: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown.py
airflow-worker-1     | [2024-12-13 14:58:13,642: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_setup_teardown>
airflow-worker-1     | [2024-12-13 14:58:13,642: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sla_dag.py
airflow-worker-1     | [2024-12-13 14:58:13,642: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,642: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,642: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,643: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sla_dag>
airflow-worker-1     | [2024-12-13 14:58:13,643: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator_decorator.py
airflow-worker-1     | [2024-12-13 14:58:13,644: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,644: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,644: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,647: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,647: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,648: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,648: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_python_operator_decorator>
airflow-worker-1     | [2024-12-13 14:58:13,648: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_datasets.py
airflow-worker-1     | [2024-12-13 14:58:13,649: DEBUG/ForkPoolWorker-15] Initializing Providers Manager[dataset_uris]
airflow-worker-1     | [2024-12-13 14:58:13,919: DEBUG/ForkPoolWorker-15] Initialization of Providers Manager[dataset_uris] took 0.27 seconds
airflow-worker-1     | [2024-12-13 14:58:13,920: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,920: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,920: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,922: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,922: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,922: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,922: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1_and_2>
airflow-worker-1     | [2024-12-13 14:58:13,922: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: conditional_dataset_and_time_based_timetable>
airflow-worker-1     | [2024-12-13 14:58:13,922: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_and_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:58:13,922: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_or_both_2_and_3_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:58:13,923: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_produces_2>
airflow-worker-1     | [2024-12-13 14:58:13,923: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1_never_scheduled>
airflow-worker-1     | [2024-12-13 14:58:13,923: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1>
airflow-worker-1     | [2024-12-13 14:58:13,923: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_or_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:58:13,923: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_unknown_never_scheduled>
airflow-worker-1     | [2024-12-13 14:58:13,923: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,923: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,923: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,923: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_produces_1>
airflow-worker-1     | [2024-12-13 14:58:13,923: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_outlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:58:13,925: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,925: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,925: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,925: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,925: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,925: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,926: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,926: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,926: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,927: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,927: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,927: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,927: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_from_classic_operator>
airflow-worker-1     | [2024-12-13 14:58:13,927: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,927: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,927: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,928: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_by_yield>
airflow-worker-1     | [2024-12-13 14:58:13,928: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,928: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,928: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,928: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_by_context>
airflow-worker-1     | [2024-12-13 14:58:13,928: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_decorator.py
airflow-worker-1     | [2024-12-13 14:58:13,930: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_decorator>
airflow-worker-1     | [2024-12-13 14:58:13,931: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_decorator.py
airflow-worker-1     | [2024-12-13 14:58:13,932: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_short_circuit_decorator>
airflow-worker-1     | [2024-12-13 14:58:13,932: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_controller_dag.py
airflow-worker-1     | [2024-12-13 14:58:13,934: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_trigger_controller_dag>
airflow-worker-1     | [2024-12-13 14:58:13,934: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only.py
airflow-worker-1     | [2024-12-13 14:58:13,936: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: latest_only>
airflow-worker-1     | [2024-12-13 14:58:13,936: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_time_delta_sensor_async.py
airflow-worker-1     | [2024-12-13 14:58:13,937: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_time_delta_sensor_async>
airflow-worker-1     | [2024-12-13 14:58:13,937: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_templates.py
airflow-worker-1     | [2024-12-13 14:58:13,937: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,937: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,937: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,938: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:13,938: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:13,938: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:13,939: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_templates>
airflow-worker-1     | [2024-12-13 14:58:13,939: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api_virtualenv.py
airflow-worker-1     | [2024-12-13 14:58:13,940: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_api_virtualenv>
airflow-worker-1     | [2024-12-13 14:58:13,940: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensor_decorator.py
airflow-worker-1     | [2024-12-13 14:58:13,941: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sensor_decorator>
airflow-worker-1     | [2024-12-13 14:58:13,941: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_operator.py
airflow-worker-1     | [2024-12-13 14:58:13,942: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_short_circuit_operator>
airflow-worker-1     | [2024-12-13 14:58:13,942: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcom.py
airflow-worker-1     | [2024-12-13 14:58:13,943: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom>
airflow-worker-1     | [2024-12-13 14:58:13,944: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias.py
airflow-worker-1     | [2024-12-13 14:58:14,078: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_producer>
airflow-worker-1     | [2024-12-13 14:58:14,078: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_consumer>
airflow-worker-1     | [2024-12-13 14:58:14,078: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_consumer>
airflow-worker-1     | [2024-12-13 14:58:14,078: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_producer>
airflow-worker-1     | [2024-12-13 14:58:14,079: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown_taskflow.py
airflow-worker-1     | [2024-12-13 14:58:14,080: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_setup_teardown_taskflow>
airflow-worker-1     | [2024-12-13 14:58:14,080: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api.py
airflow-worker-1     | [2024-12-13 14:58:14,081: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_api>
airflow-worker-1     | [2024-12-13 14:58:14,081: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_labels.py
airflow-worker-1     | [2024-12-13 14:58:14,081: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,081: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,082: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,082: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,082: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,082: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,083: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_labels>
airflow-worker-1     | [2024-12-13 14:58:14,083: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:58:14,086: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:58:14,086: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_display_name.py
airflow-worker-1     | [2024-12-13 14:58:14,086: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_display_name>
airflow-worker-1     | [2024-12-13 14:58:14,086: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
airflow-worker-1     | [2024-12-13 14:58:14,091: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:14,091: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:14,091: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:14,091: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:14,091: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group_decorator.py
airflow-worker-1     | [2024-12-13 14:58:14,094: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_task_group_decorator>
airflow-worker-1     | [2024-12-13 14:58:14,095: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_target_dag.py
airflow-worker-1     | [2024-12-13 14:58:14,095: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_trigger_target_dag>
airflow-worker-1     | [2024-12-13 14:58:14,095: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_external_task_marker_dag.py
airflow-worker-1     | [2024-12-13 14:58:14,097: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_external_task_marker_parent>
airflow-worker-1     | [2024-12-13 14:58:14,097: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_external_task_marker_child>
airflow-worker-1     | [2024-12-13 14:58:14,097: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_skip_dag.py
airflow-worker-1     | [2024-12-13 14:58:14,099: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_skip_dag>
airflow-worker-1     | [2024-12-13 14:58:14,099: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_workday_timetable.py
airflow-worker-1     | [2024-12-13 14:58:14,099: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_workday_timetable>
airflow-worker-1     | [2024-12-13 14:58:14,099: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_dag.py
airflow-worker-1     | [2024-12-13 14:58:14,100: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_dag>
airflow-worker-1     | [2024-12-13 14:58:14,100: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_operator.py
airflow-worker-1     | [2024-12-13 14:58:14,101: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_python_operator>
airflow-worker-1     | [2024-12-13 14:58:14,101: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group.py
airflow-worker-1     | [2024-12-13 14:58:14,102: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_task_group>
airflow-worker-1     | [2024-12-13 14:58:14,102: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_datetime_operator.py
airflow-worker-1     | [2024-12-13 14:58:14,103: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,103: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,103: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,104: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,104: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,104: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,104: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,104: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,104: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,105: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,105: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,105: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,105: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator_3>
airflow-worker-1     | [2024-12-13 14:58:14,105: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,105: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,105: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,106: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator>
airflow-worker-1     | [2024-12-13 14:58:14,106: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,106: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,106: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,106: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator_2>
airflow-worker-1     | [2024-12-13 14:58:14,106: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dag_decorator.py
airflow-worker-1     | [2024-12-13 14:58:14,161: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dag_decorator>
airflow-worker-1     | [2024-12-13 14:58:14,161: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_day_of_week_operator.py
airflow-worker-1     | [2024-12-13 14:58:14,162: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,162: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,162: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,163: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,163: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,163: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,163: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_weekday_branch_operator>
airflow-worker-1     | [2024-12-13 14:58:14,163: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_inlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:58:14,163: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,163: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,164: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,164: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,164: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,164: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,164: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,164: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,164: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,165: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: read_dataset_event_from_classic>
airflow-worker-1     | [2024-12-13 14:58:14,165: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,165: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,165: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,165: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: read_dataset_event>
airflow-worker-1     | [2024-12-13 14:58:14,165: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping.py
airflow-worker-1     | [2024-12-13 14:58:14,166: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dynamic_task_mapping>
airflow-worker-1     | [2024-12-13 14:58:14,166: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_subdag_operator.py
airflow-worker-1     | [2024-12-13 14:58:14,167: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,167: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,167: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,172: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,172: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,172: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,175: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator.section-1>
airflow-worker-1     | [2024-12-13 14:58:14,175: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator.section-2>
airflow-worker-1     | [2024-12-13 14:58:14,175: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator>
airflow-worker-1     | [2024-12-13 14:58:14,175: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial.py
airflow-worker-1     | [2024-12-13 14:58:14,176: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial>
airflow-worker-1     | [2024-12-13 14:58:14,177: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_complex.py
airflow-worker-1     | [2024-12-13 14:58:14,180: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_complex>
airflow-worker-1     | [2024-12-13 14:58:14,180: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_python_dop_operator_3.py
airflow-worker-1     | [2024-12-13 14:58:14,181: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:14,181: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:14,181: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:14,182: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_dop_operator_v3>
airflow-worker-1     | [2024-12-13 14:58:14,182: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/subdags/subdag.py
airflow-worker-1     | [2024-12-13 14:58:14,182: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/event_listener.py
airflow-worker-1     | [2024-12-13 14:58:14,183: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/listener_plugin.py
airflow-worker-1     | [2024-12-13 14:58:14,183: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/decreasing_priority_weight_strategy.py
airflow-worker-1     | [2024-12-13 14:58:14,183: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/workday.py
airflow-worker-1     | [2024-12-13 14:58:14,202: DEBUG/ForkPoolWorker-15] Plugins are already loaded. Skipping.
airflow-worker-1     | [2024-12-13 14:58:14,203: DEBUG/ForkPoolWorker-15] Integrate DAG plugins
airflow-worker-1     | [2024-12-13 14:58:14,205: DEBUG/ForkPoolWorker-15] previous_execution_date was called
airflow-worker-1     | [2024-12-13 14:58:14,213: INFO/ForkPoolWorker-15] Running <TaskInstance: pipeline.download_and_unwrap manual__2024-12-13T14:58:11.753084+00:00 [queued]> on host 65b6e979e5f1
airflow-worker-1     | [2024-12-13 14:58:14,213: DEBUG/ForkPoolWorker-15] Disposing DB connection pool (PID 76)
airflow-worker-1     | [2024-12-13 14:58:14,213: DEBUG/ForkPoolWorker-15] Setting up DB connection pool (PID 76)
airflow-worker-1     | [2024-12-13 14:58:14,214: DEBUG/ForkPoolWorker-15] settings.prepare_engine_args(): Using NullPool
airflow-worker-1     | [2024-12-13 14:58:17,770: DEBUG/ForkPoolWorker-15] Calling callbacks: []
airflow-worker-1     | [2024-12-13 14:58:17,785: INFO/ForkPoolWorker-15] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[9b98a5c2-6d78-4f62-bdd3-7cc49598db0e] succeeded in 5.030090170000221s: None
airflow-worker-1     | [2024-12-13 14:58:18,619: INFO/MainProcess] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[e007c70a-3a5f-41f7-a4b0-ed22818811d7] received
airflow-worker-1     | [2024-12-13 14:58:18,623: INFO/ForkPoolWorker-15] [e007c70a-3a5f-41f7-a4b0-ed22818811d7] Executing command in Celery: ['airflow', 'tasks', 'run', 'pipeline', 'split', 'manual__2024-12-13T14:58:11.753084+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-worker-1     | [2024-12-13 14:58:19,854: INFO/ForkPoolWorker-15] Filling up the DagBag from /opt/airflow/dags/pipeline.py
airflow-worker-1     | [2024-12-13 14:58:20,403: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: pipeline>
airflow-worker-1     | [2024-12-13 14:58:20,403: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcomargs.py
airflow-worker-1     | [2024-12-13 14:58:20,411: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom_args>
airflow-worker-1     | [2024-12-13 14:58:20,411: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom_args_with_operators>
airflow-worker-1     | [2024-12-13 14:58:20,411: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensors.py
airflow-worker-1     | [2024-12-13 14:58:20,415: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sensors>
airflow-worker-1     | [2024-12-13 14:58:20,415: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
airflow-worker-1     | [2024-12-13 14:58:20,416: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: latest_only_with_trigger>
airflow-worker-1     | [2024-12-13 14:58:20,416: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:58:20,417: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_local_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:58:20,418: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_nested_branch_dag.py
airflow-worker-1     | [2024-12-13 14:58:20,418: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,418: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,418: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,420: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,420: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,420: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,420: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_nested_branch_dag>
airflow-worker-1     | [2024-12-13 14:58:20,421: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py
airflow-worker-1     | [2024-12-13 14:58:20,422: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dynamic_task_mapping_with_no_taskflow_operators>
airflow-worker-1     | [2024-12-13 14:58:20,422: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_operator.py
airflow-worker-1     | [2024-12-13 14:58:20,422: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,422: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,422: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,424: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:58:20,424: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:58:20,425: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator.py
airflow-worker-1     | [2024-12-13 14:58:20,425: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,425: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,426: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,428: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,428: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,428: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,429: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_operator>
airflow-worker-1     | [2024-12-13 14:58:20,429: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_passing_params_via_test_command.py
airflow-worker-1     | [2024-12-13 14:58:20,429: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,429: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,429: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,430: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_passing_params_via_test_command>
airflow-worker-1     | [2024-12-13 14:58:20,430: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_objectstorage.py
airflow-worker-1     | [2024-12-13 14:58:20,432: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_objectstorage>
airflow-worker-1     | [2024-12-13 14:58:20,432: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_decorator.py
airflow-worker-1     | [2024-12-13 14:58:20,434: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_python_decorator>
airflow-worker-1     | [2024-12-13 14:58:20,434: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_trigger_ui.py
airflow-worker-1     | [2024-12-13 14:58:20,436: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_params_trigger_ui>
airflow-worker-1     | [2024-12-13 14:58:20,437: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_ui_tutorial.py
airflow-worker-1     | [2024-12-13 14:58:20,439: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_params_ui_tutorial>
airflow-worker-1     | [2024-12-13 14:58:20,439: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown.py
airflow-worker-1     | [2024-12-13 14:58:20,441: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_setup_teardown>
airflow-worker-1     | [2024-12-13 14:58:20,441: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sla_dag.py
airflow-worker-1     | [2024-12-13 14:58:20,441: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,441: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,441: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,442: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sla_dag>
airflow-worker-1     | [2024-12-13 14:58:20,443: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator_decorator.py
airflow-worker-1     | [2024-12-13 14:58:20,443: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,443: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,443: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,448: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,448: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,448: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,449: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_python_operator_decorator>
airflow-worker-1     | [2024-12-13 14:58:20,449: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_datasets.py
airflow-worker-1     | [2024-12-13 14:58:20,449: DEBUG/ForkPoolWorker-15] Initializing Providers Manager[dataset_uris]
airflow-worker-1     | [2024-12-13 14:58:20,755: DEBUG/ForkPoolWorker-15] Initialization of Providers Manager[dataset_uris] took 0.31 seconds
airflow-worker-1     | [2024-12-13 14:58:20,756: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,756: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,756: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,758: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,758: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,758: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,759: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_or_both_2_and_3_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:58:20,759: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_and_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:58:20,759: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_unknown_never_scheduled>
airflow-worker-1     | [2024-12-13 14:58:20,759: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: conditional_dataset_and_time_based_timetable>
airflow-worker-1     | [2024-12-13 14:58:20,759: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1_never_scheduled>
airflow-worker-1     | [2024-12-13 14:58:20,759: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,759: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,759: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,760: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_produces_1>
airflow-worker-1     | [2024-12-13 14:58:20,760: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_or_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:58:20,760: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_produces_2>
airflow-worker-1     | [2024-12-13 14:58:20,760: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1>
airflow-worker-1     | [2024-12-13 14:58:20,760: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1_and_2>
airflow-worker-1     | [2024-12-13 14:58:20,760: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_outlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:58:20,761: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,761: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,761: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,762: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,762: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,762: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,763: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,763: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,763: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,764: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,764: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,764: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,764: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_from_classic_operator>
airflow-worker-1     | [2024-12-13 14:58:20,764: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,764: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,764: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,765: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_by_yield>
airflow-worker-1     | [2024-12-13 14:58:20,765: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,765: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,765: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,765: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_by_context>
airflow-worker-1     | [2024-12-13 14:58:20,765: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_decorator.py
airflow-worker-1     | [2024-12-13 14:58:20,767: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_decorator>
airflow-worker-1     | [2024-12-13 14:58:20,767: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_decorator.py
airflow-worker-1     | [2024-12-13 14:58:20,769: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_short_circuit_decorator>
airflow-worker-1     | [2024-12-13 14:58:20,769: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_controller_dag.py
airflow-worker-1     | [2024-12-13 14:58:20,771: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_trigger_controller_dag>
airflow-worker-1     | [2024-12-13 14:58:20,771: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only.py
airflow-worker-1     | [2024-12-13 14:58:20,773: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: latest_only>
airflow-worker-1     | [2024-12-13 14:58:20,773: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_time_delta_sensor_async.py
airflow-worker-1     | [2024-12-13 14:58:20,774: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_time_delta_sensor_async>
airflow-worker-1     | [2024-12-13 14:58:20,774: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_templates.py
airflow-worker-1     | [2024-12-13 14:58:20,774: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,775: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,775: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,776: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,776: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,776: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,776: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_templates>
airflow-worker-1     | [2024-12-13 14:58:20,777: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api_virtualenv.py
airflow-worker-1     | [2024-12-13 14:58:20,778: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_api_virtualenv>
airflow-worker-1     | [2024-12-13 14:58:20,778: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensor_decorator.py
airflow-worker-1     | [2024-12-13 14:58:20,779: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sensor_decorator>
airflow-worker-1     | [2024-12-13 14:58:20,779: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_operator.py
airflow-worker-1     | [2024-12-13 14:58:20,780: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_short_circuit_operator>
airflow-worker-1     | [2024-12-13 14:58:20,780: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcom.py
airflow-worker-1     | [2024-12-13 14:58:20,782: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom>
airflow-worker-1     | [2024-12-13 14:58:20,782: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias.py
airflow-worker-1     | [2024-12-13 14:58:20,926: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_producer>
airflow-worker-1     | [2024-12-13 14:58:20,926: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_consumer>
airflow-worker-1     | [2024-12-13 14:58:20,926: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_producer>
airflow-worker-1     | [2024-12-13 14:58:20,926: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_consumer>
airflow-worker-1     | [2024-12-13 14:58:20,926: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown_taskflow.py
airflow-worker-1     | [2024-12-13 14:58:20,928: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_setup_teardown_taskflow>
airflow-worker-1     | [2024-12-13 14:58:20,928: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api.py
airflow-worker-1     | [2024-12-13 14:58:20,929: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_api>
airflow-worker-1     | [2024-12-13 14:58:20,929: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_labels.py
airflow-worker-1     | [2024-12-13 14:58:20,930: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,930: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,930: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,930: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,930: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,931: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,931: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_labels>
airflow-worker-1     | [2024-12-13 14:58:20,931: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:58:20,934: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:58:20,934: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_display_name.py
airflow-worker-1     | [2024-12-13 14:58:20,935: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_display_name>
airflow-worker-1     | [2024-12-13 14:58:20,935: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
airflow-worker-1     | [2024-12-13 14:58:20,940: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:20,940: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:20,940: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:20,941: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:20,941: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group_decorator.py
airflow-worker-1     | [2024-12-13 14:58:20,944: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_task_group_decorator>
airflow-worker-1     | [2024-12-13 14:58:20,944: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_target_dag.py
airflow-worker-1     | [2024-12-13 14:58:20,944: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_trigger_target_dag>
airflow-worker-1     | [2024-12-13 14:58:20,945: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_external_task_marker_dag.py
airflow-worker-1     | [2024-12-13 14:58:20,946: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_external_task_marker_child>
airflow-worker-1     | [2024-12-13 14:58:20,946: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_external_task_marker_parent>
airflow-worker-1     | [2024-12-13 14:58:20,946: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_skip_dag.py
airflow-worker-1     | [2024-12-13 14:58:20,948: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_skip_dag>
airflow-worker-1     | [2024-12-13 14:58:20,948: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_workday_timetable.py
airflow-worker-1     | [2024-12-13 14:58:20,949: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_workday_timetable>
airflow-worker-1     | [2024-12-13 14:58:20,949: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_dag.py
airflow-worker-1     | [2024-12-13 14:58:20,949: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_dag>
airflow-worker-1     | [2024-12-13 14:58:20,949: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_operator.py
airflow-worker-1     | [2024-12-13 14:58:20,951: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_python_operator>
airflow-worker-1     | [2024-12-13 14:58:20,951: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group.py
airflow-worker-1     | [2024-12-13 14:58:20,952: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_task_group>
airflow-worker-1     | [2024-12-13 14:58:20,953: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_datetime_operator.py
airflow-worker-1     | [2024-12-13 14:58:20,953: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,953: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,953: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,954: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,954: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,954: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,955: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,955: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,955: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,955: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,955: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,955: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,956: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator>
airflow-worker-1     | [2024-12-13 14:58:20,956: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,956: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,956: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,956: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator_2>
airflow-worker-1     | [2024-12-13 14:58:20,956: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,956: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,956: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,956: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator_3>
airflow-worker-1     | [2024-12-13 14:58:20,957: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dag_decorator.py
airflow-worker-1     | [2024-12-13 14:58:20,995: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dag_decorator>
airflow-worker-1     | [2024-12-13 14:58:20,995: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_day_of_week_operator.py
airflow-worker-1     | [2024-12-13 14:58:20,996: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,996: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,996: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,997: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,997: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,997: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,997: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_weekday_branch_operator>
airflow-worker-1     | [2024-12-13 14:58:20,997: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_inlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:58:20,998: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,998: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,998: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,998: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,998: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,998: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,999: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,999: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:20,999: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:20,999: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: read_dataset_event_from_classic>
airflow-worker-1     | [2024-12-13 14:58:20,999: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:20,999: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:21,000: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:21,000: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: read_dataset_event>
airflow-worker-1     | [2024-12-13 14:58:21,000: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping.py
airflow-worker-1     | [2024-12-13 14:58:21,002: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dynamic_task_mapping>
airflow-worker-1     | [2024-12-13 14:58:21,002: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_subdag_operator.py
airflow-worker-1     | [2024-12-13 14:58:21,003: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:21,003: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:21,003: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:21,008: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:21,008: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:21,008: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:21,014: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator.section-1>
airflow-worker-1     | [2024-12-13 14:58:21,015: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator.section-2>
airflow-worker-1     | [2024-12-13 14:58:21,015: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator>
airflow-worker-1     | [2024-12-13 14:58:21,015: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial.py
airflow-worker-1     | [2024-12-13 14:58:21,016: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial>
airflow-worker-1     | [2024-12-13 14:58:21,016: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_complex.py
airflow-worker-1     | [2024-12-13 14:58:21,019: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_complex>
airflow-worker-1     | [2024-12-13 14:58:21,019: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_python_dop_operator_3.py
airflow-worker-1     | [2024-12-13 14:58:21,019: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:21,020: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:21,020: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:21,020: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_dop_operator_v3>
airflow-worker-1     | [2024-12-13 14:58:21,021: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/subdags/subdag.py
airflow-worker-1     | [2024-12-13 14:58:21,021: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/event_listener.py
airflow-worker-1     | [2024-12-13 14:58:21,022: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/listener_plugin.py
airflow-worker-1     | [2024-12-13 14:58:21,022: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/decreasing_priority_weight_strategy.py
airflow-worker-1     | [2024-12-13 14:58:21,022: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/workday.py
airflow-worker-1     | [2024-12-13 14:58:21,041: DEBUG/ForkPoolWorker-15] Plugins are already loaded. Skipping.
airflow-worker-1     | [2024-12-13 14:58:21,041: DEBUG/ForkPoolWorker-15] Integrate DAG plugins
airflow-worker-1     | [2024-12-13 14:58:21,043: DEBUG/ForkPoolWorker-15] previous_execution_date was called
airflow-worker-1     | [2024-12-13 14:58:21,048: INFO/ForkPoolWorker-15] Running <TaskInstance: pipeline.split manual__2024-12-13T14:58:11.753084+00:00 [queued]> on host 65b6e979e5f1
airflow-worker-1     | [2024-12-13 14:58:21,049: DEBUG/ForkPoolWorker-15] Disposing DB connection pool (PID 79)
airflow-worker-1     | [2024-12-13 14:58:21,049: DEBUG/ForkPoolWorker-15] Setting up DB connection pool (PID 79)
airflow-worker-1     | [2024-12-13 14:58:21,049: DEBUG/ForkPoolWorker-15] settings.prepare_engine_args(): Using NullPool
airflow-worker-1     | [2024-12-13 14:58:21,531: DEBUG/ForkPoolWorker-15] Calling callbacks: []
airflow-worker-1     | [2024-12-13 14:58:21,545: INFO/ForkPoolWorker-15] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[e007c70a-3a5f-41f7-a4b0-ed22818811d7] succeeded in 2.9248757929999556s: None
airflow-worker-1     | [2024-12-13 14:58:21,910: INFO/MainProcess] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[1cb55bd1-bdad-4269-8673-d5be69eb2fb7] received
airflow-worker-1     | [2024-12-13 14:58:21,919: INFO/ForkPoolWorker-15] [1cb55bd1-bdad-4269-8673-d5be69eb2fb7] Executing command in Celery: ['airflow', 'tasks', 'run', 'pipeline', 'upload_to_minio', 'manual__2024-12-13T14:58:11.753084+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-worker-1     | [2024-12-13 14:58:22,187: INFO/ForkPoolWorker-15] Filling up the DagBag from /opt/airflow/dags/pipeline.py
airflow-worker-1     | [2024-12-13 14:58:22,462: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: pipeline>
airflow-worker-1     | [2024-12-13 14:58:22,466: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcomargs.py
airflow-worker-1     | [2024-12-13 14:58:22,495: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom_args>
airflow-worker-1     | [2024-12-13 14:58:22,495: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom_args_with_operators>
airflow-worker-1     | [2024-12-13 14:58:22,495: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensors.py
airflow-worker-1     | [2024-12-13 14:58:22,499: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sensors>
airflow-worker-1     | [2024-12-13 14:58:22,500: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
airflow-worker-1     | [2024-12-13 14:58:22,501: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: latest_only_with_trigger>
airflow-worker-1     | [2024-12-13 14:58:22,501: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:58:22,503: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_local_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:58:22,503: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_nested_branch_dag.py
airflow-worker-1     | [2024-12-13 14:58:22,503: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,503: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,503: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,504: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,504: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,505: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,505: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_nested_branch_dag>
airflow-worker-1     | [2024-12-13 14:58:22,505: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py
airflow-worker-1     | [2024-12-13 14:58:22,506: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dynamic_task_mapping_with_no_taskflow_operators>
airflow-worker-1     | [2024-12-13 14:58:22,506: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_operator.py
airflow-worker-1     | [2024-12-13 14:58:22,507: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,507: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,507: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,508: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:58:22,509: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:58:22,509: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator.py
airflow-worker-1     | [2024-12-13 14:58:22,509: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,509: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,510: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,512: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,512: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,512: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,513: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_operator>
airflow-worker-1     | [2024-12-13 14:58:22,514: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_passing_params_via_test_command.py
airflow-worker-1     | [2024-12-13 14:58:22,514: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,514: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,515: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,516: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_passing_params_via_test_command>
airflow-worker-1     | [2024-12-13 14:58:22,516: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_objectstorage.py
airflow-worker-1     | [2024-12-13 14:58:22,517: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_objectstorage>
airflow-worker-1     | [2024-12-13 14:58:22,517: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_decorator.py
airflow-worker-1     | [2024-12-13 14:58:22,519: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_python_decorator>
airflow-worker-1     | [2024-12-13 14:58:22,519: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_trigger_ui.py
airflow-worker-1     | [2024-12-13 14:58:22,521: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_params_trigger_ui>
airflow-worker-1     | [2024-12-13 14:58:22,522: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_ui_tutorial.py
airflow-worker-1     | [2024-12-13 14:58:22,523: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_params_ui_tutorial>
airflow-worker-1     | [2024-12-13 14:58:22,524: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown.py
airflow-worker-1     | [2024-12-13 14:58:22,525: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_setup_teardown>
airflow-worker-1     | [2024-12-13 14:58:22,525: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sla_dag.py
airflow-worker-1     | [2024-12-13 14:58:22,532: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,532: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,536: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,540: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sla_dag>
airflow-worker-1     | [2024-12-13 14:58:22,541: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator_decorator.py
airflow-worker-1     | [2024-12-13 14:58:22,546: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,548: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,560: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,651: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,651: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,651: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,652: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_python_operator_decorator>
airflow-worker-1     | [2024-12-13 14:58:22,652: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_datasets.py
airflow-worker-1     | [2024-12-13 14:58:22,653: DEBUG/ForkPoolWorker-15] Initializing Providers Manager[dataset_uris]
airflow-worker-1     | [2024-12-13 14:58:22,844: DEBUG/ForkPoolWorker-15] Initialization of Providers Manager[dataset_uris] took 0.19 seconds
airflow-worker-1     | [2024-12-13 14:58:22,844: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,844: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,844: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,847: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,847: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,848: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,848: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1_and_2>
airflow-worker-1     | [2024-12-13 14:58:22,849: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_unknown_never_scheduled>
airflow-worker-1     | [2024-12-13 14:58:22,849: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1>
airflow-worker-1     | [2024-12-13 14:58:22,849: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_or_both_2_and_3_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:58:22,849: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_produces_2>
airflow-worker-1     | [2024-12-13 14:58:22,849: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,849: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,849: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,850: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_produces_1>
airflow-worker-1     | [2024-12-13 14:58:22,850: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_and_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:58:22,850: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_consumes_1_never_scheduled>
airflow-worker-1     | [2024-12-13 14:58:22,850: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: consume_1_or_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:58:22,850: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: conditional_dataset_and_time_based_timetable>
airflow-worker-1     | [2024-12-13 14:58:22,850: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_outlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:58:22,852: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,852: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,852: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,852: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,853: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,853: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,853: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,853: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,853: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,854: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,854: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,854: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,855: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_by_context>
airflow-worker-1     | [2024-12-13 14:58:22,855: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,855: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,855: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,855: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_from_classic_operator>
airflow-worker-1     | [2024-12-13 14:58:22,855: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,855: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,856: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,856: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_with_extra_by_yield>
airflow-worker-1     | [2024-12-13 14:58:22,856: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_decorator.py
airflow-worker-1     | [2024-12-13 14:58:22,859: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_bash_decorator>
airflow-worker-1     | [2024-12-13 14:58:22,859: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_decorator.py
airflow-worker-1     | [2024-12-13 14:58:22,860: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_short_circuit_decorator>
airflow-worker-1     | [2024-12-13 14:58:22,860: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_controller_dag.py
airflow-worker-1     | [2024-12-13 14:58:22,862: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_trigger_controller_dag>
airflow-worker-1     | [2024-12-13 14:58:22,862: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only.py
airflow-worker-1     | [2024-12-13 14:58:22,864: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: latest_only>
airflow-worker-1     | [2024-12-13 14:58:22,864: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_time_delta_sensor_async.py
airflow-worker-1     | [2024-12-13 14:58:22,865: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_time_delta_sensor_async>
airflow-worker-1     | [2024-12-13 14:58:22,865: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_templates.py
airflow-worker-1     | [2024-12-13 14:58:22,866: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,866: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,866: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,867: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:22,867: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:22,867: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:22,868: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_templates>
airflow-worker-1     | [2024-12-13 14:58:22,868: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api_virtualenv.py
airflow-worker-1     | [2024-12-13 14:58:22,870: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_api_virtualenv>
airflow-worker-1     | [2024-12-13 14:58:22,870: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensor_decorator.py
airflow-worker-1     | [2024-12-13 14:58:22,872: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_sensor_decorator>
airflow-worker-1     | [2024-12-13 14:58:22,872: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_operator.py
airflow-worker-1     | [2024-12-13 14:58:22,873: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_short_circuit_operator>
airflow-worker-1     | [2024-12-13 14:58:22,874: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcom.py
airflow-worker-1     | [2024-12-13 14:58:22,875: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_xcom>
airflow-worker-1     | [2024-12-13 14:58:22,875: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias.py
airflow-worker-1     | [2024-12-13 14:58:23,045: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_consumer>
airflow-worker-1     | [2024-12-13 14:58:23,045: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_producer>
airflow-worker-1     | [2024-12-13 14:58:23,045: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_producer>
airflow-worker-1     | [2024-12-13 14:58:23,045: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_consumer>
airflow-worker-1     | [2024-12-13 14:58:23,045: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown_taskflow.py
airflow-worker-1     | [2024-12-13 14:58:23,047: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_setup_teardown_taskflow>
airflow-worker-1     | [2024-12-13 14:58:23,047: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api.py
airflow-worker-1     | [2024-12-13 14:58:23,048: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_taskflow_api>
airflow-worker-1     | [2024-12-13 14:58:23,049: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_labels.py
airflow-worker-1     | [2024-12-13 14:58:23,049: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,049: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,049: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,050: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,050: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,050: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,051: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_labels>
airflow-worker-1     | [2024-12-13 14:58:23,051: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:58:23,055: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:58:23,055: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_display_name.py
airflow-worker-1     | [2024-12-13 14:58:23,056: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_display_name>
airflow-worker-1     | [2024-12-13 14:58:23,056: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
airflow-worker-1     | [2024-12-13 14:58:23,063: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:23,063: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:23,063: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_alias_example_alias_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:23,063: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: dataset_s3_bucket_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:23,063: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group_decorator.py
airflow-worker-1     | [2024-12-13 14:58:23,068: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_task_group_decorator>
airflow-worker-1     | [2024-12-13 14:58:23,068: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_target_dag.py
airflow-worker-1     | [2024-12-13 14:58:23,068: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_trigger_target_dag>
airflow-worker-1     | [2024-12-13 14:58:23,069: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_external_task_marker_dag.py
airflow-worker-1     | [2024-12-13 14:58:23,070: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_external_task_marker_child>
airflow-worker-1     | [2024-12-13 14:58:23,071: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_external_task_marker_parent>
airflow-worker-1     | [2024-12-13 14:58:23,071: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_skip_dag.py
airflow-worker-1     | [2024-12-13 14:58:23,072: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_skip_dag>
airflow-worker-1     | [2024-12-13 14:58:23,073: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_workday_timetable.py
airflow-worker-1     | [2024-12-13 14:58:23,073: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_workday_timetable>
airflow-worker-1     | [2024-12-13 14:58:23,073: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_dag.py
airflow-worker-1     | [2024-12-13 14:58:23,074: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial_dag>
airflow-worker-1     | [2024-12-13 14:58:23,074: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_operator.py
airflow-worker-1     | [2024-12-13 14:58:23,075: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_python_operator>
airflow-worker-1     | [2024-12-13 14:58:23,075: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group.py
airflow-worker-1     | [2024-12-13 14:58:23,077: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_task_group>
airflow-worker-1     | [2024-12-13 14:58:23,077: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_datetime_operator.py
airflow-worker-1     | [2024-12-13 14:58:23,078: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,078: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,078: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,078: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,078: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,079: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,079: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,079: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,079: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,080: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,080: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,080: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,080: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator_3>
airflow-worker-1     | [2024-12-13 14:58:23,080: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,080: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,080: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,080: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator>
airflow-worker-1     | [2024-12-13 14:58:23,080: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,080: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,081: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,081: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_datetime_operator_2>
airflow-worker-1     | [2024-12-13 14:58:23,081: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dag_decorator.py
airflow-worker-1     | [2024-12-13 14:58:23,123: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dag_decorator>
airflow-worker-1     | [2024-12-13 14:58:23,123: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_day_of_week_operator.py
airflow-worker-1     | [2024-12-13 14:58:23,124: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,124: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,125: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,126: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,126: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,126: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,126: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_weekday_branch_operator>
airflow-worker-1     | [2024-12-13 14:58:23,126: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_inlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:58:23,127: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,127: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,127: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,128: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,128: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,128: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,129: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,129: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,129: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,129: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: read_dataset_event>
airflow-worker-1     | [2024-12-13 14:58:23,130: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,130: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,130: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,130: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: read_dataset_event_from_classic>
airflow-worker-1     | [2024-12-13 14:58:23,130: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping.py
airflow-worker-1     | [2024-12-13 14:58:23,132: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_dynamic_task_mapping>
airflow-worker-1     | [2024-12-13 14:58:23,132: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_subdag_operator.py
airflow-worker-1     | [2024-12-13 14:58:23,133: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,133: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,133: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,141: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,141: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,142: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,150: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator.section-1>
airflow-worker-1     | [2024-12-13 14:58:23,150: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator.section-2>
airflow-worker-1     | [2024-12-13 14:58:23,151: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_subdag_operator>
airflow-worker-1     | [2024-12-13 14:58:23,151: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial.py
airflow-worker-1     | [2024-12-13 14:58:23,153: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: tutorial>
airflow-worker-1     | [2024-12-13 14:58:23,153: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_complex.py
airflow-worker-1     | [2024-12-13 14:58:23,157: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_complex>
airflow-worker-1     | [2024-12-13 14:58:23,157: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_python_dop_operator_3.py
airflow-worker-1     | [2024-12-13 14:58:23,159: DEBUG/ForkPoolWorker-15] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:23,159: DEBUG/ForkPoolWorker-15] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:23,159: DEBUG/ForkPoolWorker-15] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:23,161: DEBUG/ForkPoolWorker-15] Loaded DAG <DAG: example_branch_dop_operator_v3>
airflow-worker-1     | [2024-12-13 14:58:23,161: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/subdags/subdag.py
airflow-worker-1     | [2024-12-13 14:58:23,161: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/event_listener.py
airflow-worker-1     | [2024-12-13 14:58:23,162: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/listener_plugin.py
airflow-worker-1     | [2024-12-13 14:58:23,163: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/decreasing_priority_weight_strategy.py
airflow-worker-1     | [2024-12-13 14:58:23,166: DEBUG/ForkPoolWorker-15] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/workday.py
airflow-worker-1     | [2024-12-13 14:58:23,191: DEBUG/ForkPoolWorker-15] Plugins are already loaded. Skipping.
airflow-worker-1     | [2024-12-13 14:58:23,191: DEBUG/ForkPoolWorker-15] Integrate DAG plugins
airflow-worker-1     | [2024-12-13 14:58:23,196: DEBUG/ForkPoolWorker-15] previous_execution_date was called
airflow-worker-1     | [2024-12-13 14:58:23,203: INFO/ForkPoolWorker-15] Running <TaskInstance: pipeline.upload_to_minio manual__2024-12-13T14:58:11.753084+00:00 [queued]> on host 65b6e979e5f1
airflow-worker-1     | [2024-12-13 14:58:23,203: DEBUG/ForkPoolWorker-15] Disposing DB connection pool (PID 82)
airflow-worker-1     | [2024-12-13 14:58:23,203: DEBUG/ForkPoolWorker-15] Setting up DB connection pool (PID 82)
airflow-worker-1     | [2024-12-13 14:58:23,203: DEBUG/ForkPoolWorker-15] settings.prepare_engine_args(): Using NullPool
airflow-worker-1     | [2024-12-13 14:58:23,678: INFO/MainProcess] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[62403189-caef-43d6-a734-72b5db033277] received
airflow-worker-1     | [2024-12-13 14:58:23,696: DEBUG/ForkPoolWorker-15] Calling callbacks: []
airflow-worker-1     | [2024-12-13 14:58:23,710: INFO/ForkPoolWorker-15] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[1cb55bd1-bdad-4269-8673-d5be69eb2fb7] succeeded in 1.7973406259998228s: None
airflow-worker-1     | [2024-12-13 14:58:23,758: INFO/ForkPoolWorker-16] [62403189-caef-43d6-a734-72b5db033277] Executing command in Celery: ['airflow', 'tasks', 'run', 'pipeline', 'train_and_predict', 'manual__2024-12-13T14:58:11.753084+00:00', '--local', '--subdir', 'DAGS_FOLDER/pipeline.py']
airflow-worker-1     | [2024-12-13 14:58:24,092: INFO/ForkPoolWorker-16] Filling up the DagBag from /opt/airflow/dags/pipeline.py
airflow-worker-1     | [2024-12-13 14:58:24,356: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: pipeline>
airflow-worker-1     | [2024-12-13 14:58:24,357: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcomargs.py
airflow-worker-1     | [2024-12-13 14:58:24,364: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_xcom_args>
airflow-worker-1     | [2024-12-13 14:58:24,364: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_xcom_args_with_operators>
airflow-worker-1     | [2024-12-13 14:58:24,364: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensors.py
airflow-worker-1     | [2024-12-13 14:58:24,367: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_sensors>
airflow-worker-1     | [2024-12-13 14:58:24,367: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
airflow-worker-1     | [2024-12-13 14:58:24,368: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: latest_only_with_trigger>
airflow-worker-1     | [2024-12-13 14:58:24,369: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:58:24,370: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_local_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:58:24,370: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_nested_branch_dag.py
airflow-worker-1     | [2024-12-13 14:58:24,370: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,370: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,370: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,372: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,372: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,372: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,372: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_nested_branch_dag>
airflow-worker-1     | [2024-12-13 14:58:24,372: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py
airflow-worker-1     | [2024-12-13 14:58:24,374: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_dynamic_task_mapping_with_no_taskflow_operators>
airflow-worker-1     | [2024-12-13 14:58:24,374: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_operator.py
airflow-worker-1     | [2024-12-13 14:58:24,374: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,374: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,374: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,376: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:58:24,376: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_bash_operator>
airflow-worker-1     | [2024-12-13 14:58:24,376: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator.py
airflow-worker-1     | [2024-12-13 14:58:24,377: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,377: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,377: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,380: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,380: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,380: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,381: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_operator>
airflow-worker-1     | [2024-12-13 14:58:24,381: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_passing_params_via_test_command.py
airflow-worker-1     | [2024-12-13 14:58:24,382: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,382: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,382: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,384: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_passing_params_via_test_command>
airflow-worker-1     | [2024-12-13 14:58:24,384: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_objectstorage.py
airflow-worker-1     | [2024-12-13 14:58:24,385: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: tutorial_objectstorage>
airflow-worker-1     | [2024-12-13 14:58:24,385: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_decorator.py
airflow-worker-1     | [2024-12-13 14:58:24,388: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_python_decorator>
airflow-worker-1     | [2024-12-13 14:58:24,388: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_trigger_ui.py
airflow-worker-1     | [2024-12-13 14:58:24,390: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_params_trigger_ui>
airflow-worker-1     | [2024-12-13 14:58:24,390: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_params_ui_tutorial.py
airflow-worker-1     | [2024-12-13 14:58:24,393: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_params_ui_tutorial>
airflow-worker-1     | [2024-12-13 14:58:24,393: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown.py
airflow-worker-1     | [2024-12-13 14:58:24,394: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_setup_teardown>
airflow-worker-1     | [2024-12-13 14:58:24,394: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sla_dag.py
airflow-worker-1     | [2024-12-13 14:58:24,395: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,395: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,395: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,396: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_sla_dag>
airflow-worker-1     | [2024-12-13 14:58:24,396: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_operator_decorator.py
airflow-worker-1     | [2024-12-13 14:58:24,397: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,397: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,397: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,401: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,401: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,401: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,402: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_python_operator_decorator>
airflow-worker-1     | [2024-12-13 14:58:24,402: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_datasets.py
airflow-worker-1     | [2024-12-13 14:58:24,403: DEBUG/ForkPoolWorker-16] Initializing Providers Manager[dataset_uris]
airflow-worker-1     | [2024-12-13 14:58:24,618: DEBUG/ForkPoolWorker-16] Initialization of Providers Manager[dataset_uris] took 0.21 seconds
airflow-worker-1     | [2024-12-13 14:58:24,618: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,618: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,618: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,620: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,620: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,621: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,621: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_consumes_unknown_never_scheduled>
airflow-worker-1     | [2024-12-13 14:58:24,622: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_consumes_1>
airflow-worker-1     | [2024-12-13 14:58:24,622: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: consume_1_and_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:58:24,622: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,622: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,622: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,623: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_produces_1>
airflow-worker-1     | [2024-12-13 14:58:24,623: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_consumes_1_and_2>
airflow-worker-1     | [2024-12-13 14:58:24,623: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: conditional_dataset_and_time_based_timetable>
airflow-worker-1     | [2024-12-13 14:58:24,623: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_consumes_1_never_scheduled>
airflow-worker-1     | [2024-12-13 14:58:24,623: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: consume_1_or_both_2_and_3_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:58:24,623: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_produces_2>
airflow-worker-1     | [2024-12-13 14:58:24,623: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: consume_1_or_2_with_dataset_expressions>
airflow-worker-1     | [2024-12-13 14:58:24,623: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_outlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:58:24,625: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,625: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,625: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,626: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,626: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,626: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,627: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,627: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,627: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,627: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,627: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,627: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,627: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_with_extra_from_classic_operator>
airflow-worker-1     | [2024-12-13 14:58:24,628: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,628: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,628: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,628: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_with_extra_by_yield>
airflow-worker-1     | [2024-12-13 14:58:24,628: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,628: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,628: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,629: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_with_extra_by_context>
airflow-worker-1     | [2024-12-13 14:58:24,629: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_bash_decorator.py
airflow-worker-1     | [2024-12-13 14:58:24,631: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_bash_decorator>
airflow-worker-1     | [2024-12-13 14:58:24,632: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_decorator.py
airflow-worker-1     | [2024-12-13 14:58:24,633: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_short_circuit_decorator>
airflow-worker-1     | [2024-12-13 14:58:24,633: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_controller_dag.py
airflow-worker-1     | [2024-12-13 14:58:24,635: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_trigger_controller_dag>
airflow-worker-1     | [2024-12-13 14:58:24,635: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_latest_only.py
airflow-worker-1     | [2024-12-13 14:58:24,636: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: latest_only>
airflow-worker-1     | [2024-12-13 14:58:24,637: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_time_delta_sensor_async.py
airflow-worker-1     | [2024-12-13 14:58:24,637: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_time_delta_sensor_async>
airflow-worker-1     | [2024-12-13 14:58:24,637: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_templates.py
airflow-worker-1     | [2024-12-13 14:58:24,638: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,638: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,638: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,639: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,639: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,639: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,640: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: tutorial_taskflow_templates>
airflow-worker-1     | [2024-12-13 14:58:24,640: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api_virtualenv.py
airflow-worker-1     | [2024-12-13 14:58:24,641: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: tutorial_taskflow_api_virtualenv>
airflow-worker-1     | [2024-12-13 14:58:24,641: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_sensor_decorator.py
airflow-worker-1     | [2024-12-13 14:58:24,642: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_sensor_decorator>
airflow-worker-1     | [2024-12-13 14:58:24,642: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_short_circuit_operator.py
airflow-worker-1     | [2024-12-13 14:58:24,643: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_short_circuit_operator>
airflow-worker-1     | [2024-12-13 14:58:24,643: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_xcom.py
airflow-worker-1     | [2024-12-13 14:58:24,645: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_xcom>
airflow-worker-1     | [2024-12-13 14:58:24,645: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias.py
airflow-worker-1     | [2024-12-13 14:58:24,827: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_alias_example_alias_producer>
airflow-worker-1     | [2024-12-13 14:58:24,827: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_s3_bucket_consumer>
airflow-worker-1     | [2024-12-13 14:58:24,827: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_s3_bucket_producer>
airflow-worker-1     | [2024-12-13 14:58:24,827: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_alias_example_alias_consumer>
airflow-worker-1     | [2024-12-13 14:58:24,827: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_setup_teardown_taskflow.py
airflow-worker-1     | [2024-12-13 14:58:24,829: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_setup_teardown_taskflow>
airflow-worker-1     | [2024-12-13 14:58:24,829: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_taskflow_api.py
airflow-worker-1     | [2024-12-13 14:58:24,830: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: tutorial_taskflow_api>
airflow-worker-1     | [2024-12-13 14:58:24,830: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_labels.py
airflow-worker-1     | [2024-12-13 14:58:24,830: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,830: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,831: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,831: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,831: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,831: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,832: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_labels>
airflow-worker-1     | [2024-12-13 14:58:24,832: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_kubernetes_executor.py
airflow-worker-1     | [2024-12-13 14:58:24,835: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_kubernetes_executor>
airflow-worker-1     | [2024-12-13 14:58:24,835: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_display_name.py
airflow-worker-1     | [2024-12-13 14:58:24,835: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_display_name>
airflow-worker-1     | [2024-12-13 14:58:24,835: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dataset_alias_with_no_taskflow.py
airflow-worker-1     | [2024-12-13 14:58:24,841: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_alias_example_alias_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:24,841: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_s3_bucket_consumer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:24,841: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_alias_example_alias_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:24,841: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: dataset_s3_bucket_producer_with_no_taskflow>
airflow-worker-1     | [2024-12-13 14:58:24,841: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group_decorator.py
airflow-worker-1     | [2024-12-13 14:58:24,844: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_task_group_decorator>
airflow-worker-1     | [2024-12-13 14:58:24,844: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_trigger_target_dag.py
airflow-worker-1     | [2024-12-13 14:58:24,845: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_trigger_target_dag>
airflow-worker-1     | [2024-12-13 14:58:24,845: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_external_task_marker_dag.py
airflow-worker-1     | [2024-12-13 14:58:24,846: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_external_task_marker_parent>
airflow-worker-1     | [2024-12-13 14:58:24,846: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_external_task_marker_child>
airflow-worker-1     | [2024-12-13 14:58:24,846: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_skip_dag.py
airflow-worker-1     | [2024-12-13 14:58:24,848: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_skip_dag>
airflow-worker-1     | [2024-12-13 14:58:24,848: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_workday_timetable.py
airflow-worker-1     | [2024-12-13 14:58:24,848: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_workday_timetable>
airflow-worker-1     | [2024-12-13 14:58:24,849: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial_dag.py
airflow-worker-1     | [2024-12-13 14:58:24,849: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: tutorial_dag>
airflow-worker-1     | [2024-12-13 14:58:24,849: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_python_operator.py
airflow-worker-1     | [2024-12-13 14:58:24,851: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_python_operator>
airflow-worker-1     | [2024-12-13 14:58:24,851: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_task_group.py
airflow-worker-1     | [2024-12-13 14:58:24,852: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_task_group>
airflow-worker-1     | [2024-12-13 14:58:24,852: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_datetime_operator.py
airflow-worker-1     | [2024-12-13 14:58:24,852: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,852: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,852: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,853: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,853: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,853: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,853: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,854: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,854: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,854: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,855: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,855: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,855: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_datetime_operator>
airflow-worker-1     | [2024-12-13 14:58:24,855: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,855: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,855: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,855: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_datetime_operator_3>
airflow-worker-1     | [2024-12-13 14:58:24,855: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,855: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,855: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,855: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_datetime_operator_2>
airflow-worker-1     | [2024-12-13 14:58:24,856: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dag_decorator.py
airflow-worker-1     | [2024-12-13 14:58:24,916: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_dag_decorator>
airflow-worker-1     | [2024-12-13 14:58:24,917: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_day_of_week_operator.py
airflow-worker-1     | [2024-12-13 14:58:24,918: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,918: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,918: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,919: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,919: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,919: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,920: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_weekday_branch_operator>
airflow-worker-1     | [2024-12-13 14:58:24,921: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_inlet_event_extra.py
airflow-worker-1     | [2024-12-13 14:58:24,921: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,922: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,922: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,923: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,923: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,923: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,924: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,924: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,924: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,925: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: read_dataset_event>
airflow-worker-1     | [2024-12-13 14:58:24,925: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,925: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,925: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,925: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: read_dataset_event_from_classic>
airflow-worker-1     | [2024-12-13 14:58:24,926: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_dynamic_task_mapping.py
airflow-worker-1     | [2024-12-13 14:58:24,929: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_dynamic_task_mapping>
airflow-worker-1     | [2024-12-13 14:58:24,929: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_subdag_operator.py
airflow-worker-1     | [2024-12-13 14:58:24,930: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,930: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,930: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,936: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,936: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,936: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,942: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_subdag_operator.section-1>
airflow-worker-1     | [2024-12-13 14:58:24,942: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_subdag_operator.section-2>
airflow-worker-1     | [2024-12-13 14:58:24,942: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_subdag_operator>
airflow-worker-1     | [2024-12-13 14:58:24,942: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/tutorial.py
airflow-worker-1     | [2024-12-13 14:58:24,943: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: tutorial>
airflow-worker-1     | [2024-12-13 14:58:24,943: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_complex.py
airflow-worker-1     | [2024-12-13 14:58:24,946: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_complex>
airflow-worker-1     | [2024-12-13 14:58:24,946: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/example_branch_python_dop_operator_3.py
airflow-worker-1     | [2024-12-13 14:58:24,947: DEBUG/ForkPoolWorker-16] Failed to find locale C
airflow-worker-1     | [2024-12-13 14:58:24,947: DEBUG/ForkPoolWorker-16] Attempting to load en_US as fallback
airflow-worker-1     | [2024-12-13 14:58:24,947: DEBUG/ForkPoolWorker-16] /home/airflow/.local/lib/python3.12/site-packages/cron_descriptor/locale/en_US.mo Loaded
airflow-worker-1     | [2024-12-13 14:58:24,948: DEBUG/ForkPoolWorker-16] Loaded DAG <DAG: example_branch_dop_operator_v3>
airflow-worker-1     | [2024-12-13 14:58:24,948: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/subdags/subdag.py
airflow-worker-1     | [2024-12-13 14:58:24,948: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/event_listener.py
airflow-worker-1     | [2024-12-13 14:58:24,948: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/listener_plugin.py
airflow-worker-1     | [2024-12-13 14:58:24,949: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/decreasing_priority_weight_strategy.py
airflow-worker-1     | [2024-12-13 14:58:24,949: DEBUG/ForkPoolWorker-16] Importing /home/airflow/.local/lib/python3.12/site-packages/airflow/example_dags/plugins/workday.py
airflow-worker-1     | [2024-12-13 14:58:24,969: DEBUG/ForkPoolWorker-16] Plugins are already loaded. Skipping.
airflow-worker-1     | [2024-12-13 14:58:24,969: DEBUG/ForkPoolWorker-16] Integrate DAG plugins
airflow-worker-1     | [2024-12-13 14:58:24,971: DEBUG/ForkPoolWorker-16] previous_execution_date was called
airflow-worker-1     | [2024-12-13 14:58:24,976: INFO/ForkPoolWorker-16] Running <TaskInstance: pipeline.train_and_predict manual__2024-12-13T14:58:11.753084+00:00 [queued]> on host 65b6e979e5f1
airflow-worker-1     | [2024-12-13 14:58:24,976: DEBUG/ForkPoolWorker-16] Disposing DB connection pool (PID 85)
airflow-worker-1     | [2024-12-13 14:58:24,976: DEBUG/ForkPoolWorker-16] Setting up DB connection pool (PID 85)
airflow-worker-1     | [2024-12-13 14:58:24,976: DEBUG/ForkPoolWorker-16] settings.prepare_engine_args(): Using NullPool
airflow-worker-1     | Setting default log level to "WARN".
airflow-worker-1     | To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
airflow-worker-1     | 24/12/13 14:58:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
airflow-worker-1     | 24/12/13 14:58:35 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://movielens/train.csv.
airflow-worker-1     | java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
airflow-worker-1     | 	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
airflow-worker-1     | 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
airflow-worker-1     | 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
airflow-worker-1     | 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
airflow-worker-1     | 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
airflow-worker-1     | 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
airflow-worker-1     | 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
airflow-worker-1     | 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
airflow-worker-1     | 	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
airflow-worker-1     | 	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
airflow-worker-1     | 	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
airflow-worker-1     | 	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
airflow-worker-1     | 	at scala.Option.getOrElse(Option.scala:189)
airflow-worker-1     | 	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
airflow-worker-1     | 	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
airflow-worker-1     | 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
airflow-worker-1     | 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
airflow-worker-1     | 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
airflow-worker-1     | 	at java.lang.reflect.Method.invoke(Method.java:498)
airflow-worker-1     | 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
airflow-worker-1     | 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
airflow-worker-1     | 	at py4j.Gateway.invoke(Gateway.java:282)
airflow-worker-1     | 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
airflow-worker-1     | 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
airflow-worker-1     | 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
airflow-worker-1     | 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
airflow-worker-1     | 	at java.lang.Thread.run(Thread.java:750)
airflow-worker-1     | Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
airflow-worker-1     | 	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
airflow-worker-1     | 	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
airflow-worker-1     | 	... 26 more
airflow-worker-1     | [2024-12-13 14:58:35,715: DEBUG/ForkPoolWorker-16] Calling callbacks: []
airflow-worker-1     | [2024-12-13 14:58:35,737: INFO/ForkPoolWorker-16] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[62403189-caef-43d6-a734-72b5db033277] succeeded in 12.05583671399927s: None
airflow-worker-1     | 
airflow-worker-1     | worker: Warm shutdown (MainProcess)
airflow-worker-1     | [2024-12-13 15:02:04 +0000] [32] [INFO] Handling signal: term
airflow-worker-1     | [2024-12-13 15:02:04 +0000] [33] [INFO] Worker exiting (pid: 33)
airflow-worker-1     | [2024-12-13 15:02:04 +0000] [35] [INFO] Worker exiting (pid: 35)
airflow-worker-1     | [2024-12-13 15:02:04 +0000] [32] [INFO] Shutting down: Master
